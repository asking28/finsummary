{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_extractor_ml_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/finsummary/blob/master/train_extractor_ml_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-J5IVnU_7u5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k8CoqplBgeG"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcHUchaAU4o"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from os.path import join, exists\n",
        "from datetime import timedelta\n",
        "import pickle as pkl\n",
        "from toolz import compose\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from torch import nn\n",
        "from itertools import starmap\n",
        "from toolz import curry, reduce\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import init\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from toolz.sandbox import unzip\n",
        "from toolz import curry, concat, compose\n",
        "from toolz import curried\n",
        "import torch.multiprocessing as mp\n",
        "from os.path import basename\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import tensorboardX\n",
        "from time import time\n",
        "from glob import glob\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVsF_pmRbzDw"
      },
      "source": [
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zi8Tei2AltL"
      },
      "source": [
        "def prob_normalize(score, mask):\n",
        "    \"\"\" [(...), T]\n",
        "    user should handle mask shape\"\"\"\n",
        "    score = score.masked_fill(mask == 0, -1e18)\n",
        "    norm_score = F.softmax(score, dim=-1)\n",
        "    return norm_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwBDaQxiCR5s"
      },
      "source": [
        "def dot_attention_score(key,query):\n",
        "  return query.matmul(key.transpose(1,2))\n",
        "\n",
        "def prob_normalize(score,mask):\n",
        "  score=score.masked_fill(mask==0,-1e18)\n",
        "  norm_score=F.softmax(score,dim=-1)\n",
        "  return norm_score\n",
        "\n",
        "def attention_aggregate(value, score):\n",
        "  output=score.matmul(value)\n",
        "  return output\n",
        "\n",
        "def step_attention(query, key, value, mem_mask=None):\n",
        "    \"\"\" query[(Bs), B, D], key[B, T, D], value[B, T, D]\"\"\"\n",
        "    score = dot_attention_score(key, query.unsqueeze(-2))\n",
        "    if mem_mask is None:\n",
        "        norm_score = F.softmax(score, dim=-1)\n",
        "    else:\n",
        "        norm_score = prob_normalize(score, mem_mask)\n",
        "    output = attention_aggregate(value, norm_score)\n",
        "    return output.squeeze(-2), norm_score.squeeze(-2)\n",
        "def len_mask(lens, device):\n",
        "    \"\"\" users are resposible for shaping\n",
        "    Return: tensor_type [B, T]\n",
        "    \"\"\"\n",
        "    max_len = max(lens)\n",
        "    batch_size = len(lens)\n",
        "    mask = torch.ByteTensor(batch_size, max_len).to(device)\n",
        "    mask.fill_(0)\n",
        "    for i, l in enumerate(lens):\n",
        "        mask[i, :l].fill_(1)\n",
        "    return mask\n",
        "\n",
        "def sequence_mean(sequence, seq_lens, dim=1):\n",
        "    if seq_lens:\n",
        "        assert sequence.size(0) == len(seq_lens)   # batch_size\n",
        "        sum_ = torch.sum(sequence, dim=dim, keepdim=False)\n",
        "        mean = torch.stack([s/l for s, l in zip(sum_, seq_lens)], dim=0)\n",
        "    else:\n",
        "        mean = torch.mean(sequence, dim=dim, keepdim=False)\n",
        "    return mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWpXs8VCZ5A"
      },
      "source": [
        "INI = 1e-2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BiD9RkMT12g"
      },
      "source": [
        "\r\n",
        "def reorder_sequence(sequence_emb, order, batch_first=False):\r\n",
        "    \"\"\"\r\n",
        "    sequence_emb: [T, B, D] if not batch_first\r\n",
        "    order: list of sequence length\r\n",
        "    \"\"\"\r\n",
        "    batch_dim = 0 if batch_first else 1\r\n",
        "    assert len(order) == sequence_emb.size()[batch_dim]\r\n",
        "\r\n",
        "    order = torch.LongTensor(order).to(sequence_emb.device)\r\n",
        "    sorted_ = sequence_emb.index_select(index=order, dim=batch_dim)\r\n",
        "\r\n",
        "    return sorted_\r\n",
        "\r\n",
        "def reorder_lstm_states(lstm_states, order):\r\n",
        "    \"\"\"\r\n",
        "    lstm_states: (H, C) of tensor [layer, batch, hidden]\r\n",
        "    order: list of sequence length\r\n",
        "    \"\"\"\r\n",
        "    assert isinstance(lstm_states, tuple)\r\n",
        "    assert len(lstm_states) == 2\r\n",
        "    assert lstm_states[0].size() == lstm_states[1].size()\r\n",
        "    assert len(order) == lstm_states[0].size()[1]\r\n",
        "\r\n",
        "    order = torch.LongTensor(order).to(lstm_states[0].device)\r\n",
        "    sorted_states = (lstm_states[0].index_select(index=order, dim=1),\r\n",
        "                     lstm_states[1].index_select(index=order, dim=1))\r\n",
        "\r\n",
        "    return sorted_states\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjvBHftMTrJI"
      },
      "source": [
        "def lstm_encoder(sequence, lstm,\r\n",
        "                 seq_lens=None, init_states=None, embedding=None):\r\n",
        "    \"\"\" functional LSTM encoder (sequence is [b, t]/[b, t, d],\r\n",
        "    lstm should be rolled lstm)\"\"\"\r\n",
        "    batch_size = sequence.size(0)\r\n",
        "    if not lstm.batch_first:\r\n",
        "        sequence = sequence.transpose(0, 1)\r\n",
        "        emb_sequence = (embedding(sequence) if embedding is not None\r\n",
        "                        else sequence)\r\n",
        "    if seq_lens:\r\n",
        "        assert batch_size == len(seq_lens)\r\n",
        "        sort_ind = sorted(range(len(seq_lens)),\r\n",
        "                          key=lambda i: seq_lens[i], reverse=True)\r\n",
        "        seq_lens = [seq_lens[i] for i in sort_ind]\r\n",
        "        emb_sequence = reorder_sequence(emb_sequence, sort_ind,\r\n",
        "                                        lstm.batch_first)\r\n",
        "\r\n",
        "    if init_states is None:\r\n",
        "        device = sequence.device\r\n",
        "        init_states = init_lstm_states(lstm, batch_size, device)\r\n",
        "    else:\r\n",
        "        init_states = (init_states[0].contiguous(),\r\n",
        "                       init_states[1].contiguous())\r\n",
        "\r\n",
        "    if seq_lens:\r\n",
        "        packed_seq = nn.utils.rnn.pack_padded_sequence(emb_sequence,\r\n",
        "                                                       seq_lens)\r\n",
        "        packed_out, final_states = lstm(packed_seq, init_states)\r\n",
        "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out)\r\n",
        "\r\n",
        "        back_map = {ind: i for i, ind in enumerate(sort_ind)}\r\n",
        "        reorder_ind = [back_map[i] for i in range(len(seq_lens))]\r\n",
        "        lstm_out = reorder_sequence(lstm_out, reorder_ind, lstm.batch_first)\r\n",
        "        final_states = reorder_lstm_states(final_states, reorder_ind)\r\n",
        "    else:\r\n",
        "        lstm_out, final_states = lstm(emb_sequence, init_states)\r\n",
        "\r\n",
        "    return lstm_out, final_states\r\n",
        "\r\n",
        "\r\n",
        "def init_lstm_states(lstm, batch_size, device):\r\n",
        "    n_layer = lstm.num_layers*(2 if lstm.bidirectional else 1)\r\n",
        "    n_hidden = lstm.hidden_size\r\n",
        "\r\n",
        "    states = (torch.zeros(n_layer, batch_size, n_hidden).to(device),\r\n",
        "              torch.zeros(n_layer, batch_size, n_hidden).to(device))\r\n",
        "    return states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae5sd8-kxg1o"
      },
      "source": [
        "class ConvSentEncoder(nn.Module):\r\n",
        "    \"\"\"\r\n",
        "    Convolutional word-level sentence encoder\r\n",
        "    w/ max-over-time pooling, [3, 4, 5] kernel sizes, ReLU activation\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, vocab_size, emb_dim, n_hidden, dropout):\r\n",
        "        super().__init__()\r\n",
        "        self._embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\r\n",
        "        self._convs = nn.ModuleList([nn.Conv1d(emb_dim, n_hidden, i)\r\n",
        "                                     for i in range(3, 6)])\r\n",
        "        self._dropout = dropout\r\n",
        "        self._grad_handle = None\r\n",
        "\r\n",
        "    def forward(self, input_):\r\n",
        "        emb_input = self._embedding(input_)\r\n",
        "        conv_in = F.dropout(emb_input.transpose(1, 2),\r\n",
        "                            self._dropout, training=self.training)\r\n",
        "        output = torch.cat([F.relu(conv(conv_in)).max(dim=2)[0]\r\n",
        "                            for conv in self._convs], dim=1)\r\n",
        "        return output\r\n",
        "\r\n",
        "    def set_embedding(self, embedding):\r\n",
        "        \"\"\"embedding is the weight matrix\"\"\"\r\n",
        "        assert self._embedding.weight.size() == embedding.size()\r\n",
        "        self._embedding.weight.data.copy_(embedding)\r\n",
        "\r\n",
        "\r\n",
        "class LSTMEncoder(nn.Module):\r\n",
        "    def __init__(self, input_dim, n_hidden, n_layer, dropout, bidirectional):\r\n",
        "        super().__init__()\r\n",
        "        self._init_h = nn.Parameter(\r\n",
        "            torch.Tensor(n_layer*(2 if bidirectional else 1), n_hidden))\r\n",
        "        self._init_c = nn.Parameter(\r\n",
        "            torch.Tensor(n_layer*(2 if bidirectional else 1), n_hidden))\r\n",
        "        init.uniform_(self._init_h, -INI, INI)\r\n",
        "        init.uniform_(self._init_c, -INI, INI)\r\n",
        "        self._lstm = nn.LSTM(input_dim, n_hidden, n_layer,\r\n",
        "                             dropout=dropout, bidirectional=bidirectional)\r\n",
        "\r\n",
        "    def forward(self, input_, in_lens=None):\r\n",
        "        \"\"\" [batch_size, max_num_sent, input_dim] Tensor\"\"\"\r\n",
        "        size = (self._init_h.size(0), input_.size(0), self._init_h.size(1))\r\n",
        "        init_states = (self._init_h.unsqueeze(1).expand(*size),\r\n",
        "                       self._init_c.unsqueeze(1).expand(*size))\r\n",
        "        lstm_out, _ = lstm_encoder(\r\n",
        "            input_, self._lstm, in_lens, init_states)\r\n",
        "        return lstm_out.transpose(0, 1)\r\n",
        "\r\n",
        "    @property\r\n",
        "    def input_size(self):\r\n",
        "        return self._lstm.input_size\r\n",
        "\r\n",
        "    @property\r\n",
        "    def hidden_size(self):\r\n",
        "        return self._lstm.hidden_size\r\n",
        "\r\n",
        "    @property\r\n",
        "    def num_layers(self):\r\n",
        "        return self._lstm.num_layers\r\n",
        "\r\n",
        "    @property\r\n",
        "    def bidirectional(self):\r\n",
        "        return self._lstm.bidirectional\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGo3_Pp5Cmup"
      },
      "source": [
        "\n",
        "class LSTMPointerNet(nn.Module):\n",
        "    \"\"\"Pointer network as in Vinyals et al \"\"\"\n",
        "    def __init__(self, input_dim, n_hidden, n_layer,\n",
        "                 dropout, n_hop):\n",
        "        super().__init__()\n",
        "        self._init_h = nn.Parameter(torch.Tensor(n_layer, n_hidden))\n",
        "        self._init_c = nn.Parameter(torch.Tensor(n_layer, n_hidden))\n",
        "        self._init_i = nn.Parameter(torch.Tensor(input_dim))\n",
        "        init.uniform_(self._init_h, -INI, INI)\n",
        "        init.uniform_(self._init_c, -INI, INI)\n",
        "        init.uniform_(self._init_i, -0.1, 0.1)\n",
        "        self._lstm = nn.LSTM(\n",
        "            input_dim, n_hidden, n_layer,\n",
        "            bidirectional=False, dropout=dropout\n",
        "        )\n",
        "        self._lstm_cell = None\n",
        "\n",
        "        # attention parameters\n",
        "        self._attn_wm = nn.Parameter(torch.Tensor(input_dim, n_hidden))\n",
        "        self._attn_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
        "        self._attn_v = nn.Parameter(torch.Tensor(n_hidden))\n",
        "        init.xavier_normal_(self._attn_wm)\n",
        "        init.xavier_normal_(self._attn_wq)\n",
        "        init.uniform_(self._attn_v, -INI, INI)\n",
        "\n",
        "        # hop parameters\n",
        "        self._hop_wm = nn.Parameter(torch.Tensor(input_dim, n_hidden))\n",
        "        self._hop_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
        "        self._hop_v = nn.Parameter(torch.Tensor(n_hidden))\n",
        "        init.xavier_normal_(self._hop_wm)\n",
        "        init.xavier_normal_(self._hop_wq)\n",
        "        init.uniform_(self._hop_v, -INI, INI)\n",
        "        self._n_hop = n_hop\n",
        "\n",
        "    def forward(self, attn_mem, mem_sizes, lstm_in):\n",
        "        \"\"\"atten_mem: Tensor of size [batch_size, max_sent_num, input_dim]\"\"\"\n",
        "        attn_feat, hop_feat, lstm_states, init_i = self._prepare(attn_mem)\n",
        "        lstm_in = torch.cat([init_i, lstm_in], dim=1).transpose(0, 1)\n",
        "        query, final_states = self._lstm(lstm_in, lstm_states)\n",
        "        query = query.transpose(0, 1)\n",
        "        for _ in range(self._n_hop):\n",
        "            query = LSTMPointerNet.attention(\n",
        "                hop_feat, query, self._hop_v, self._hop_wq, mem_sizes)\n",
        "        output = LSTMPointerNet.attention_score(\n",
        "            attn_feat, query, self._attn_v, self._attn_wq)\n",
        "        return output  # unormalized extraction logit\n",
        "\n",
        "    def extract(self, attn_mem, mem_sizes, k):\n",
        "        \"\"\"extract k sentences, decode only, batch_size==1\"\"\"\n",
        "        attn_feat, hop_feat, lstm_states, lstm_in = self._prepare(attn_mem)\n",
        "        lstm_in = lstm_in.squeeze(1)\n",
        "        if self._lstm_cell is None:\n",
        "            self._lstm_cell = MultiLayerLSTMCells.convert(\n",
        "                self._lstm).to(attn_mem.device)\n",
        "        extracts = []\n",
        "        for _ in range(k):\n",
        "            h, c = self._lstm_cell(lstm_in, lstm_states)\n",
        "            query = h[-1]\n",
        "            for _ in range(self._n_hop):\n",
        "                query = LSTMPointerNet.attention(\n",
        "                    hop_feat, query, self._hop_v, self._hop_wq, mem_sizes)\n",
        "            score = LSTMPointerNet.attention_score(\n",
        "                attn_feat, query, self._attn_v, self._attn_wq)\n",
        "            score = score.squeeze()\n",
        "            for e in extracts:\n",
        "                score[e] = -1e6\n",
        "            ext = score.max(dim=0)[1].item()\n",
        "            extracts.append(ext)\n",
        "            lstm_states = (h, c)\n",
        "            lstm_in = attn_mem[:, ext, :]\n",
        "        return extracts\n",
        "\n",
        "    def _prepare(self, attn_mem):\n",
        "        attn_feat = torch.matmul(attn_mem, self._attn_wm.unsqueeze(0))\n",
        "        hop_feat = torch.matmul(attn_mem, self._hop_wm.unsqueeze(0))\n",
        "        bs = attn_mem.size(0)\n",
        "        n_l, d = self._init_h.size()\n",
        "        size = (n_l, bs, d)\n",
        "        lstm_states = (self._init_h.unsqueeze(1).expand(*size).contiguous(),\n",
        "                       self._init_c.unsqueeze(1).expand(*size).contiguous())\n",
        "        d = self._init_i.size(0)\n",
        "        init_i = self._init_i.unsqueeze(0).unsqueeze(1).expand(bs, 1, d)\n",
        "        return attn_feat, hop_feat, lstm_states, init_i\n",
        "\n",
        "    @staticmethod\n",
        "    def attention_score(attention, query, v, w):\n",
        "        \"\"\" unnormalized attention score\"\"\"\n",
        "        sum_ = attention.unsqueeze(1) + torch.matmul(\n",
        "            query, w.unsqueeze(0)\n",
        "        ).unsqueeze(2)  # [B, Nq, Ns, D]\n",
        "        score = torch.matmul(\n",
        "            F.tanh(sum_), v.unsqueeze(0).unsqueeze(1).unsqueeze(3)\n",
        "        ).squeeze(3)  # [B, Nq, Ns]\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(attention, query, v, w, mem_sizes):\n",
        "        \"\"\" attention context vector\"\"\"\n",
        "        score = LSTMPointerNet.attention_score(attention, query, v, w)\n",
        "        if mem_sizes is None:\n",
        "            norm_score = F.softmax(score, dim=-1)\n",
        "        else:\n",
        "            mask = len_mask(mem_sizes, score.device).unsqueeze(-2)\n",
        "            norm_score = prob_normalize(score, mask)\n",
        "        output = torch.matmul(norm_score, attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PtrExtractSumm(nn.Module):\n",
        "    \"\"\" rnn-ext\"\"\"\n",
        "    def __init__(self, emb_dim, vocab_size, conv_hidden,\n",
        "                 lstm_hidden, lstm_layer, bidirectional,\n",
        "                 n_hop=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self._sent_enc = ConvSentEncoder(\n",
        "            vocab_size, emb_dim, conv_hidden, dropout)\n",
        "        self._art_enc = LSTMEncoder(\n",
        "            3*conv_hidden, lstm_hidden, lstm_layer,\n",
        "            dropout=dropout, bidirectional=bidirectional\n",
        "        )\n",
        "        enc_out_dim = lstm_hidden * (2 if bidirectional else 1)\n",
        "        self._extractor = LSTMPointerNet(\n",
        "            enc_out_dim, lstm_hidden, lstm_layer,\n",
        "            dropout, n_hop\n",
        "        )\n",
        "\n",
        "    def forward(self, article_sents, sent_nums, target):\n",
        "        enc_out = self._encode(article_sents, sent_nums)\n",
        "        bs, nt = target.size()\n",
        "        d = enc_out.size(2)\n",
        "        ptr_in = torch.gather(\n",
        "            enc_out, dim=1, index=target.unsqueeze(2).expand(bs, nt, d)\n",
        "        )\n",
        "        output = self._extractor(enc_out, sent_nums, ptr_in)\n",
        "        return output\n",
        "\n",
        "    def extract(self, article_sents, sent_nums=None, k=4):\n",
        "        enc_out = self._encode(article_sents, sent_nums)\n",
        "        output = self._extractor.extract(enc_out, sent_nums, k)\n",
        "        return output\n",
        "\n",
        "    def _encode(self, article_sents, sent_nums):\n",
        "        if sent_nums is None:  # test-time excode only\n",
        "            enc_sent = self._sent_enc(article_sents[0]).unsqueeze(0)\n",
        "        else:\n",
        "            max_n = max(sent_nums)\n",
        "            enc_sents = [self._sent_enc(art_sent)\n",
        "                         for art_sent in article_sents]\n",
        "            def zero(n, device):\n",
        "                z = torch.zeros(n, self._art_enc.input_size).to(device)\n",
        "                return z\n",
        "            enc_sent = torch.stack(\n",
        "                [torch.cat([s, zero(max_n-n, s.device)], dim=0)\n",
        "                   if n != max_n\n",
        "                 else s\n",
        "                 for s, n in zip(enc_sents, sent_nums)],\n",
        "                dim=0\n",
        "            )\n",
        "        lstm_out = self._art_enc(enc_sent, sent_nums)\n",
        "        return lstm_out\n",
        "\n",
        "    def set_embedding(self, embedding):\n",
        "        self._sent_enc.set_embedding(embedding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGd-fBrVC2fA"
      },
      "source": [
        "def sequence_loss(logits,targets,xent_fn=None,pad_idx=0):\n",
        "  assert logits.size()[:-1]==targets.size()\n",
        "  mask=targets!=pad_idx\n",
        "  target=targets.masked_select(mask)\n",
        "  logit=logits.masked_select(mask.unsqueeze(2).expand_as(logits)).contiguous().view(-1,logits.size(-1))\n",
        "  if xent_fn:\n",
        "    loss=xent_fn(logit,target)\n",
        "  else:\n",
        "    loss=F.cross_entropy(logit,target)\n",
        "  assert (not math.isnan(loss.mean().item()))\n",
        "  return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmSDCTi8DBaj"
      },
      "source": [
        "def get_basic_grad_fn(net, clip_grad, max_grad=1e2):\n",
        "    def f():\n",
        "        grad_norm = clip_grad_norm_(\n",
        "            [p for p in net.parameters() if p.requires_grad], clip_grad)\n",
        "        grad_norm = grad_norm.item()\n",
        "        if max_grad is not None and grad_norm >= max_grad:\n",
        "            print('WARNING: Exploding Gradients {:.2f}'.format(grad_norm))\n",
        "            grad_norm = max_grad\n",
        "        grad_log = {}\n",
        "        grad_log['grad_norm'] = grad_norm\n",
        "        return grad_log\n",
        "    return f\n",
        "\n",
        "@curry\n",
        "def compute_loss(net, criterion, fw_args, loss_args):\n",
        "    loss = criterion(*((net(*fw_args),) + loss_args))\n",
        "    return loss\n",
        "\n",
        "@curry\n",
        "def val_step(loss_step, fw_args, loss_args):\n",
        "    loss = loss_step(fw_args, loss_args)\n",
        "    return loss.size(0), loss.sum().item()\n",
        "\n",
        "@curry\n",
        "def basic_validate(net, criterion, val_batches):\n",
        "    print('running validation ... ', end='')\n",
        "    net.eval()\n",
        "    start = time()\n",
        "    with torch.no_grad():\n",
        "        validate_fn = val_step(compute_loss(net, criterion))\n",
        "        n_data, tot_loss = reduce(\n",
        "            lambda a, b: (a[0]+b[0], a[1]+b[1]),\n",
        "            starmap(validate_fn, val_batches),\n",
        "            (0, 0)\n",
        "        )\n",
        "    val_loss = tot_loss / n_data\n",
        "    print(\n",
        "        'validation finished in {}                                    '.format(\n",
        "            timedelta(seconds=int(time()-start)))\n",
        "    )\n",
        "    print('validation loss: {:.4f} ... '.format(val_loss))\n",
        "    return {'loss': val_loss}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ZAIJx7DSun"
      },
      "source": [
        "class BasicPipeline(object):\n",
        "    def __init__(self, name, net,\n",
        "                 train_batcher, val_batcher, batch_size,\n",
        "                 val_fn, criterion, optim, grad_fn=None):\n",
        "        self.name = name\n",
        "        self._net = net\n",
        "        self._train_batcher = train_batcher\n",
        "        self._val_batcher = val_batcher\n",
        "        self._criterion = criterion\n",
        "        self._opt = optim\n",
        "        # grad_fn is calleble without input args that modifyies gradient\n",
        "        # it should return a dictionary of logging values\n",
        "        self._grad_fn = grad_fn\n",
        "        self._val_fn = val_fn\n",
        "\n",
        "        self._n_epoch = 0  # epoch not very useful?\n",
        "        self._batch_size = batch_size\n",
        "        self._batches = self.batches()\n",
        "\n",
        "    def batches(self):\n",
        "        while True:\n",
        "            for fw_args, bw_args in self._train_batcher(self._batch_size):\n",
        "                yield fw_args, bw_args\n",
        "            self._n_epoch += 1\n",
        "\n",
        "    def get_loss_args(self, net_out, bw_args):\n",
        "        if isinstance(net_out, tuple):\n",
        "            loss_args = net_out + bw_args\n",
        "        else:\n",
        "            loss_args = (net_out, ) + bw_args\n",
        "        return loss_args\n",
        "\n",
        "    def train_step(self):\n",
        "        # forward pass of model\n",
        "        self._net.train()\n",
        "        fw_args, bw_args = next(self._batches)\n",
        "        net_out = self._net(*fw_args)\n",
        "\n",
        "        # get logs and output for logging, backward\n",
        "        log_dict = {}\n",
        "        loss_args = self.get_loss_args(net_out, bw_args)\n",
        "\n",
        "        # backward and update ( and optional gradient monitoring )\n",
        "        loss = self._criterion(*loss_args).mean()\n",
        "        loss.backward()\n",
        "        log_dict['loss'] = loss.item()\n",
        "        if self._grad_fn is not None:\n",
        "            log_dict.update(self._grad_fn())\n",
        "        self._opt.step()\n",
        "        self._net.zero_grad()\n",
        "\n",
        "        return log_dict\n",
        "\n",
        "    def validate(self):\n",
        "        return self._val_fn(self._val_batcher(self._batch_size))\n",
        "\n",
        "    def checkpoint(self, save_path, step, val_metric=None):\n",
        "        save_dict = {}\n",
        "        if val_metric is not None:\n",
        "            name = 'ckpt-{:6f}-{}'.format(val_metric, step)\n",
        "            save_dict['val_metric'] = val_metric\n",
        "        else:\n",
        "            name = 'ckpt-{}'.format(step)\n",
        "\n",
        "        save_dict['state_dict'] = self._net.state_dict()\n",
        "        save_dict['optimizer'] = self._opt.state_dict()\n",
        "        torch.save(save_dict, join(save_path, name))\n",
        "        print(\"Check point saved to \",join(save_path,name))\n",
        "\n",
        "    def terminate(self):\n",
        "        self._train_batcher.terminate()\n",
        "        self._val_batcher.terminate()\n",
        "\n",
        "\n",
        "class BasicTrainer(object):\n",
        "    \"\"\" Basic trainer with minimal function and early stopping\"\"\"\n",
        "    def __init__(self, pipeline, save_dir, ckpt_freq, patience,\n",
        "                 scheduler=None, val_mode='loss'):\n",
        "        assert isinstance(pipeline, BasicPipeline)\n",
        "        assert val_mode in ['loss', 'score']\n",
        "        self._pipeline = pipeline\n",
        "        self._save_dir = save_dir\n",
        "        self._logger = tensorboardX.SummaryWriter(join(save_dir, 'log'))\n",
        "        ckpt_dir=join(save_dir,'extract_ckpt')\n",
        "        try:\n",
        "          os.makedirs(join(ckpt_dir, 'ckpt'))\n",
        "        except:\n",
        "          print(\"Path already present\")\n",
        "        self._ckpt_freq = ckpt_freq\n",
        "        self._patience = patience\n",
        "        self._sched = scheduler\n",
        "        self._val_mode = val_mode\n",
        "\n",
        "        self._step = 0\n",
        "        self._running_loss = None\n",
        "        # state vars for early stopping\n",
        "        self._current_p = 0\n",
        "        self._best_val = None\n",
        "\n",
        "    def log(self, log_dict):\n",
        "        loss = log_dict['loss'] if 'loss' in log_dict else log_dict['reward']\n",
        "        if self._running_loss is not None:\n",
        "            self._running_loss = 0.99*self._running_loss + 0.01*loss\n",
        "        else:\n",
        "            self._running_loss = loss\n",
        "        print('train step: {}, {}: {:.4f}\\r'.format(\n",
        "            self._step,\n",
        "            'loss' if 'loss' in log_dict else 'reward',\n",
        "            self._running_loss), end='')\n",
        "        for key, value in log_dict.items():\n",
        "            self._logger.add_scalar(\n",
        "                '{}_{}'.format(key, self._pipeline.name), value, self._step)\n",
        "\n",
        "    def validate(self):\n",
        "        print()\n",
        "        val_log = self._pipeline.validate()\n",
        "        for key, value in val_log.items():\n",
        "            self._logger.add_scalar(\n",
        "                'val_{}_{}'.format(key, self._pipeline.name),\n",
        "                value, self._step\n",
        "            )\n",
        "        if 'reward' in val_log:\n",
        "            val_metric = val_log['reward']\n",
        "        else:\n",
        "            val_metric = (val_log['loss'] if self._val_mode == 'loss'\n",
        "                          else val_log['score'])\n",
        "        return val_metric\n",
        "\n",
        "    def checkpoint(self):\n",
        "        val_metric = self.validate()\n",
        "        self._pipeline.checkpoint(\n",
        "            join(self._save_dir, 'extract_dir/ckpt'), self._step, val_metric)\n",
        "        if isinstance(self._sched, ReduceLROnPlateau):\n",
        "            self._sched.step(val_metric)\n",
        "        else:\n",
        "            self._sched.step()\n",
        "        stop = self.check_stop(val_metric)\n",
        "        return stop\n",
        "\n",
        "    def check_stop(self, val_metric):\n",
        "        if self._best_val is None:\n",
        "            self._best_val = val_metric\n",
        "        elif ((val_metric < self._best_val and self._val_mode == 'loss')\n",
        "              or (val_metric > self._best_val and self._val_mode == 'score')):\n",
        "            self._current_p = 0\n",
        "            self._best_val = val_metric\n",
        "        else:\n",
        "            self._current_p += 1\n",
        "        return self._current_p >= self._patience\n",
        "\n",
        "    def train(self):\n",
        "        try:\n",
        "            start = time()\n",
        "            print('Start training')\n",
        "            while True:\n",
        "                log_dict = self._pipeline.train_step()\n",
        "                self._step += 1\n",
        "                self.log(log_dict)\n",
        "\n",
        "                if self._step % self._ckpt_freq == 0:\n",
        "                    stop = self.checkpoint()\n",
        "                    if stop:\n",
        "                        break\n",
        "            print('Training finised in ', timedelta(seconds=time()-start))\n",
        "        finally:\n",
        "            self._pipeline.terminate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scnSbcctDiJn"
      },
      "source": [
        "PAD = 0\n",
        "UNK = 1\n",
        "START = 2\n",
        "END = 3\n",
        "BUCKET_SIZE=1000\n",
        "\n",
        "DATA_DIR = r\"/content/drive/My Drive/finsummary/Data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOd9OV4GD5xh"
      },
      "source": [
        "\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "START = 2\n",
        "END = 3\n",
        "def make_vocab(wc, vocab_size):\n",
        "    word2id, id2word = {}, {}\n",
        "    word2id['<pad>'] = PAD\n",
        "    word2id['<unk>'] = UNK\n",
        "    word2id['<start>'] = START\n",
        "    word2id['<end>'] = END\n",
        "    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):\n",
        "        word2id[w] = i\n",
        "    return word2id\n",
        "\n",
        "\n",
        "def make_embedding(id2word, w2v_file, initializer=None):\n",
        "    attrs = basename(w2v_file).split('.')  #word2vec.{dim}d.{vsize}k.bin\n",
        "    w2v = gensim.models.Word2Vec.load(w2v_file).wv\n",
        "    vocab_size = len(id2word)\n",
        "    emb_dim = int(attrs[-3][:-1])\n",
        "    embedding = nn.Embedding(vocab_size, emb_dim).weight\n",
        "    if initializer is not None:\n",
        "        initializer(embedding)\n",
        "\n",
        "    oovs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(id2word)):\n",
        "            # NOTE: id2word can be list or dict\n",
        "            if i == START:\n",
        "                embedding[i, :] = torch.Tensor(w2v['<s>'])\n",
        "            elif i == END:\n",
        "                embedding[i, :] = torch.Tensor(w2v['<\\\\s>'])\n",
        "            elif id2word[i] in w2v:\n",
        "                embedding[i, :] = torch.Tensor(w2v[id2word[i]])\n",
        "            else:\n",
        "                oovs.append(i)\n",
        "    return embedding, oovs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14-NZMs4xOOy"
      },
      "source": [
        "import sys\n",
        "class CnnDmDataset(Dataset):\n",
        "    def __init__(self, split: str, path: str) -> None:\n",
        "        assert split in ['training', 'validation', 'test']\n",
        "        self._data_path = join(path, split)\n",
        "        self._n_data = _count_data(self._data_path)\n",
        "        self._json_files=os.listdir(join(self._data_path,'extraction'))\n",
        "        self._json_files=list(filter(lambda x: x.endswith('json'),self._json_files))\n",
        "        self._idx_list=[]\n",
        "        for i in range(len(self._json_files)):\n",
        "          self._idx_list.append(self._json_files[i].split('.')[0])\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._json_files)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        idx=self._idx_list[i]\n",
        "        js_path=join(self._data_path,'extraction')\n",
        "        try:\n",
        "          with open(join(js_path, '{}.json'.format(idx)),'r') as f:\n",
        "              js = json.loads(f.read())\n",
        "        except:\n",
        "          print(join(js_path, '{}.json'.format(idx)))\n",
        "          sys.exit()\n",
        "        report_name=idx.split('.')[0].split()[0]+'.txt'\n",
        "        article_path=join(self._data_path,'annual_reports')\n",
        "        with open(join(article_path,report_name),encoding='utf8') as f:\n",
        "          article_file=f.read()\n",
        "        article_sentences=[]\n",
        "        for sent in tokenizer.tokenize(article_file):\n",
        "          sent=sent.replace('\\n',' ')\n",
        "          article_sentences.append(sent)\n",
        "        js['article']=article_sentences\n",
        "        return js\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J61OcmRtFuWy"
      },
      "source": [
        "\n",
        "def coll_fn_extract(data):\n",
        "    def is_good_data(d):\n",
        "        \"\"\" make sure data is not empty\"\"\"\n",
        "        source_sents, extracts = d\n",
        "        return source_sents and extracts\n",
        "    batch = list(filter(is_good_data, data))\n",
        "    assert all(map(is_good_data, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def tokenize(max_len, texts):\n",
        "    return [t.lower().split()[:max_len] for t in texts]\n",
        "\n",
        "def conver2id(unk, word2id, words_list):\n",
        "    word2id = defaultdict(lambda: unk, word2id)\n",
        "    return [[word2id[w] for w in words] for words in words_list]\n",
        "\n",
        "\n",
        "@curry\n",
        "def prepro_fn_extract(max_src_len, max_src_num, batch):\n",
        "    def prepro_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        tokenized_sents = tokenize(max_src_len, source_sents)[:max_src_num]\n",
        "        cleaned_extracts = list(filter(lambda e: e < len(tokenized_sents),\n",
        "                                       extracts))\n",
        "        return tokenized_sents, cleaned_extracts\n",
        "    batch = list(map(prepro_one, batch))\n",
        "    return batch\n",
        "\n",
        "\n",
        "@curry\n",
        "def convert_batch_extract_ptr(unk, word2id, batch):\n",
        "    def convert_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        id_sents = conver2id(unk, word2id, source_sents)\n",
        "        return id_sents, extracts\n",
        "    batch = list(map(convert_one, batch))\n",
        "    return batch\n",
        "\n",
        "@curry#used\n",
        "def pad_batch_tensorize(inputs, pad, cuda=True):\n",
        "    \"\"\"pad_batch_tensorize\n",
        "\n",
        "    :param inputs: List of size B containing torch tensors of shape [T, ...]\n",
        "    :type inputs: List[np.ndarray]\n",
        "    :rtype: TorchTensor of size (B, T, ...)\n",
        "    \"\"\"\n",
        "    tensor_type = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "    batch_size = len(inputs)\n",
        "    max_len = max(len(ids) for ids in inputs)\n",
        "    tensor_shape = (batch_size, max_len)\n",
        "    tensor = tensor_type(*tensor_shape)\n",
        "    tensor.fill_(pad)\n",
        "    for i, ids in enumerate(inputs):\n",
        "        tensor[i, :len(ids)] = tensor_type(ids)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "@curry#used\n",
        "def batchify_fn_extract_ptr(pad, data, cuda=True):\n",
        "    source_lists, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_nums = list(map(len, source_lists))\n",
        "    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n",
        "\n",
        "    # PAD is -1 (dummy extraction index) for using sequence loss\n",
        "    target = pad_batch_tensorize(targets, pad=-1, cuda=cuda)\n",
        "    remove_last = lambda tgt: tgt[:-1]\n",
        "    tar_in = pad_batch_tensorize(\n",
        "        list(map(remove_last, targets)),\n",
        "        pad=-0, cuda=cuda # use 0 here for feeding first conv sentence repr.\n",
        "    )\n",
        "\n",
        "    fw_args = (sources, src_nums, tar_in)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "def _batch2q(loader, prepro, q, single_run=True):\n",
        "    epoch = 0\n",
        "    while True:\n",
        "        for batch in loader:\n",
        "            q.put(prepro(batch))\n",
        "        if single_run:\n",
        "            break\n",
        "        epoch += 1\n",
        "        q.put(epoch)\n",
        "    q.put(None)\n",
        "\n",
        "class BucketedGenerater(object):\n",
        "    def __init__(self, loader, prepro,\n",
        "                 sort_key, batchify,\n",
        "                 single_run=True, queue_size=8, fork=True):\n",
        "        self._loader = loader\n",
        "        self._prepro = prepro\n",
        "        self._sort_key = sort_key\n",
        "        self._batchify = batchify\n",
        "        self._single_run = single_run\n",
        "        if fork:\n",
        "            ctx = mp.get_context('forkserver')\n",
        "            self._queue = ctx.Queue(queue_size)\n",
        "        else:\n",
        "            # for easier debugging\n",
        "            self._queue = None\n",
        "        self._process = None\n",
        "\n",
        "    def __call__(self, batch_size: int):\n",
        "        def get_batches(hyper_batch):\n",
        "            indexes = list(range(0, len(hyper_batch), batch_size))\n",
        "            if not self._single_run:\n",
        "                # random shuffle for training batches\n",
        "                random.shuffle(hyper_batch)\n",
        "                random.shuffle(indexes)\n",
        "            hyper_batch.sort(key=self._sort_key)\n",
        "            for i in indexes:\n",
        "                batch = self._batchify(hyper_batch[i:i+batch_size])\n",
        "                yield batch\n",
        "\n",
        "        if self._queue is not None:\n",
        "            ctx = mp.get_context('forkserver')\n",
        "            self._process = ctx.Process(\n",
        "                target=_batch2q,\n",
        "                args=(self._loader, self._prepro,\n",
        "                      self._queue, self._single_run)\n",
        "            )\n",
        "            self._process.start()\n",
        "            while True:\n",
        "                d = self._queue.get()\n",
        "                if d is None:\n",
        "                    break\n",
        "                if isinstance(d, int):\n",
        "                    print('\\nepoch {} done'.format(d))\n",
        "                    continue\n",
        "                yield from get_batches(d)\n",
        "            self._process.join()\n",
        "        else:\n",
        "            i = 0\n",
        "            while True:\n",
        "                for batch in self._loader:\n",
        "                    yield from get_batches(self._prepro(batch))\n",
        "                if self._single_run:\n",
        "                    break\n",
        "                i += 1\n",
        "                print('\\nepoch {} done'.format(i))\n",
        "\n",
        "    def terminate(self):\n",
        "        if self._process is not None:\n",
        "            self._process.terminate()\n",
        "            self._process.join()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlWbMrIVF_YZ"
      },
      "source": [
        "class ExtractDataset(CnnDmDataset):\n",
        "    \"\"\" article sentences -> extraction indices\n",
        "    (dataset created by greedily matching ROUGE)\n",
        "    \"\"\"\n",
        "    def __init__(self, split):\n",
        "        super().__init__(split, DATA_DIR)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        js_data = super().__getitem__(i)\n",
        "        art_sents, extracts = js_data['article'], js_data['extracted_labels']\n",
        "        return art_sents, extracts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3mA8Cq7YZSD"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI4j7JiAVZZM"
      },
      "source": [
        "w2v_bin_path='/content/drive/My Drive/finsummary/Data/word2vec.300d.629k.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEbZi6W2thF2"
      },
      "source": [
        "def configure_net(net_type, vocab_size, emb_dim, conv_hidden,\r\n",
        "                  lstm_hidden, lstm_layer, bidirectional):\r\n",
        "    assert net_type in ['ff', 'rnn']\r\n",
        "    net_args = {}\r\n",
        "    net_args['vocab_size']    = vocab_size\r\n",
        "    net_args['emb_dim']       = emb_dim\r\n",
        "    net_args['conv_hidden']   = conv_hidden\r\n",
        "    net_args['lstm_hidden']   = lstm_hidden\r\n",
        "    net_args['lstm_layer']    = lstm_layer\r\n",
        "    net_args['bidirectional'] = bidirectional\r\n",
        "\r\n",
        "    net = (ExtractSumm(**net_args) if net_type == 'ff'\r\n",
        "           else PtrExtractSumm(**net_args))\r\n",
        "    return net, net_args\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MElarSvBXaSJ"
      },
      "source": [
        "def build_batchers(net_type, word2id, cuda, debug):\r\n",
        "    assert net_type in ['ff', 'rnn']\r\n",
        "    prepro = prepro_fn_extract(args.max_word, args.max_sent)\r\n",
        "    def sort_key(sample):\r\n",
        "        src_sents, _ = sample\r\n",
        "        return len(src_sents)\r\n",
        "    batchify_fn = (batchify_fn_extract_ff if net_type == 'ff'\r\n",
        "                   else batchify_fn_extract_ptr)\r\n",
        "    convert_batch = (convert_batch_extract_ff if net_type == 'ff'\r\n",
        "                     else convert_batch_extract_ptr)\r\n",
        "    batchify = compose(batchify_fn(PAD, cuda=cuda),\r\n",
        "                       convert_batch(UNK, word2id))\r\n",
        "\r\n",
        "    train_loader = DataLoader(\r\n",
        "        ExtractDataset('train'), batch_size=BUCKET_SIZE,\r\n",
        "        shuffle=not debug,\r\n",
        "        num_workers=4 if cuda and not debug else 0,\r\n",
        "        collate_fn=coll_fn_extract\r\n",
        "    )\r\n",
        "    train_batcher = BucketedGenerater(train_loader, prepro, sort_key, batchify,\r\n",
        "                                      single_run=False, fork=not debug)\r\n",
        "\r\n",
        "    val_loader = DataLoader(\r\n",
        "        ExtractDataset('val'), batch_size=BUCKET_SIZE,\r\n",
        "        shuffle=False, num_workers=4 if cuda and not debug else 0,\r\n",
        "        collate_fn=coll_fn_extract\r\n",
        "    )\r\n",
        "    val_batcher = BucketedGenerater(val_loader, prepro, sort_key, batchify,\r\n",
        "                                    single_run=True, fork=not debug)\r\n",
        "    return train_batcher, val_batcher"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1RMO1-Nqh0v"
      },
      "source": [
        "\r\n",
        "def configure_training(net_type, opt, lr, clip_grad, lr_decay, batch_size):\r\n",
        "    \"\"\" supports Adam optimizer only\"\"\"\r\n",
        "    assert opt in ['adam']\r\n",
        "    assert net_type in ['ff', 'rnn']\r\n",
        "    opt_kwargs = {}\r\n",
        "    opt_kwargs['lr'] = lr\r\n",
        "\r\n",
        "    train_params = {}\r\n",
        "    train_params['optimizer']      = (opt, opt_kwargs)\r\n",
        "    train_params['clip_grad_norm'] = clip_grad\r\n",
        "    train_params['batch_size']     = batch_size\r\n",
        "    train_params['lr_decay']       = lr_decay\r\n",
        "\r\n",
        "    if net_type == 'ff':\r\n",
        "        criterion = lambda logit, target: F.binary_cross_entropy_with_logits(\r\n",
        "            logit, target, reduce=False)\r\n",
        "    else:\r\n",
        "        ce = lambda logit, target: F.cross_entropy(logit, target, reduce=False)\r\n",
        "        def criterion(logits, targets):\r\n",
        "            return sequence_loss(logits, targets, ce, pad_idx=-1)\r\n",
        "\r\n",
        "    return criterion, train_params\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDiAAL6ZX5IB"
      },
      "source": [
        "def main(args):\n",
        "    assert args.net_type in ['ff', 'rnn']\n",
        "    # create data batcher, vocabulary\n",
        "    # batcher\n",
        "    with open(join(DATA_DIR, 'vocab_cnt.pkl'), 'rb') as f:\n",
        "        wc = pkl.load(f)\n",
        "    word2id = make_vocab(wc, args.vsize)\n",
        "    train_batcher, val_batcher = build_batchers(args.net_type, word2id,\n",
        "                                                args.cuda, args.debug)\n",
        "\n",
        "    # make net\n",
        "    net, net_args = configure_net(args.net_type,\n",
        "                                  len(word2id), args.emb_dim, args.conv_hidden,\n",
        "                                  args.lstm_hidden, args.lstm_layer, args.bi)\n",
        "    if args.w2v:\n",
        "        # NOTE: the pretrained embedding having the same dimension\n",
        "        #       as args.emb_dim should already be trained\n",
        "        embedding, _ = make_embedding(\n",
        "            {i: w for w, i in word2id.items()}, args.w2v)\n",
        "        net.set_embedding(embedding)\n",
        "\n",
        "    # configure training setting\n",
        "    criterion, train_params = configure_training(\n",
        "        args.net_type, 'adam', args.lr, args.clip, args.decay, args.batch\n",
        "    )\n",
        "\n",
        "    # save experiment setting\n",
        "    if not exists(args.path):\n",
        "        os.makedirs(args.path)\n",
        "    with open(join(args.path, 'vocab.pkl'), 'wb') as f:\n",
        "        pkl.dump(word2id, f, pkl.HIGHEST_PROTOCOL)\n",
        "    meta = {}\n",
        "    meta['net']           = 'ml_{}_extractor'.format(args.net_type)\n",
        "    meta['net_args']      = net_args\n",
        "    meta['traing_params'] = train_params\n",
        "    with open(join(args.path, 'meta.json'), 'w') as f:\n",
        "        json.dump(meta, f, indent=4)\n",
        "\n",
        "    # prepare trainer\n",
        "    val_fn = basic_validate(net, criterion)\n",
        "    grad_fn = get_basic_grad_fn(net, args.clip)\n",
        "    optimizer = optim.Adam(net.parameters(), **train_params['optimizer'][1])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True,\n",
        "                                  factor=args.decay, min_lr=0,\n",
        "                                  patience=args.lr_p)\n",
        "\n",
        "    if args.cuda:\n",
        "        net = net.cuda()\n",
        "    pipeline = BasicPipeline(meta['net'], net,\n",
        "                             train_batcher, val_batcher, args.batch, val_fn,\n",
        "                             criterion, optimizer, grad_fn)\n",
        "    trainer = BasicTrainer(pipeline, args.path,\n",
        "                           args.ckpt_freq, args.patience, scheduler)\n",
        "\n",
        "    print('start training with the following hyper-parameters:')\n",
        "    print(meta)\n",
        "    trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4iBDPphTmoj"
      },
      "source": [
        "class Args():\n",
        "  path=DATA_DIR\n",
        "  net_type='rnn'\n",
        "\n",
        "  vsize=20000\n",
        "                          \n",
        "  emb_dim=300\n",
        "                        \n",
        "  w2v=w2v_bin_path\n",
        "\n",
        "  n_hidden=256\n",
        "  lstm_layer=1            \n",
        "  n_layer=2\n",
        "  conv_hidden=100    \n",
        "  no_bi=False\n",
        "  max_word=100\n",
        "  max_sent=60\n",
        "  lstm_hidden=256\n",
        "      # length limit\n",
        "  max_art=100\n",
        "  max_abs=50\n",
        "  lr=1e-3\n",
        "  decay=0.5\n",
        "  lr_p=0\n",
        "  clip=2.0\n",
        "  batch=16\n",
        "  ckpt_freq=32\n",
        "  patience=5\n",
        "  debug=True\n",
        "  no_cuda=True\n",
        "  bi = not no_bi\n",
        "  cuda = torch.cuda.is_available() and not no_cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zxrrvAFX1hr"
      },
      "source": [
        "args=Args()\n",
        "main(args)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}