{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_extractor_ml_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/finsummary/blob/master/train_extractor_ml_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-J5IVnU_7u5",
        "outputId": "1e2b6920-f168-40dc-a100-c71aa1d1dc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9k8CoqplBgeG",
        "outputId": "ce7d6924-842c-48d9-c279-ffe8aca52823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcHUchaAU4o"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from os.path import join, exists\n",
        "from datetime import timedelta\n",
        "import pickle as pkl\n",
        "from toolz import compose\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from torch import nn\n",
        "from itertools import starmap\n",
        "from toolz import curry, reduce\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import init\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from toolz.sandbox import unzip\n",
        "from toolz import curry, concat, compose\n",
        "from toolz import curried\n",
        "import torch.multiprocessing as mp\n",
        "from os.path import basename\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import tensorboardX\n",
        "from time import time\n",
        "from glob import glob\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVsF_pmRbzDw",
        "outputId": "6f0d872f-e9af-4cc5-e227-55fb77ed0704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zi8Tei2AltL"
      },
      "source": [
        "def prob_normalize(score, mask):\n",
        "    \"\"\" [(...), T]\n",
        "    user should handle mask shape\"\"\"\n",
        "    score = score.masked_fill(mask == 0, -1e18)\n",
        "    norm_score = F.softmax(score, dim=-1)\n",
        "    return norm_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwBDaQxiCR5s"
      },
      "source": [
        "def dot_attention_score(key,query):\n",
        "  return query.matmul(key.transpose(1,2))\n",
        "\n",
        "def prob_normalize(score,mask):\n",
        "  score=score.masked_fill(mask==0,-1e18)\n",
        "  norm_score=F.softmax(score,dim=-1)\n",
        "  return norm_score\n",
        "\n",
        "def attention_aggregate(value, score):\n",
        "  output=score.matmul(value)\n",
        "  return output\n",
        "\n",
        "def step_attention(query, key, value, mem_mask=None):\n",
        "    \"\"\" query[(Bs), B, D], key[B, T, D], value[B, T, D]\"\"\"\n",
        "    score = dot_attention_score(key, query.unsqueeze(-2))\n",
        "    if mem_mask is None:\n",
        "        norm_score = F.softmax(score, dim=-1)\n",
        "    else:\n",
        "        norm_score = prob_normalize(score, mem_mask)\n",
        "    output = attention_aggregate(value, norm_score)\n",
        "    return output.squeeze(-2), norm_score.squeeze(-2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWpXs8VCZ5A"
      },
      "source": [
        "INI = 1e-2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt9f4hIWZ4to"
      },
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, n_hidden, n_layer, dropout, bidirectional):\n",
        "        super().__init__()\n",
        "        self._init_h = nn.Parameter(\n",
        "            torch.Tensor(n_layer*(2 if bidirectional else 1), n_hidden))\n",
        "        self._init_c = nn.Parameter(\n",
        "            torch.Tensor(n_layer*(2 if bidirectional else 1), n_hidden))\n",
        "        init.uniform_(self._init_h, -INI, INI)\n",
        "        init.uniform_(self._init_c, -INI, INI)\n",
        "        self._lstm = nn.LSTM(input_dim, n_hidden, n_layer,\n",
        "                             dropout=dropout, bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, input_, in_lens=None):\n",
        "        \"\"\" [batch_size, max_num_sent, input_dim] Tensor\"\"\"\n",
        "        size = (self._init_h.size(0), input_.size(0), self._init_h.size(1))\n",
        "        init_states = (self._init_h.unsqueeze(1).expand(*size),\n",
        "                       self._init_c.unsqueeze(1).expand(*size))\n",
        "        lstm_out, _ = lstm_encoder(\n",
        "            input_, self._lstm, in_lens, init_states)\n",
        "        return lstm_out.transpose(0, 1)\n",
        "\n",
        "    @property\n",
        "    def input_size(self):\n",
        "        return self._lstm.input_size\n",
        "\n",
        "    @property\n",
        "    def hidden_size(self):\n",
        "        return self._lstm.hidden_size\n",
        "\n",
        "    @property\n",
        "    def num_layers(self):\n",
        "        return self._lstm.num_layers\n",
        "\n",
        "    @property\n",
        "    def bidirectional(self):\n",
        "        return self._lstm.bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGo3_Pp5Cmup"
      },
      "source": [
        "class ExtractSumm(nn.Module):\n",
        "    \"\"\" ff-ext \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim,\n",
        "                 conv_hidden, lstm_hidden, lstm_layer,\n",
        "                 bidirectional, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self._sent_enc = ConvSentEncoder(\n",
        "            vocab_size, emb_dim, conv_hidden, dropout)\n",
        "        self._art_enc = LSTMEncoder(\n",
        "            3*conv_hidden, lstm_hidden, lstm_layer,\n",
        "            dropout=dropout, bidirectional=bidirectional\n",
        "        )\n",
        "\n",
        "        lstm_out_dim = lstm_hidden * (2 if bidirectional else 1)\n",
        "        self._sent_linear = nn.Linear(lstm_out_dim, 1)\n",
        "        self._art_linear = nn.Linear(lstm_out_dim, lstm_out_dim)\n",
        "\n",
        "    def forward(self, article_sents, sent_nums):\n",
        "        enc_sent, enc_art = self._encode(article_sents, sent_nums)\n",
        "        saliency = torch.matmul(enc_sent, enc_art.unsqueeze(2))\n",
        "        saliency = torch.cat(\n",
        "            [s[:n] for s, n in zip(saliency, sent_nums)], dim=0)\n",
        "        content = self._sent_linear(\n",
        "            torch.cat([s[:n] for s, n in zip(enc_sent, sent_nums)], dim=0)\n",
        "        )\n",
        "        logit = (content + saliency).squeeze(1)\n",
        "        return logit\n",
        "\n",
        "    def extract(self, article_sents, sent_nums=None, k=4):\n",
        "        \"\"\" extract top-k scored sentences from article (eval only)\"\"\"\n",
        "        enc_sent, enc_art = self._encode(article_sents, sent_nums)\n",
        "        saliency = torch.matmul(enc_sent, enc_art.unsqueeze(2))\n",
        "        content = self._sent_linear(enc_sent)\n",
        "        logit = (content + saliency).squeeze(2)\n",
        "        if sent_nums is None:  # test-time extract only\n",
        "            assert len(article_sents) == 1\n",
        "            n_sent = logit.size(1)\n",
        "            extracted = logit[0].topk(\n",
        "                k if k < n_sent else n_sent, sorted=False  # original order\n",
        "            )[1].tolist()\n",
        "        else:\n",
        "            extracted = [l[:n].topk(k if k < n else n)[1].tolist()\n",
        "                         for n, l in zip(sent_nums, logit)]\n",
        "        return extracted\n",
        "\n",
        "    def _encode(self, article_sents, sent_nums):\n",
        "        if sent_nums is None:  # test-time extract only\n",
        "            enc_sent = self._sent_enc(article_sents[0]).unsqueeze(0)\n",
        "        else:\n",
        "            max_n = max(sent_nums)\n",
        "            enc_sents = [self._sent_enc(art_sent)\n",
        "                         for art_sent in article_sents]\n",
        "            def zero(n, device):\n",
        "                z = torch.zeros(n, self._art_enc.input_size).to(device)\n",
        "                return z\n",
        "            enc_sent = torch.stack(\n",
        "                [torch.cat([s, zero(max_n-n, s.device)],\n",
        "                           dim=0) if n != max_n\n",
        "                 else s\n",
        "                 for s, n in zip(enc_sents, sent_nums)],\n",
        "                dim=0\n",
        "            )\n",
        "        lstm_out = self._art_enc(enc_sent, sent_nums)\n",
        "        enc_art = F.tanh(\n",
        "            self._art_linear(sequence_mean(lstm_out, sent_nums, dim=1)))\n",
        "        return lstm_out, enc_art\n",
        "\n",
        "    def set_embedding(self, embedding):\n",
        "        self._sent_enc.set_embedding(embedding)\n",
        "\n",
        "\n",
        "class LSTMPointerNet(nn.Module):\n",
        "    \"\"\"Pointer network as in Vinyals et al \"\"\"\n",
        "    def __init__(self, input_dim, n_hidden, n_layer,\n",
        "                 dropout, n_hop):\n",
        "        super().__init__()\n",
        "        self._init_h = nn.Parameter(torch.Tensor(n_layer, n_hidden))\n",
        "        self._init_c = nn.Parameter(torch.Tensor(n_layer, n_hidden))\n",
        "        self._init_i = nn.Parameter(torch.Tensor(input_dim))\n",
        "        init.uniform_(self._init_h, -INI, INI)\n",
        "        init.uniform_(self._init_c, -INI, INI)\n",
        "        init.uniform_(self._init_i, -0.1, 0.1)\n",
        "        self._lstm = nn.LSTM(\n",
        "            input_dim, n_hidden, n_layer,\n",
        "            bidirectional=False, dropout=dropout\n",
        "        )\n",
        "        self._lstm_cell = None\n",
        "\n",
        "        # attention parameters\n",
        "        self._attn_wm = nn.Parameter(torch.Tensor(input_dim, n_hidden))\n",
        "        self._attn_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
        "        self._attn_v = nn.Parameter(torch.Tensor(n_hidden))\n",
        "        init.xavier_normal_(self._attn_wm)\n",
        "        init.xavier_normal_(self._attn_wq)\n",
        "        init.uniform_(self._attn_v, -INI, INI)\n",
        "\n",
        "        # hop parameters\n",
        "        self._hop_wm = nn.Parameter(torch.Tensor(input_dim, n_hidden))\n",
        "        self._hop_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
        "        self._hop_v = nn.Parameter(torch.Tensor(n_hidden))\n",
        "        init.xavier_normal_(self._hop_wm)\n",
        "        init.xavier_normal_(self._hop_wq)\n",
        "        init.uniform_(self._hop_v, -INI, INI)\n",
        "        self._n_hop = n_hop\n",
        "\n",
        "    def forward(self, attn_mem, mem_sizes, lstm_in):\n",
        "        \"\"\"atten_mem: Tensor of size [batch_size, max_sent_num, input_dim]\"\"\"\n",
        "        attn_feat, hop_feat, lstm_states, init_i = self._prepare(attn_mem)\n",
        "        lstm_in = torch.cat([init_i, lstm_in], dim=1).transpose(0, 1)\n",
        "        query, final_states = self._lstm(lstm_in, lstm_states)\n",
        "        query = query.transpose(0, 1)\n",
        "        for _ in range(self._n_hop):\n",
        "            query = LSTMPointerNet.attention(\n",
        "                hop_feat, query, self._hop_v, self._hop_wq, mem_sizes)\n",
        "        output = LSTMPointerNet.attention_score(\n",
        "            attn_feat, query, self._attn_v, self._attn_wq)\n",
        "        return output  # unormalized extraction logit\n",
        "\n",
        "    def extract(self, attn_mem, mem_sizes, k):\n",
        "        \"\"\"extract k sentences, decode only, batch_size==1\"\"\"\n",
        "        attn_feat, hop_feat, lstm_states, lstm_in = self._prepare(attn_mem)\n",
        "        lstm_in = lstm_in.squeeze(1)\n",
        "        if self._lstm_cell is None:\n",
        "            self._lstm_cell = MultiLayerLSTMCells.convert(\n",
        "                self._lstm).to(attn_mem.device)\n",
        "        extracts = []\n",
        "        for _ in range(k):\n",
        "            h, c = self._lstm_cell(lstm_in, lstm_states)\n",
        "            query = h[-1]\n",
        "            for _ in range(self._n_hop):\n",
        "                query = LSTMPointerNet.attention(\n",
        "                    hop_feat, query, self._hop_v, self._hop_wq, mem_sizes)\n",
        "            score = LSTMPointerNet.attention_score(\n",
        "                attn_feat, query, self._attn_v, self._attn_wq)\n",
        "            score = score.squeeze()\n",
        "            for e in extracts:\n",
        "                score[e] = -1e6\n",
        "            ext = score.max(dim=0)[1].item()\n",
        "            extracts.append(ext)\n",
        "            lstm_states = (h, c)\n",
        "            lstm_in = attn_mem[:, ext, :]\n",
        "        return extracts\n",
        "\n",
        "    def _prepare(self, attn_mem):\n",
        "        attn_feat = torch.matmul(attn_mem, self._attn_wm.unsqueeze(0))\n",
        "        hop_feat = torch.matmul(attn_mem, self._hop_wm.unsqueeze(0))\n",
        "        bs = attn_mem.size(0)\n",
        "        n_l, d = self._init_h.size()\n",
        "        size = (n_l, bs, d)\n",
        "        lstm_states = (self._init_h.unsqueeze(1).expand(*size).contiguous(),\n",
        "                       self._init_c.unsqueeze(1).expand(*size).contiguous())\n",
        "        d = self._init_i.size(0)\n",
        "        init_i = self._init_i.unsqueeze(0).unsqueeze(1).expand(bs, 1, d)\n",
        "        return attn_feat, hop_feat, lstm_states, init_i\n",
        "\n",
        "    @staticmethod\n",
        "    def attention_score(attention, query, v, w):\n",
        "        \"\"\" unnormalized attention score\"\"\"\n",
        "        sum_ = attention.unsqueeze(1) + torch.matmul(\n",
        "            query, w.unsqueeze(0)\n",
        "        ).unsqueeze(2)  # [B, Nq, Ns, D]\n",
        "        score = torch.matmul(\n",
        "            F.tanh(sum_), v.unsqueeze(0).unsqueeze(1).unsqueeze(3)\n",
        "        ).squeeze(3)  # [B, Nq, Ns]\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(attention, query, v, w, mem_sizes):\n",
        "        \"\"\" attention context vector\"\"\"\n",
        "        score = LSTMPointerNet.attention_score(attention, query, v, w)\n",
        "        if mem_sizes is None:\n",
        "            norm_score = F.softmax(score, dim=-1)\n",
        "        else:\n",
        "            mask = len_mask(mem_sizes, score.device).unsqueeze(-2)\n",
        "            norm_score = prob_normalize(score, mask)\n",
        "        output = torch.matmul(norm_score, attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PtrExtractSumm(nn.Module):\n",
        "    \"\"\" rnn-ext\"\"\"\n",
        "    def __init__(self, emb_dim, vocab_size, conv_hidden,\n",
        "                 lstm_hidden, lstm_layer, bidirectional,\n",
        "                 n_hop=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self._sent_enc = ConvSentEncoder(\n",
        "            vocab_size, emb_dim, conv_hidden, dropout)\n",
        "        self._art_enc = LSTMEncoder(\n",
        "            3*conv_hidden, lstm_hidden, lstm_layer,\n",
        "            dropout=dropout, bidirectional=bidirectional\n",
        "        )\n",
        "        enc_out_dim = lstm_hidden * (2 if bidirectional else 1)\n",
        "        self._extractor = LSTMPointerNet(\n",
        "            enc_out_dim, lstm_hidden, lstm_layer,\n",
        "            dropout, n_hop\n",
        "        )\n",
        "\n",
        "    def forward(self, article_sents, sent_nums, target):\n",
        "        enc_out = self._encode(article_sents, sent_nums)\n",
        "        bs, nt = target.size()\n",
        "        d = enc_out.size(2)\n",
        "        ptr_in = torch.gather(\n",
        "            enc_out, dim=1, index=target.unsqueeze(2).expand(bs, nt, d)\n",
        "        )\n",
        "        output = self._extractor(enc_out, sent_nums, ptr_in)\n",
        "        return output\n",
        "\n",
        "    def extract(self, article_sents, sent_nums=None, k=4):\n",
        "        enc_out = self._encode(article_sents, sent_nums)\n",
        "        output = self._extractor.extract(enc_out, sent_nums, k)\n",
        "        return output\n",
        "\n",
        "    def _encode(self, article_sents, sent_nums):\n",
        "        if sent_nums is None:  # test-time excode only\n",
        "            enc_sent = self._sent_enc(article_sents[0]).unsqueeze(0)\n",
        "        else:\n",
        "            max_n = max(sent_nums)\n",
        "            enc_sents = [self._sent_enc(art_sent)\n",
        "                         for art_sent in article_sents]\n",
        "            def zero(n, device):\n",
        "                z = torch.zeros(n, self._art_enc.input_size).to(device)\n",
        "                return z\n",
        "            enc_sent = torch.stack(\n",
        "                [torch.cat([s, zero(max_n-n, s.device)], dim=0)\n",
        "                   if n != max_n\n",
        "                 else s\n",
        "                 for s, n in zip(enc_sents, sent_nums)],\n",
        "                dim=0\n",
        "            )\n",
        "        lstm_out = self._art_enc(enc_sent, sent_nums)\n",
        "        return lstm_out\n",
        "\n",
        "    def set_embedding(self, embedding):\n",
        "        self._sent_enc.set_embedding(embedding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGd-fBrVC2fA"
      },
      "source": [
        "def sequence_loss(logits,targets,xent_fn=None,pad_idx=0):\n",
        "  assert logits.size()[:-1]==targets.size()\n",
        "  mask=targets!=pad_idx\n",
        "  target=targets.masked_select(mask)\n",
        "  logit=logits.masked_select(mask.unsqueeze(2).expand_as(logits)).contiguous().view(-1,logits.size(-1))\n",
        "  if xent_fn:\n",
        "    loss=xent_fn(logit,target)\n",
        "  else:\n",
        "    loss=F.cross_entropy(logit,target)\n",
        "  assert (not math.isnan(loss.mean().item()))\n",
        "  return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmSDCTi8DBaj"
      },
      "source": [
        "def get_basic_grad_fn(net, clip_grad, max_grad=1e2):\n",
        "    def f():\n",
        "        grad_norm = clip_grad_norm_(\n",
        "            [p for p in net.parameters() if p.requires_grad], clip_grad)\n",
        "        grad_norm = grad_norm.item()\n",
        "        if max_grad is not None and grad_norm >= max_grad:\n",
        "            print('WARNING: Exploding Gradients {:.2f}'.format(grad_norm))\n",
        "            grad_norm = max_grad\n",
        "        grad_log = {}\n",
        "        grad_log['grad_norm'] = grad_norm\n",
        "        return grad_log\n",
        "    return f\n",
        "\n",
        "@curry\n",
        "def compute_loss(net, criterion, fw_args, loss_args):\n",
        "    loss = criterion(*((net(*fw_args),) + loss_args))\n",
        "    return loss\n",
        "\n",
        "@curry\n",
        "def val_step(loss_step, fw_args, loss_args):\n",
        "    loss = loss_step(fw_args, loss_args)\n",
        "    return loss.size(0), loss.sum().item()\n",
        "\n",
        "@curry\n",
        "def basic_validate(net, criterion, val_batches):\n",
        "    print('running validation ... ', end='')\n",
        "    net.eval()\n",
        "    start = time()\n",
        "    with torch.no_grad():\n",
        "        validate_fn = val_step(compute_loss(net, criterion))\n",
        "        n_data, tot_loss = reduce(\n",
        "            lambda a, b: (a[0]+b[0], a[1]+b[1]),\n",
        "            starmap(validate_fn, val_batches),\n",
        "            (0, 0)\n",
        "        )\n",
        "    val_loss = tot_loss / n_data\n",
        "    print(\n",
        "        'validation finished in {}                                    '.format(\n",
        "            timedelta(seconds=int(time()-start)))\n",
        "    )\n",
        "    print('validation loss: {:.4f} ... '.format(val_loss))\n",
        "    return {'loss': val_loss}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2ZAIJx7DSun"
      },
      "source": [
        "class BasicPipeline(object):\n",
        "    def __init__(self, name, net,\n",
        "                 train_batcher, val_batcher, batch_size,\n",
        "                 val_fn, criterion, optim, grad_fn=None):\n",
        "        self.name = name\n",
        "        self._net = net\n",
        "        self._train_batcher = train_batcher\n",
        "        self._val_batcher = val_batcher\n",
        "        self._criterion = criterion\n",
        "        self._opt = optim\n",
        "        # grad_fn is calleble without input args that modifyies gradient\n",
        "        # it should return a dictionary of logging values\n",
        "        self._grad_fn = grad_fn\n",
        "        self._val_fn = val_fn\n",
        "\n",
        "        self._n_epoch = 0  # epoch not very useful?\n",
        "        self._batch_size = batch_size\n",
        "        self._batches = self.batches()\n",
        "\n",
        "    def batches(self):\n",
        "        while True:\n",
        "            for fw_args, bw_args in self._train_batcher(self._batch_size):\n",
        "                yield fw_args, bw_args\n",
        "            self._n_epoch += 1\n",
        "\n",
        "    def get_loss_args(self, net_out, bw_args):\n",
        "        if isinstance(net_out, tuple):\n",
        "            loss_args = net_out + bw_args\n",
        "        else:\n",
        "            loss_args = (net_out, ) + bw_args\n",
        "        return loss_args\n",
        "\n",
        "    def train_step(self):\n",
        "        # forward pass of model\n",
        "        self._net.train()\n",
        "        fw_args, bw_args = next(self._batches)\n",
        "        net_out = self._net(*fw_args)\n",
        "\n",
        "        # get logs and output for logging, backward\n",
        "        log_dict = {}\n",
        "        loss_args = self.get_loss_args(net_out, bw_args)\n",
        "\n",
        "        # backward and update ( and optional gradient monitoring )\n",
        "        loss = self._criterion(*loss_args).mean()\n",
        "        loss.backward()\n",
        "        log_dict['loss'] = loss.item()\n",
        "        if self._grad_fn is not None:\n",
        "            log_dict.update(self._grad_fn())\n",
        "        self._opt.step()\n",
        "        self._net.zero_grad()\n",
        "\n",
        "        return log_dict\n",
        "\n",
        "    def validate(self):\n",
        "        return self._val_fn(self._val_batcher(self._batch_size))\n",
        "\n",
        "    def checkpoint(self, save_path, step, val_metric=None):\n",
        "        save_dict = {}\n",
        "        if val_metric is not None:\n",
        "            name = 'ckpt-{:6f}-{}'.format(val_metric, step)\n",
        "            save_dict['val_metric'] = val_metric\n",
        "        else:\n",
        "            name = 'ckpt-{}'.format(step)\n",
        "\n",
        "        save_dict['state_dict'] = self._net.state_dict()\n",
        "        save_dict['optimizer'] = self._opt.state_dict()\n",
        "        torch.save(save_dict, join(save_path, name))\n",
        "        print(\"Check point saved to \",join(save_path,name))\n",
        "\n",
        "    def terminate(self):\n",
        "        self._train_batcher.terminate()\n",
        "        self._val_batcher.terminate()\n",
        "\n",
        "\n",
        "class BasicTrainer(object):\n",
        "    \"\"\" Basic trainer with minimal function and early stopping\"\"\"\n",
        "    def __init__(self, pipeline, save_dir, ckpt_freq, patience,\n",
        "                 scheduler=None, val_mode='loss'):\n",
        "        assert isinstance(pipeline, BasicPipeline)\n",
        "        assert val_mode in ['loss', 'score']\n",
        "        self._pipeline = pipeline\n",
        "        self._save_dir = save_dir\n",
        "        self._logger = tensorboardX.SummaryWriter(join(save_dir, 'log'))\n",
        "        ckpt_dir=join(save_dir,'extract_ckpt')\n",
        "        try:\n",
        "          os.makedirs(join(ckpt_dir, 'ckpt'))\n",
        "        except:\n",
        "          print(\"Path already present\")\n",
        "        self._ckpt_freq = ckpt_freq\n",
        "        self._patience = patience\n",
        "        self._sched = scheduler\n",
        "        self._val_mode = val_mode\n",
        "\n",
        "        self._step = 0\n",
        "        self._running_loss = None\n",
        "        # state vars for early stopping\n",
        "        self._current_p = 0\n",
        "        self._best_val = None\n",
        "\n",
        "    def log(self, log_dict):\n",
        "        loss = log_dict['loss'] if 'loss' in log_dict else log_dict['reward']\n",
        "        if self._running_loss is not None:\n",
        "            self._running_loss = 0.99*self._running_loss + 0.01*loss\n",
        "        else:\n",
        "            self._running_loss = loss\n",
        "        print('train step: {}, {}: {:.4f}\\r'.format(\n",
        "            self._step,\n",
        "            'loss' if 'loss' in log_dict else 'reward',\n",
        "            self._running_loss), end='')\n",
        "        for key, value in log_dict.items():\n",
        "            self._logger.add_scalar(\n",
        "                '{}_{}'.format(key, self._pipeline.name), value, self._step)\n",
        "\n",
        "    def validate(self):\n",
        "        print()\n",
        "        val_log = self._pipeline.validate()\n",
        "        for key, value in val_log.items():\n",
        "            self._logger.add_scalar(\n",
        "                'val_{}_{}'.format(key, self._pipeline.name),\n",
        "                value, self._step\n",
        "            )\n",
        "        if 'reward' in val_log:\n",
        "            val_metric = val_log['reward']\n",
        "        else:\n",
        "            val_metric = (val_log['loss'] if self._val_mode == 'loss'\n",
        "                          else val_log['score'])\n",
        "        return val_metric\n",
        "\n",
        "    def checkpoint(self):\n",
        "        val_metric = self.validate()\n",
        "        self._pipeline.checkpoint(\n",
        "            join(self._save_dir, 'extract_dir/ckpt'), self._step, val_metric)\n",
        "        if isinstance(self._sched, ReduceLROnPlateau):\n",
        "            self._sched.step(val_metric)\n",
        "        else:\n",
        "            self._sched.step()\n",
        "        stop = self.check_stop(val_metric)\n",
        "        return stop\n",
        "\n",
        "    def check_stop(self, val_metric):\n",
        "        if self._best_val is None:\n",
        "            self._best_val = val_metric\n",
        "        elif ((val_metric < self._best_val and self._val_mode == 'loss')\n",
        "              or (val_metric > self._best_val and self._val_mode == 'score')):\n",
        "            self._current_p = 0\n",
        "            self._best_val = val_metric\n",
        "        else:\n",
        "            self._current_p += 1\n",
        "        return self._current_p >= self._patience\n",
        "\n",
        "    def train(self):\n",
        "        try:\n",
        "            start = time()\n",
        "            print('Start training')\n",
        "            while True:\n",
        "                log_dict = self._pipeline.train_step()\n",
        "                self._step += 1\n",
        "                self.log(log_dict)\n",
        "\n",
        "                if self._step % self._ckpt_freq == 0:\n",
        "                    stop = self.checkpoint()\n",
        "                    if stop:\n",
        "                        break\n",
        "            print('Training finised in ', timedelta(seconds=time()-start))\n",
        "        finally:\n",
        "            self._pipeline.terminate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scnSbcctDiJn"
      },
      "source": [
        "PAD = 0\n",
        "UNK = 1\n",
        "START = 2\n",
        "END = 3\n",
        "BUCKET_SIZE=1000\n",
        "\n",
        "DATA_DIR = r\"/content/drive/My Drive/finsummary/Data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOd9OV4GD5xh"
      },
      "source": [
        "\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "START = 2\n",
        "END = 3\n",
        "def make_vocab(wc, vocab_size):\n",
        "    word2id, id2word = {}, {}\n",
        "    word2id['<pad>'] = PAD\n",
        "    word2id['<unk>'] = UNK\n",
        "    word2id['<start>'] = START\n",
        "    word2id['<end>'] = END\n",
        "    for i, (w, _) in enumerate(wc.most_common(vocab_size), 4):\n",
        "        word2id[w] = i\n",
        "    return word2id\n",
        "\n",
        "\n",
        "def make_embedding(id2word, w2v_file, initializer=None):\n",
        "    attrs = basename(w2v_file).split('.')  #word2vec.{dim}d.{vsize}k.bin\n",
        "    w2v = gensim.models.Word2Vec.load(w2v_file).wv\n",
        "    vocab_size = len(id2word)\n",
        "    emb_dim = int(attrs[-3][:-1])\n",
        "    embedding = nn.Embedding(vocab_size, emb_dim).weight\n",
        "    if initializer is not None:\n",
        "        initializer(embedding)\n",
        "\n",
        "    oovs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(len(id2word)):\n",
        "            # NOTE: id2word can be list or dict\n",
        "            if i == START:\n",
        "                embedding[i, :] = torch.Tensor(w2v['<s>'])\n",
        "            elif i == END:\n",
        "                embedding[i, :] = torch.Tensor(w2v['<\\\\s>'])\n",
        "            elif id2word[i] in w2v:\n",
        "                embedding[i, :] = torch.Tensor(w2v[id2word[i]])\n",
        "            else:\n",
        "                oovs.append(i)\n",
        "    return embedding, oovs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14-NZMs4xOOy"
      },
      "source": [
        "import sys\n",
        "class CnnDmDataset(Dataset):\n",
        "    def __init__(self, split: str, path: str) -> None:\n",
        "        assert split in ['training', 'validation', 'test']\n",
        "        self._data_path = join(path, split)\n",
        "        self._n_data = _count_data(self._data_path)\n",
        "        self._json_files=os.listdir(join(self._data_path,'extraction'))\n",
        "        self._json_files=list(filter(lambda x: x.endswith('json'),self._json_files))\n",
        "        self._idx_list=[]\n",
        "        for i in range(len(self._json_files)):\n",
        "          self._idx_list.append(self._json_files[i].split('.')[0])\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._json_files)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        # token = compose(list, _split_words)\n",
        "        idx=self._idx_list[i]\n",
        "        js_path=join(self._data_path,'extraction')\n",
        "        try:\n",
        "          with open(join(js_path, '{}.json'.format(idx)),'r') as f:\n",
        "              js = json.loads(f.read())\n",
        "        except:\n",
        "          print(join(js_path, '{}.json'.format(idx)))\n",
        "          sys.exit()\n",
        "        # f_name=js['filename']\n",
        "        # summ_path=join(self._data_path,'gold_summaries')\n",
        "        # with open(join(summ_path,f_name),encoding='utf8') as f:\n",
        "        #   abs_data = f.read()\n",
        "        # abs_sentences=[]\n",
        "        # for sent in tokenizer.tokenize(abs_data):\n",
        "        #   sent = sent.replace('\\n', ' ')\n",
        "        #   abs_sentences.append(sent)\n",
        "        # abs_sents = token(abs_sentences)\n",
        "        # js['summary_sents']=abs_sents\n",
        "        report_name=idx.split('.')[0].split()[0]+'.txt'\n",
        "        article_path=join(self._data_path,'annual_reports')\n",
        "        with open(join(article_path,report_name),encoding='utf8') as f:\n",
        "          article_file=f.read()\n",
        "        article_sentences=[]\n",
        "        for sent in tokenizer.tokenize(article_file):\n",
        "          sent=sent.replace('\\n',' ')\n",
        "          article_sentences.append(sent)\n",
        "        js['article']=article_sentences\n",
        "        return js\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J61OcmRtFuWy"
      },
      "source": [
        "def coll_fn(data):\n",
        "    source_lists, target_lists = unzip(data)\n",
        "    # NOTE: independent filtering works because\n",
        "    #       source and targets are matched properly by the Dataset\n",
        "    sources = list(filter(bool, concat(source_lists)))\n",
        "    targets = list(filter(bool, concat(target_lists)))\n",
        "    assert all(sources) and all(targets)\n",
        "    return sources, targets\n",
        "\n",
        "def coll_fn_extract(data):\n",
        "    def is_good_data(d):\n",
        "        \"\"\" make sure data is not empty\"\"\"\n",
        "        source_sents, extracts = d\n",
        "        return source_sents and extracts\n",
        "    batch = list(filter(is_good_data, data))\n",
        "    assert all(map(is_good_data, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def tokenize(max_len, texts):\n",
        "    return [t.lower().split()[:max_len] for t in texts]\n",
        "\n",
        "def conver2id(unk, word2id, words_list):\n",
        "    word2id = defaultdict(lambda: unk, word2id)\n",
        "    return [[word2id[w] for w in words] for words in words_list]\n",
        "\n",
        "@curry\n",
        "def prepro_fn(max_src_len, max_tgt_len, batch):\n",
        "    sources, targets = batch\n",
        "    # sources = tokenize(max_src_len, sources)\n",
        "    # targets = tokenize(max_tgt_len, targets)\n",
        "    \n",
        "    for i in range(len(sources)):\n",
        "      sources[i]=sources[i][:max_src_len]\n",
        "    for i in range(len(targets)):\n",
        "      targets[i]=targets[i][:max_tgt_len]\n",
        "    # targets=targets[]\n",
        "    batch = list(zip(sources, targets))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def prepro_fn_extract(max_src_len, max_src_num, batch):\n",
        "    def prepro_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        tokenized_sents = tokenize(max_src_len, source_sents)[:max_src_num]\n",
        "        cleaned_extracts = list(filter(lambda e: e < len(tokenized_sents),\n",
        "                                       extracts))\n",
        "        return tokenized_sents, cleaned_extracts\n",
        "    batch = list(map(prepro_one, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch(unk, word2id, batch):\n",
        "    sources, targets = unzip(batch)\n",
        "    sources = conver2id(unk, word2id, sources)\n",
        "    targets = conver2id(unk, word2id, targets)\n",
        "    batch = list(zip(sources, targets))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch_copy(unk, word2id, batch):\n",
        "    sources, targets = map(list, unzip(batch))\n",
        "    ext_word2id = dict(word2id)\n",
        "    for source in sources:\n",
        "        for word in source:\n",
        "            if word not in ext_word2id:\n",
        "                ext_word2id[word] = len(ext_word2id)\n",
        "    src_exts = conver2id(unk, ext_word2id, sources)\n",
        "    sources = conver2id(unk, word2id, sources)\n",
        "    tar_ins = conver2id(unk, word2id, targets)\n",
        "    targets = conver2id(unk, ext_word2id, targets)\n",
        "    batch = list(zip(sources, src_exts, tar_ins, targets))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch_extract_ptr(unk, word2id, batch):\n",
        "    def convert_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        id_sents = conver2id(unk, word2id, source_sents)\n",
        "        return id_sents, extracts\n",
        "    batch = list(map(convert_one, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch_extract_ff(unk, word2id, batch):\n",
        "    def convert_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        id_sents = conver2id(unk, word2id, source_sents)\n",
        "        binary_extracts = [0] * len(source_sents)\n",
        "        for ext in extracts:\n",
        "            binary_extracts[ext] = 1\n",
        "        return id_sents, binary_extracts\n",
        "    batch = list(map(convert_one, batch))\n",
        "    return batch\n",
        "\n",
        "\n",
        "@curry\n",
        "def pad_batch_tensorize(inputs, pad, cuda=True):\n",
        "    \"\"\"pad_batch_tensorize\n",
        "\n",
        "    :param inputs: List of size B containing torch tensors of shape [T, ...]\n",
        "    :type inputs: List[np.ndarray]\n",
        "    :rtype: TorchTensor of size (B, T, ...)\n",
        "    \"\"\"\n",
        "    tensor_type = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "    batch_size = len(inputs)\n",
        "    max_len = max(len(ids) for ids in inputs)\n",
        "    tensor_shape = (batch_size, max_len)\n",
        "    tensor = tensor_type(*tensor_shape)\n",
        "    tensor.fill_(pad)\n",
        "    for i, ids in enumerate(inputs):\n",
        "        tensor[i, :len(ids)] = tensor_type(ids)\n",
        "    return tensor\n",
        "\n",
        "@curry\n",
        "def batchify_fn(pad, start, end, data, cuda=True):\n",
        "    sources, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_lens = [len(src) for src in sources]\n",
        "    tar_ins = [[start] + tgt for tgt in targets]\n",
        "    targets = [tgt + [end] for tgt in targets]\n",
        "\n",
        "    source = pad_batch_tensorize(sources, pad, cuda)\n",
        "    tar_in = pad_batch_tensorize(tar_ins, pad, cuda)\n",
        "    target = pad_batch_tensorize(targets, pad, cuda)\n",
        "\n",
        "    fw_args = (source, src_lens, tar_in)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "@curry\n",
        "def batchify_fn_copy(pad, start, end, data, cuda=True):\n",
        "    sources, ext_srcs, tar_ins, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_lens = [len(src) for src in sources]\n",
        "    sources = [src for src in sources]\n",
        "    ext_srcs = [ext for ext in ext_srcs]\n",
        "\n",
        "    tar_ins = [[start] + tgt for tgt in tar_ins]\n",
        "    targets = [tgt + [end] for tgt in targets]\n",
        "\n",
        "    source = pad_batch_tensorize(sources, pad, cuda)\n",
        "    tar_in = pad_batch_tensorize(tar_ins, pad, cuda)\n",
        "    target = pad_batch_tensorize(targets, pad, cuda)\n",
        "    ext_src = pad_batch_tensorize(ext_srcs, pad, cuda)\n",
        "\n",
        "    ext_vsize = ext_src.max().item() + 1\n",
        "    fw_args = (source, src_lens, tar_in, ext_src, ext_vsize)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "@curry\n",
        "def batchify_fn_extract_ptr(pad, data, cuda=True):\n",
        "    source_lists, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_nums = list(map(len, source_lists))\n",
        "    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n",
        "\n",
        "    # PAD is -1 (dummy extraction index) for using sequence loss\n",
        "    target = pad_batch_tensorize(targets, pad=-1, cuda=cuda)\n",
        "    remove_last = lambda tgt: tgt[:-1]\n",
        "    tar_in = pad_batch_tensorize(\n",
        "        list(map(remove_last, targets)),\n",
        "        pad=-0, cuda=cuda # use 0 here for feeding first conv sentence repr.\n",
        "    )\n",
        "\n",
        "    fw_args = (sources, src_nums, tar_in)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "@curry\n",
        "def batchify_fn_extract_ff(pad, data, cuda=True):\n",
        "    source_lists, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_nums = list(map(len, source_lists))\n",
        "    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n",
        "\n",
        "    tensor_type = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "    target = tensor_type(list(concat(targets)))\n",
        "\n",
        "    fw_args = (sources, src_nums)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "def _batch2q(loader, prepro, q, single_run=True):\n",
        "    epoch = 0\n",
        "    while True:\n",
        "        for batch in loader:\n",
        "            q.put(prepro(batch))\n",
        "        if single_run:\n",
        "            break\n",
        "        epoch += 1\n",
        "        q.put(epoch)\n",
        "    q.put(None)\n",
        "\n",
        "class BucketedGenerater(object):\n",
        "    def __init__(self, loader, prepro,\n",
        "                 sort_key, batchify,\n",
        "                 single_run=True, queue_size=8, fork=True):\n",
        "        self._loader = loader\n",
        "        self._prepro = prepro\n",
        "        self._sort_key = sort_key\n",
        "        self._batchify = batchify\n",
        "        self._single_run = single_run\n",
        "        if fork:\n",
        "            ctx = mp.get_context('forkserver')\n",
        "            self._queue = ctx.Queue(queue_size)\n",
        "        else:\n",
        "            # for easier debugging\n",
        "            self._queue = None\n",
        "        self._process = None\n",
        "\n",
        "    def __call__(self, batch_size: int):\n",
        "        def get_batches(hyper_batch):\n",
        "            indexes = list(range(0, len(hyper_batch), batch_size))\n",
        "            if not self._single_run:\n",
        "                # random shuffle for training batches\n",
        "                random.shuffle(hyper_batch)\n",
        "                random.shuffle(indexes)\n",
        "            hyper_batch.sort(key=self._sort_key)\n",
        "            for i in indexes:\n",
        "                batch = self._batchify(hyper_batch[i:i+batch_size])\n",
        "                yield batch\n",
        "\n",
        "        if self._queue is not None:\n",
        "            ctx = mp.get_context('forkserver')\n",
        "            self._process = ctx.Process(\n",
        "                target=_batch2q,\n",
        "                args=(self._loader, self._prepro,\n",
        "                      self._queue, self._single_run)\n",
        "            )\n",
        "            self._process.start()\n",
        "            while True:\n",
        "                d = self._queue.get()\n",
        "                if d is None:\n",
        "                    break\n",
        "                if isinstance(d, int):\n",
        "                    print('\\nepoch {} done'.format(d))\n",
        "                    continue\n",
        "                yield from get_batches(d)\n",
        "            self._process.join()\n",
        "        else:\n",
        "            i = 0\n",
        "            while True:\n",
        "                for batch in self._loader:\n",
        "                    yield from get_batches(self._prepro(batch))\n",
        "                if self._single_run:\n",
        "                    break\n",
        "                i += 1\n",
        "                print('\\nepoch {} done'.format(i))\n",
        "\n",
        "    def terminate(self):\n",
        "        if self._process is not None:\n",
        "            self._process.terminate()\n",
        "            self._process.join()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlWbMrIVF_YZ"
      },
      "source": [
        "class ExtractDataset(CnnDmDataset):\n",
        "    \"\"\" article sentences -> extraction indices\n",
        "    (dataset created by greedily matching ROUGE)\n",
        "    \"\"\"\n",
        "    def __init__(self, split):\n",
        "        super().__init__(split, DATA_DIR)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        js_data = super().__getitem__(i)\n",
        "        art_sents, extracts = js_data['article'], js_data['extracted_labels']\n",
        "        return art_sents, extracts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3mA8Cq7YZSD"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eI4j7JiAVZZM"
      },
      "source": [
        "w2v_bin_path='/content/drive/My Drive/finsummary/Data/word2vec.300d.629k.bin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDiAAL6ZX5IB"
      },
      "source": [
        "def main(args):\n",
        "    assert args.net_type in ['ff', 'rnn']\n",
        "    # create data batcher, vocabulary\n",
        "    # batcher\n",
        "    with open(join(DATA_DIR, 'vocab_cnt.pkl'), 'rb') as f:\n",
        "        wc = pkl.load(f)\n",
        "    word2id = make_vocab(wc, args.vsize)\n",
        "    train_batcher, val_batcher = build_batchers(args.net_type, word2id,\n",
        "                                                args.cuda, args.debug)\n",
        "\n",
        "    # make net\n",
        "    net, net_args = configure_net(args.net_type,\n",
        "                                  len(word2id), args.emb_dim, args.conv_hidden,\n",
        "                                  args.lstm_hidden, args.lstm_layer, args.bi)\n",
        "    if args.w2v:\n",
        "        # NOTE: the pretrained embedding having the same dimension\n",
        "        #       as args.emb_dim should already be trained\n",
        "        embedding, _ = make_embedding(\n",
        "            {i: w for w, i in word2id.items()}, args.w2v)\n",
        "        net.set_embedding(embedding)\n",
        "\n",
        "    # configure training setting\n",
        "    criterion, train_params = configure_training(\n",
        "        args.net_type, 'adam', args.lr, args.clip, args.decay, args.batch\n",
        "    )\n",
        "\n",
        "    # save experiment setting\n",
        "    if not exists(args.path):\n",
        "        os.makedirs(args.path)\n",
        "    with open(join(args.path, 'vocab.pkl'), 'wb') as f:\n",
        "        pkl.dump(word2id, f, pkl.HIGHEST_PROTOCOL)\n",
        "    meta = {}\n",
        "    meta['net']           = 'ml_{}_extractor'.format(args.net_type)\n",
        "    meta['net_args']      = net_args\n",
        "    meta['traing_params'] = train_params\n",
        "    with open(join(args.path, 'meta.json'), 'w') as f:\n",
        "        json.dump(meta, f, indent=4)\n",
        "\n",
        "    # prepare trainer\n",
        "    val_fn = basic_validate(net, criterion)\n",
        "    grad_fn = get_basic_grad_fn(net, args.clip)\n",
        "    optimizer = optim.Adam(net.parameters(), **train_params['optimizer'][1])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True,\n",
        "                                  factor=args.decay, min_lr=0,\n",
        "                                  patience=args.lr_p)\n",
        "\n",
        "    if args.cuda:\n",
        "        net = net.cuda()\n",
        "    pipeline = BasicPipeline(meta['net'], net,\n",
        "                             train_batcher, val_batcher, args.batch, val_fn,\n",
        "                             criterion, optimizer, grad_fn)\n",
        "    trainer = BasicTrainer(pipeline, args.path,\n",
        "                           args.ckpt_freq, args.patience, scheduler)\n",
        "\n",
        "    print('start training with the following hyper-parameters:')\n",
        "    print(meta)\n",
        "    trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4iBDPphTmoj"
      },
      "source": [
        "class Args():\n",
        "  path=DATA_DIR\n",
        "  net_type='rnn'\n",
        "\n",
        "  vsize=20000\n",
        "                          \n",
        "  emb_dim=300\n",
        "                        \n",
        "  w2v=w2v_bin_path\n",
        "\n",
        "  n_hidden=256\n",
        "  lstm_layer=1            \n",
        "  n_layer=2\n",
        "  conv_hidden=100    \n",
        "  no_bi=False\n",
        "  max_word=100\n",
        "  max_sent=60\n",
        "  lstm_hidden=256\n",
        "      # length limit\n",
        "  max_art=100\n",
        "  max_abs=50\n",
        "  lr=1e-3\n",
        "  decay=0.5\n",
        "  lr_p=0\n",
        "  clip=2.0\n",
        "  batch=16\n",
        "  ckpt_freq=32\n",
        "  patience=5\n",
        "  debug=True\n",
        "  no_cuda=True\n",
        "  bi = not no_bi\n",
        "  cuda = torch.cuda.is_available() and not no_cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zxrrvAFX1hr",
        "outputId": "69b597cd-1f9e-469b-a949-cc516248ed7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "args=Args()\n",
        "main(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Path already present\n",
            "start training with the following hyper-parameters:\n",
            "{'net': 'ml_rnn_extractor', 'net_args': {'vocab_size': 20004, 'emb_dim': 300, 'conv_hidden': 100, 'lstm_hidden': 256, 'lstm_layer': 1, 'bidirectional': True}, 'traing_params': {'optimizer': ('adam', {'lr': 0.001}), 'clip_grad_norm': 2.0, 'batch_size': 16, 'lr_decay': 0.5}}\n",
            "Start training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train step: 32, loss: 4.0513\n",
            "running validation ... validation finished in 0:09:52                                    \n",
            "validation loss: 3.8518 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-3.851771-32\n",
            "train step: 64, loss: 4.0125\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 3.8537 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-3.853740-64\n",
            "Epoch     2: reducing learning rate of group 0 to 5.0000e-04.\n",
            "train step: 96, loss: 3.9776\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 3.7918 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-3.791816-96\n",
            "train step: 128, loss: 3.8529\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 3.3265 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-3.326531-128\n",
            "train step: 160, loss: 3.6786\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 2.9333 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-2.933295-160\n",
            "train step: 190, loss: 3.4608\n",
            "epoch 1 done\n",
            "train step: 192, loss: 3.4457\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 2.7721 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-2.772085-192\n",
            "train step: 224, loss: 3.2041\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 2.3373 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-2.337334-224\n",
            "train step: 256, loss: 2.9869\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 2.2753 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-2.275271-256\n",
            "train step: 288, loss: 2.8357\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 2.4394 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-2.439413-288\n",
            "Epoch     9: reducing learning rate of group 0 to 2.5000e-04.\n",
            "train step: 320, loss: 2.6238\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 1.8698 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.869812-320\n",
            "train step: 352, loss: 2.4415\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 1.7909 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.790917-352\n",
            "train step: 380, loss: 2.2826\n",
            "epoch 2 done\n",
            "train step: 384, loss: 2.2632\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 1.7127 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.712663-384\n",
            "train step: 416, loss: 2.1620\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 1.6165 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.616493-416\n",
            "train step: 448, loss: 2.0358\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 1.5407 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.540659-448\n",
            "train step: 480, loss: 1.9329\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 1.4829 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.482876-480\n",
            "train step: 512, loss: 1.8157\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 1.4364 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.436360-512\n",
            "train step: 544, loss: 1.7081\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 1.3577 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.357725-544\n",
            "train step: 570, loss: 1.6411\n",
            "epoch 3 done\n",
            "train step: 576, loss: 1.6234\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 1.2792 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.279159-576\n",
            "train step: 608, loss: 1.5800\n",
            "running validation ... validation finished in 0:01:28                                    \n",
            "validation loss: 1.3376 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.337571-608\n",
            "Epoch    19: reducing learning rate of group 0 to 1.2500e-04.\n",
            "train step: 640, loss: 1.5435\n",
            "running validation ... validation finished in 0:01:27                                    \n",
            "validation loss: 1.2608 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.260801-640\n",
            "train step: 672, loss: 1.4512\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 1.1440 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.144035-672\n",
            "train step: 704, loss: 1.3583\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 1.1005 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.100498-704\n",
            "train step: 736, loss: 1.3070\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 1.1001 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.100138-736\n",
            "train step: 760, loss: 1.2615\n",
            "epoch 4 done\n",
            "train step: 768, loss: 1.2602\n",
            "running validation ... validation finished in 0:01:24                                    \n",
            "validation loss: 1.1253 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.125265-768\n",
            "Epoch    24: reducing learning rate of group 0 to 6.2500e-05.\n",
            "train step: 800, loss: 1.2446\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 1.0377 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.037732-800\n",
            "train step: 832, loss: 1.2015\n",
            "running validation ... validation finished in 0:01:25                                    \n",
            "validation loss: 1.0208 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-1.020764-832\n",
            "train step: 864, loss: 1.1719\n",
            "running validation ... validation finished in 0:01:25                                    \n",
            "validation loss: 0.9938 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-0.993787-864\n",
            "train step: 896, loss: 1.1372\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 0.9760 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-0.976022-896\n",
            "train step: 928, loss: 1.1025\n",
            "running validation ... validation finished in 0:01:26                                    \n",
            "validation loss: 0.9509 ... \n",
            "Check point saved to  /content/drive/My Drive/finsummary/Data/extract_dir/ckpt/ckpt-0.950905-928\n",
            "train step: 950, loss: 1.0755\n",
            "epoch 5 done\n",
            "train step: 960, loss: 1.0910\n",
            "running validation ... "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}