{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5_summarization_github.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPK0/WSoJ2Sj9PPe7ItTkO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/finsummary/blob/master/t5_summarization_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Nw8JCv2Z5W"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u44w4PSWdgr"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmNAke7K2lGq"
      },
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjnnHC223HMo"
      },
      "source": [
        "\n",
        "import glob\n",
        "import logging\n",
        "\n",
        "import time\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from os.path import join, exists\n",
        "from datetime import timedelta\n",
        "import pickle as pkl\n",
        "from toolz import compose\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from torch import nn\n",
        "from itertools import starmap\n",
        "from toolz import curry, reduce\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import init\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from toolz.sandbox import unzip\n",
        "from toolz import curry, concat, compose\n",
        "from toolz import curried\n",
        "import torch.multiprocessing as mp\n",
        "from os.path import basename\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import tensorboardX\n",
        "from time import time\n",
        "from glob import glob\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from transformers import get_constant_schedule_with_warmup\n",
        "import pytorch_lightning as pl\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYmMs7KrWs9d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXM-JYuoW_NC"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfXMo7H4Zzpx"
      },
      "source": [
        "nltk_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igDims8zesqP"
      },
      "source": [
        "def encode_sentences(tokenizer, sentences, max_length, pad_to_max_length=True, return_tensors=\"pt\"):\n",
        "    \"\"\"\n",
        "    This function reads the text files that we prepared and returns them in tokenized form.\n",
        "\n",
        "    Actually tokenizer.batch_encode_plus returns these as a list of dictionaries where \n",
        "    each dictionary contains the word piece indices among other relevant inputs for training & inference\n",
        "    \"\"\"\n",
        "    examples = []\n",
        "    \n",
        "    for text in sentences:\n",
        "        tokenized = tokenizer.batch_encode_plus(\n",
        "            [text], max_length=max_length, pad_to_max_length=pad_to_max_length, return_tensors=return_tensors,\n",
        "        )\n",
        "        examples.append(tokenized)\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAoJHq5PZ2He"
      },
      "source": [
        "import sys\n",
        "class CnnDmDataset(Dataset):\n",
        "    def __init__(self, split: str, path: str,tokenizer,source_length,target_length) -> None:\n",
        "        assert split in ['training', 'validation', 'test']\n",
        "        self._data_path = join(path, split)\n",
        "        self._n_data = _count_data(self._data_path)\n",
        "        self._json_files=os.listdir(join(self._data_path,'extraction'))\n",
        "        self._json_files=list(filter(lambda x: x.endswith('json'),self._json_files))\n",
        "        self._idx_list=[]\n",
        "        self.tokenizer=tokenizer\n",
        "        self.source_length=source_length\n",
        "        self.target_length=target_length\n",
        "        for i in range(len(self._json_files)):\n",
        "          self._idx_list.append(self._json_files[i].split('.')[0])\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._json_files)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        # token = compose(list, _split_words)\n",
        "        idx=self._idx_list[i]\n",
        "        js_path=join(self._data_path,'extraction')\n",
        "        try:\n",
        "          with open(join(js_path, '{}.json'.format(idx)),'r') as f:\n",
        "              js = json.loads(f.read())\n",
        "        except:\n",
        "          print(join(js_path, '{}.json'.format(idx)))\n",
        "          sys.exit()\n",
        "        f_name=js['filename']\n",
        "        ext_fname=idx.split('_')[0]+'.txt'\n",
        "        summ_path=join(self._data_path,'gold_summaries')\n",
        "        with open(join(summ_path,f_name),encoding='utf8') as f:\n",
        "          abs_data = f.read()\n",
        "        abs_sentences=[]\n",
        "        for sent in nltk_tokenizer.tokenize(abs_data):\n",
        "          sent = sent.replace('\\n', ' ')\n",
        "          abs_sentences.append(sent)\n",
        "        # abs_sents = token(abs_sentences)\n",
        "        report_path=join(self._data_path,'annual_reports')\n",
        "        with open(join(report_path,ext_fname),encoding='utf8') as f:\n",
        "          ext_data=f.read()\n",
        "        ext_sentences=[]\n",
        "        for sent in nltk_tokenizer.tokenize(ext_data):\n",
        "          sent=sent.replace('\\n',' ')\n",
        "          ext_sentences.append(sent)\n",
        "        matching_report_sentences=[]\n",
        "        for label in js['extracted_labels']:\n",
        "          matching_report_sentences.append(ext_sentences[label])\n",
        "        encoded_abs=encode_sentences(self.tokenizer,abs_sentences,self.target_length)\n",
        "        encoded_exts=encode_sentences(self.tokenizer,matching_report_sentences,self.target_length)\n",
        "\n",
        "        js['report']=encoded_exts\n",
        "        js['summary']=encoded_abs\n",
        "        return js\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJW-QzRvmTtf"
      },
      "source": [
        "BUCKET_SIZE = 100\n",
        "DATA_DIR = r\"/content/drive/My Drive/finsummary/Data\"\n",
        "max_length=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MmBj7T-mwX4"
      },
      "source": [
        "t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNSks2OgcmXX"
      },
      "source": [
        "class MatchDataset(CnnDmDataset):\n",
        "    \"\"\" single article sentence -> single abstract sentence\n",
        "    (dataset created by greedily matching ROUGE)\n",
        "    \"\"\"\n",
        "    def __init__(self, split,t5_tokenizer,source_length,target_length):\n",
        "        super().__init__(split, DATA_DIR,t5_tokenizer,source_length,target_length)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        js_data = super().__getitem__(i)\n",
        "        input_ids = torch.stack([x[\"input_ids\"].squeeze() for x in js_data['report']])\n",
        "        input_attention_mask = torch.stack([x[\"attention_mask\"].squeeze() for x in js_data['report']])\n",
        "        target_ids= torch.stack([x[\"input_ids\"].squeeze() for x in js_data['summary']])\n",
        "        return input_ids,input_attention_mask,target_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWzfpEwkrzbW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhbKv7aQFr9h"
      },
      "source": [
        "def set_seed(args: argparse.Namespace):\n",
        "    \"\"\"\n",
        "    Set all the seeds to make results replicable\n",
        "    \"\"\"\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR2Ng8eTxSIQ"
      },
      "source": [
        "\n",
        "def coll_fn(data):\n",
        "    input_ids, input_attention_mask,target_ids = unzip(data)\n",
        "    input_id_list =torch.stack(list(concat(input_ids)))\n",
        "    input_mask_list=torch.stack(list(concat(input_attention_mask)))\n",
        "    target_ids_list=torch.stack(list(concat(target_ids)))\n",
        "    return input_id_list, input_mask_list, target_ids_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD7rR0pzymJO"
      },
      "source": [
        "class T5Module(pl.LightningModule):\n",
        "  def __init__(self, hparams: argparse.Namespace, **config_kwargs):\n",
        "        \"Initialize a model.\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "        cache_dir = self.hparams.cache_dir if self.hparams.cache_dir else None\n",
        "        # Read the config file of the T5 model (T5Config)\n",
        "        # AutoConfig allows you to read the configuration for a specified model (e.g. in this case, t5-base)\n",
        "        # Reference: https://huggingface.co/transformers/model_doc/auto.html#autoconfig\n",
        "        self.config = AutoConfig.from_pretrained(self.hparams.model_name_or_path)\n",
        "        # Read the tokenizer of the T5 model (T5Tokenizer)\n",
        "        # AutoTokenizer allows you to read the tokenizer for a specified model (e.g. in this case, t5-base)\n",
        "        # Reference: https://huggingface.co/transformers/model_doc/t5.html#t5tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            self.hparams.model_name_or_path,\n",
        "            cache_dir=cache_dir,\n",
        "        )\n",
        "        # Read the model file for the pre-trained T5 model (T5ForConditionalGeneration)\n",
        "        # AutoModelWithLMHead allows you to read any of the language modelling models from the transformers library (e.g. in this case, t5-base)\n",
        "        # Automodels reference: https://huggingface.co/transformers/model_doc/auto.html#automodel\n",
        "        self.model = AutoModelWithLMHead.from_pretrained(\n",
        "            self.hparams.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in self.hparams.model_name_or_path), # Checkpoint is a TF format\n",
        "            config=self.config,\n",
        "            cache_dir=cache_dir,\n",
        "        )\n",
        "\n",
        "        # Save dataset params\n",
        "        self.dataset_kwargs: dict = dict(\n",
        "            data_dir=self.hparams.data_dir,\n",
        "            max_source_length=self.hparams.max_source_length,\n",
        "            max_target_length=self.hparams.max_target_length,\n",
        "        )\n",
        "  def forward(\n",
        "        self,\n",
        "        input_ids, # Indices of input sequence tokens in the vocabulary. \n",
        "        attention_mask=None, # Mask to avoid performing attention on padding token indices\n",
        "        decoder_input_ids=None, # T5 uses the pad_token_id as the starting token for decoder_input_ids generation.\n",
        "        lm_labels=None # Labels for computing the sequence classification/regression loss (see T5Model). Note: loss is returned when lm_label is provided.\n",
        "        ):\n",
        "        \"\"\"\n",
        "          loss (torch.FloatTensor of shape (1,), optional, returned when lm_label is provided\n",
        "        \"\"\"\n",
        "        # Details on how to use this in the Hugging Face T5 docs: https://huggingface.co/transformers/model_doc/t5.html\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            lm_labels=lm_labels,\n",
        "        )\n",
        "  def get_dataloader(self, type_path: str, batch_size: int, shuffle: bool = False) -> DataLoader:\n",
        "        dataset = MatchDataset(type_path,self.tokenizer,self.hparams.max_source_length,self.hparams.max_target_length)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=coll_fn, shuffle=shuffle)\n",
        "        return dataloader\n",
        "  def train_dataloader(self) -> DataLoader:\n",
        "        dataloader = self.get_dataloader(\"training\", batch_size=self.hparams.train_batch_size, shuffle=True)\n",
        "        t_total = (\n",
        "            (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "            // self.hparams.gradient_accumulation_steps\n",
        "            * float(self.hparams.num_train_epochs)\n",
        "        )\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n",
        "        )\n",
        "        self.lr_scheduler = scheduler\n",
        "        return dataloader\n",
        "\n",
        "  def val_dataloader(self) -> DataLoader:\n",
        "      return self.get_dataloader(\"validation\", batch_size=self.hparams.eval_batch_size)\n",
        "  def configure_optimizers(self):\n",
        "      \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
        "\n",
        "      model = self.model\n",
        "      # Weight decay will not be applied to \"bias\" and \"LayerNorm.weight\" parameters\n",
        "      no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "\n",
        "      # Group parameters to those that will and will not have weight decay applied\n",
        "      optimizer_grouped_parameters = [\n",
        "          {\n",
        "              \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "              \"weight_decay\": self.hparams.weight_decay,\n",
        "          },\n",
        "          {\n",
        "              \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "              \"weight_decay\": 0.0,\n",
        "          },\n",
        "      ]\n",
        "      # Use AdamW as an optimizer\n",
        "      # Intro here: https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
        "      optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n",
        "      self.opt = optimizer\n",
        "      return [optimizer]\n",
        "  def _step(self, batch, return_text=False):\n",
        "        \"\"\"\n",
        "        Runs forward pass and calculates loss per batch. Applied for training_step, and validation_step\n",
        "        \"\"\"\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, y = batch\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone()\n",
        "        # Change pad_token_id to -100\n",
        "        lm_labels[y[:, 1:] == pad_token_id] = -100\n",
        "        # Run forward pass and calculate loss\n",
        "        outputs = self(source_ids, attention_mask=source_mask, decoder_input_ids=y_ids, lm_labels=lm_labels,)\n",
        "        # Only get loss from the output since that's all we need to apply our optimizer\n",
        "        loss = outputs[0]\n",
        "        if return_text:\n",
        "            target_text = [self.tokenizer.decode(ids) for ids in y_ids]\n",
        "            return loss, target_text\n",
        "        else:\n",
        "            return loss\n",
        "  def training_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Runs forward pass, calculates loss, and returns loss (and logs) in a dict\n",
        "        \"\"\"\n",
        "        loss = self._step(batch)\n",
        "\n",
        "        # Notice that each training step loss is recorded on tensorboard, which makes sense since we're tracking loss per batch\n",
        "        tensorboard_logs = {\"train_loss\": loss}\n",
        "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
        "  def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n",
        "        \"\"\"\n",
        "        Adjust weights based on calculated gradients + learning rate scheduler, and refresh gradients\n",
        "        Reference for optimizer_step: https://pytorch-lightning.readthedocs.io/en/latest/optimizers.html\n",
        "        \"\"\"\n",
        "        if self.trainer.use_tpu:\n",
        "            xm.optimizer_step(optimizer)\n",
        "        else:\n",
        "            # Adjust weights based on calculated gradients\n",
        "            optimizer.step()\n",
        "\n",
        "        # Refresh gradients (to zero)\n",
        "        optimizer.zero_grad()\n",
        "        # Update the learning rate scheduler\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "    # Step during validation\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "      \"\"\"\n",
        "      Runs forward pass, calculates loss, and returns loss in a dict\n",
        "      \"\"\"\n",
        "\n",
        "      # Return source and target text to calculate jaccard score only for validation\n",
        "      loss, target_text = self._step(batch, return_text=True)\n",
        "\n",
        "      # preds = self.test_step(batch, batch_idx)\n",
        "      # preds_text = preds[\"preds\"]\n",
        "      # Track jaccard score to get validation accuracy\n",
        "      # jaccard_score = [jaccard(p, t) for p, t in zip(preds_text, target_text)]\n",
        "\n",
        "      return {\"val_loss\": loss}\n",
        "  def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Runs forward pass on test set and returns calculated loss, predictions, and targets\n",
        "        Note: this assumes that your test set has targets (doesn't have for kaggle).\n",
        "        \"\"\"\n",
        "        pad_token_id = self.tokenizer.pad_token_id\n",
        "        source_ids, source_mask, _ = T5Dataset.trim_seq2seq_batch(batch, pad_token_id, test=True)\n",
        "        # NOTE: the following kwargs get more speed and lower quality summaries than those in evaluate_cnn.py\n",
        "        # Generate reference: https://github.com/huggingface/transformers/blob/3e0f06210646a440509efa718b30d18322d6a830/src/transformers/modeling_utils.py#L769\n",
        "        # For the sentiment span extraction task, turning off early stopping proved superior\n",
        "        generated_ids = self.model.generate(\n",
        "            input_ids=source_ids,\n",
        "            attention_mask=source_mask,\n",
        "            num_beams=1,\n",
        "            max_length=80,\n",
        "            repetition_penalty=2.5,\n",
        "            length_penalty=1.0,\n",
        "            early_stopping=True,\n",
        "            use_cache=True,\n",
        "        )\n",
        "        preds = [\n",
        "            self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for g in generated_ids\n",
        "        ]\n",
        "\n",
        "        return {\"preds\": preds}\n",
        "\n",
        "  # Show loss after validation\n",
        "\n",
        "  def validation_end(self, outputs):\n",
        "      \"\"\"\n",
        "      Calculate average loss for all the validation batches\n",
        "      \"\"\"\n",
        "      avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
        "      # jaccard_scores = sum([x[\"jaccard_score\"] for x in outputs], [])\n",
        "      # avg_jaccard_score = np.mean(jaccard_scores)\n",
        "      tensorboard_logs = {\"val_loss\": avg_loss}\n",
        "      return {\"avg_val_loss\": avg_loss,\"log\": tensorboard_logs}\n",
        "\n",
        "  # Step during testing\n",
        "  @staticmethod\n",
        "  def add_model_specific_args(parser, root_dir):\n",
        "      parser.add_argument(\n",
        "          \"--model_name_or_path\",\n",
        "          default=None,\n",
        "          type=str,\n",
        "          required=True,\n",
        "          help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
        "      )\n",
        "      parser.add_argument(\n",
        "          \"--config_name\", default=\"\", type=str, help=\"Pretrained config name or path if not the same as model_name\"\n",
        "      )\n",
        "      parser.add_argument(\n",
        "          \"--tokenizer_name\",\n",
        "          default=\"\",\n",
        "          type=str,\n",
        "          help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
        "      )\n",
        "      parser.add_argument(\n",
        "          \"--cache_dir\",\n",
        "          default=\"\",\n",
        "          type=str,\n",
        "          help=\"Where do you want to store the pre-trained models downloaded from s3\",\n",
        "      )\n",
        "      parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "      parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "      parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "      parser.add_argument(\"--warmup_steps\", default=5, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "      parser.add_argument(\n",
        "          \"--num_train_epochs\", default=3, type=int, help=\"Total number of training epochs to perform.\"\n",
        "      )\n",
        "\n",
        "      parser.add_argument(\"--train_batch_size\", default=1, type=int)\n",
        "      parser.add_argument(\"--eval_batch_size\", default=1, type=int)\n",
        "\n",
        "      parser.add_argument(\n",
        "          \"--max_source_length\",\n",
        "          default=50,\n",
        "          type=int,\n",
        "          help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "          \"than this will be truncated, sequences shorter will be padded.\",\n",
        "      )\n",
        "      parser.add_argument(\n",
        "          \"--max_target_length\",\n",
        "          default=50,\n",
        "          type=int,\n",
        "          help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
        "          \"than this will be truncated, sequences shorter will be padded.\",\n",
        "      )\n",
        "\n",
        "      parser.add_argument(\n",
        "          \"--data_dir\",\n",
        "          default=DATA_DIR,\n",
        "          type=str,\n",
        "          required=True,\n",
        "          help=\"The input data dir. Should contain the dataset files for the text generation task.\",\n",
        "      )\n",
        "      return parser\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3gB-pxKIQrJ"
      },
      "source": [
        "class LoggingCallback(pl.Callback):\n",
        "    def on_validation_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule):\n",
        "        logger.info(\"***** Validation results *****\")\n",
        "        if pl_module.is_logger():\n",
        "            metrics = trainer.callback_metrics\n",
        "            # Log results\n",
        "            for key in sorted(metrics):\n",
        "                if key not in [\"log\", \"progress_bar\"]:\n",
        "                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
        "\n",
        "\n",
        "def add_generic_args(parser, root_dir):\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--n_gpu\", type=int, default=1)\n",
        "    parser.add_argument(\"--n_tpu_cores\", type=int, default=0)\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Whether to run predictions on the test set.\")\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIB5C11MIppK"
      },
      "source": [
        "def generic_train(model: T5Module, args: argparse.Namespace):\n",
        "    # init model\n",
        "    set_seed(args)\n",
        "\n",
        "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n",
        "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "\n",
        "    # Can take out checkpoint saving after each epoch to save memory\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
        "    )\n",
        "\n",
        "    train_params = dict(\n",
        "        accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "        gpus=args.n_gpu,\n",
        "        max_epochs=args.num_train_epochs,\n",
        "        early_stop_callback=False,\n",
        "        gradient_clip_val=args.max_grad_norm,\n",
        "        checkpoint_callback=checkpoint_callback,\n",
        "        callbacks=[LoggingCallback()],\n",
        "    )\n",
        "\n",
        "    if args.fp16:\n",
        "        train_params[\"use_amp\"] = args.fp16\n",
        "        train_params[\"amp_level\"] = args.fp16_opt_level\n",
        "\n",
        "    if args.n_tpu_cores > 0:\n",
        "        global xm\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        train_params[\"num_tpu_cores\"] = args.n_tpu_cores\n",
        "        train_params[\"gpus\"] = 0\n",
        "\n",
        "    if args.n_gpu > 1:\n",
        "        train_params[\"distributed_backend\"] = \"ddp\"\n",
        "\n",
        "    trainer = pl.Trainer(**train_params)\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.fit(model)\n",
        "\n",
        "    return trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm5GH2FrI9It"
      },
      "source": [
        "\n",
        "logging.basicConfig(level = logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main(args):\n",
        "\n",
        "    # If output_dir not provided, a folder will be generated in pwd\n",
        "    if not args.output_dir:\n",
        "        args.output_dir = os.path.join(\"/content/drive/My Drive/finsummary/results\", f\"{args.task}_{time.strftime('%Y%m%d_%H%M%S')}\",)\n",
        "        os.makedirs(args.output_dir)\n",
        "    model = T5Module(args)\n",
        "    trainer = generic_train(model, args)\n",
        "\n",
        "    # Save the last model as model.bin\n",
        "    #checkpoints = list(sorted(glob.glob(os.path.join(args.output_dir, \"checkpointepoch=*.ckpt\"), recursive=True)))\n",
        "    #model = model.load_from_checkpoint(checkpoints[-1])\n",
        "    model.model.save_pretrained(args.output_dir)\n",
        "    # Save tokenizer files\n",
        "    model.tokenizer.save_pretrained('./')\n",
        "    \n",
        "    # Optionally, predict on dev set and write to output_dir\n",
        "    if args.do_predict:\n",
        "        # See https://github.com/huggingface/transformers/issues/3159\n",
        "        # pl use this format to create a checkpoint:\n",
        "        # https://github.com/PyTorchLightning/pytorch-lightning/blob/master\\\n",
        "        # /pytorch_lightning/callbacks/model_checkpoint.py#L169\n",
        "        trainer.test(model)\n",
        "    return trainer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yast6-GEnQk"
      },
      "source": [
        "ARGS_STR = \"\"\"\n",
        "--data_dir=./ \\\n",
        "--model_name_or_path=t5-small \\\n",
        "--learning_rate=3e-5 \\\n",
        "--train_batch_size=1 \\\n",
        "--output_dir=output/ \\\n",
        "--do_train \\\n",
        "--n_gpu=1 \\\n",
        "--num_train_epochs 1 \\\n",
        "--max_source_length 80 \\\n",
        "\"\"\"\n",
        "#\n",
        "#--eval_batch_size=3 \\\n",
        "#--do_predict \\\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "add_generic_args(parser, os.getcwd())\n",
        "parser = T5Module.add_model_specific_args(parser, os.getcwd())\n",
        "args = parser.parse_args(ARGS_STR.split())\n",
        "trainer = main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p75MgXveFarQ"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('t5-base',\n",
        "            cache_dir=\"/root/.cache/torch/transformers/\",\n",
        "        )\n",
        "train_loader = DataLoader(\n",
        "    MatchDataset('training',tokenizer,100,100), batch_size=BUCKET_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=coll_fn\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW88xwB0QdpJ"
      },
      "source": [
        "# data=train_loader.dataset[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDl9FN04RF0m"
      },
      "source": [
        "# len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bygcZ8oxROD8"
      },
      "source": [
        "# source_lists, target_lists = data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0ZnXMZ6Rn2E"
      },
      "source": [
        "# ten_list=[]\n",
        "# for x in source_lists:\n",
        "#   ten_list.append(x['input_ids'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9ouRrgyS2MT"
      },
      "source": [
        "# torch.stack(ten_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd3Gpq0zRdIg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chlfrmDeQr8z"
      },
      "source": [
        "# coll_fn(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58evbSjhQ5m7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}