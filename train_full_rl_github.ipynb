{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_full_rl_github.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/finsummary/blob/master/train_full_rl_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPoYrnYtjvv5",
        "outputId": "b2f512f6-39fc-4c15-9dfc-4bfb94332e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bOX1Ns-Ire5",
        "outputId": "3ceadd2b-3d04-41e9-c4a2-eafd1192232e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 5.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 6.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (47.1.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlX1JA1hk73G"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import pickle as pkl\n",
        "import os\n",
        "from os.path import join, exists\n",
        "from itertools import cycle\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from time import time\n",
        "from datetime import timedelta\n",
        "from itertools import starmap\n",
        "import threading\n",
        "import subprocess as sp\n",
        "import math\n",
        "from collections import Counter, deque\n",
        "import nltk\n",
        "\n",
        "from toolz.sandbox.core import unzip\n",
        "from toolz import identity\n",
        "from toolz.sandbox import unzip\n",
        "from toolz import curry, concat, compose\n",
        "from toolz import curried\n",
        "from toolz import reduce\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.multiprocessing as mp\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import tensorboardX\n",
        "\n",
        "import numpy as np\n",
        "from torch import autograd\n",
        "from torch.nn.utils import clip_grad_norm_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rn0rtK4EdZm",
        "outputId": "078d5e1b-212f-479f-dc41-8eed41b60980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSDbtM8o3PpX"
      },
      "source": [
        "INI = 1e-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meC_UzP7BeeR"
      },
      "source": [
        "def _split_words(texts):\n",
        "    return map(lambda t: t.split(), texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Npn569lhga"
      },
      "source": [
        "import sys\n",
        "class CnnDmDataset(Dataset):\n",
        "    def __init__(self, split: str, path: str) -> None:\n",
        "        assert split in ['training', 'validation', 'test']\n",
        "        self._data_path = join(path, split)\n",
        "        self._n_data = _count_data(self._data_path)\n",
        "        self._json_files=os.listdir(join(self._data_path,'extraction'))\n",
        "        self._json_files=list(filter(lambda x: x.endswith('json'),self._json_files))\n",
        "        self._idx_list=[]\n",
        "        for i in range(len(self._json_files)):\n",
        "          self._idx_list.append(self._json_files[i].split('.')[0])\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._json_files)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        token = compose(list, _split_words)\n",
        "        idx=self._idx_list[i]\n",
        "        js_path=join(self._data_path,'extraction')\n",
        "        try:\n",
        "          with open(join(js_path, '{}.json'.format(idx)),'r') as f:\n",
        "              js = json.loads(f.read())\n",
        "        except:\n",
        "          print(join(js_path, '{}.json'.format(idx)))\n",
        "          sys.exit()\n",
        "          \n",
        "        f_name=js['filename']\n",
        "        summ_path=join(self._data_path,'gold_summaries')\n",
        "        with open(join(summ_path,f_name),encoding='utf8') as f:\n",
        "          abs_data = f.read()\n",
        "        abs_sentences=[]\n",
        "        for sent in tokenizer.tokenize(abs_data):\n",
        "          sent = sent.replace('\\n', ' ')\n",
        "          abs_sentences.append(sent)\n",
        "        # abs_sents = token(abs_sentences)\n",
        "        ext_name=f_name.split('.')[0].split('_')[0]+'.txt'\n",
        "        ext_path=join(self._data_path,'annual_reports')\n",
        "        with open(join(ext_path,ext_name),encoding='utf8') as f:\n",
        "          ext_data=f.read()\n",
        "        ext_sentences=[]\n",
        "        for sent in tokenizer.tokenize(ext_data):\n",
        "          sent = sent.replace('\\n', ' ')\n",
        "          ext_sentences.append(sent)\n",
        "        # ext_sents = token(ext_sentences)\n",
        "        js['summary_sents']=abs_sentences[:40]  \n",
        "        js['report_sents']=ext_sentences[:40]\n",
        "        return js\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myt3XZzRm0BF"
      },
      "source": [
        "def coll_fn(data):\n",
        "    source_lists, target_lists = unzip(data)\n",
        "    # print(len(data))\n",
        "    \n",
        "    # sys.exit()\n",
        "    # NOTE: independent filtering works because\n",
        "    #       source and targets are matched properly by the Dataset\n",
        "    # in_ids=list(concat(source_lists))\n",
        "    # print(len(in_ids))\n",
        "    # print(len(in_ids[0]))\n",
        "    sources = list(filter(bool, concat(source_lists)))\n",
        "    targets = list(filter(bool, concat(target_lists)))\n",
        "    print(len(sources))\n",
        "    print(len(targets))\n",
        "    # sys.exit()\n",
        "    assert all(sources) and all(targets)\n",
        "    return sources, targets\n",
        "\n",
        "def coll_fn_extract(data):\n",
        "    def is_good_data(d):\n",
        "        \"\"\" make sure data is not empty\"\"\"\n",
        "        source_sents, extracts = d\n",
        "        return source_sents and extracts\n",
        "    batch = list(filter(is_good_data, data))\n",
        "    assert all(map(is_good_data, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def tokenize(max_len, texts):\n",
        "    return [t.lower().split()[:max_len] for t in texts]\n",
        "\n",
        "def conver2id(unk, word2id, words_list):\n",
        "    word2id = defaultdict(lambda: unk, word2id)\n",
        "    return [[word2id[w] for w in words] for words in words_list]\n",
        "\n",
        "@curry\n",
        "def prepro_fn(max_src_len, max_tgt_len, batch):\n",
        "    sources, targets = batch\n",
        "    # sources = tokenize(max_src_len, sources)\n",
        "    # targets = tokenize(max_tgt_len, targets)\n",
        "    \n",
        "    for i in range(len(sources)):\n",
        "      sources[i]=sources[i][:max_src_len]\n",
        "    for i in range(len(targets)):\n",
        "      targets[i]=targets[i][:max_tgt_len]\n",
        "    # targets=targets[]\n",
        "    batch = list(zip(sources, targets))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def prepro_fn_extract(max_src_len, max_src_num, batch):\n",
        "    def prepro_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        tokenized_sents = tokenize(max_src_len, source_sents)[:max_src_num]\n",
        "        cleaned_extracts = list(filter(lambda e: e < len(tokenized_sents),\n",
        "                                       extracts))\n",
        "        return tokenized_sents, cleaned_extracts\n",
        "    batch = list(map(prepro_one, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch(unk, word2id, batch):\n",
        "    sources, targets = unzip(batch)\n",
        "    sources = conver2id(unk, word2id, sources)\n",
        "    targets = conver2id(unk, word2id, targets)\n",
        "    batch = list(zip(sources, targets))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch_copy(unk, word2id, batch):\n",
        "    sources, targets = map(list, unzip(batch))\n",
        "    ext_word2id = dict(word2id)\n",
        "    for source in sources:\n",
        "        for word in source:\n",
        "            if word not in ext_word2id:\n",
        "                ext_word2id[word] = len(ext_word2id)\n",
        "    src_exts = conver2id(unk, ext_word2id, sources)\n",
        "    sources = conver2id(unk, word2id, sources)\n",
        "    tar_ins = conver2id(unk, word2id, targets)\n",
        "    targets = conver2id(unk, ext_word2id, targets)\n",
        "    batch = list(zip(sources, src_exts, tar_ins, targets))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch_extract_ptr(unk, word2id, batch):\n",
        "    def convert_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        id_sents = conver2id(unk, word2id, source_sents)\n",
        "        return id_sents, extracts\n",
        "    batch = list(map(convert_one, batch))\n",
        "    return batch\n",
        "\n",
        "@curry\n",
        "def convert_batch_extract_ff(unk, word2id, batch):\n",
        "    def convert_one(sample):\n",
        "        source_sents, extracts = sample\n",
        "        id_sents = conver2id(unk, word2id, source_sents)\n",
        "        binary_extracts = [0] * len(source_sents)\n",
        "        for ext in extracts:\n",
        "            binary_extracts[ext] = 1\n",
        "        return id_sents, binary_extracts\n",
        "    batch = list(map(convert_one, batch))\n",
        "    return batch\n",
        "\n",
        "\n",
        "@curry\n",
        "def pad_batch_tensorize(inputs, pad, cuda=True):\n",
        "    \"\"\"pad_batch_tensorize\n",
        "\n",
        "    :param inputs: List of size B containing torch tensors of shape [T, ...]\n",
        "    :type inputs: List[np.ndarray]\n",
        "    :rtype: TorchTensor of size (B, T, ...)\n",
        "    \"\"\"\n",
        "    tensor_type = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "    batch_size = len(inputs)\n",
        "    max_len = max(len(ids) for ids in inputs)\n",
        "    tensor_shape = (batch_size, max_len)\n",
        "    tensor = tensor_type(*tensor_shape)\n",
        "    tensor.fill_(pad)\n",
        "    for i, ids in enumerate(inputs):\n",
        "        tensor[i, :len(ids)] = tensor_type(ids)\n",
        "    return tensor\n",
        "\n",
        "@curry\n",
        "def batchify_fn(pad, start, end, data, cuda=True):\n",
        "    sources, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_lens = [len(src) for src in sources]\n",
        "    tar_ins = [[start] + tgt for tgt in targets]\n",
        "    targets = [tgt + [end] for tgt in targets]\n",
        "\n",
        "    source = pad_batch_tensorize(sources, pad, cuda)\n",
        "    tar_in = pad_batch_tensorize(tar_ins, pad, cuda)\n",
        "    target = pad_batch_tensorize(targets, pad, cuda)\n",
        "\n",
        "    fw_args = (source, src_lens, tar_in)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "@curry\n",
        "def batchify_fn_copy(pad, start, end, data, cuda=True):\n",
        "    sources, ext_srcs, tar_ins, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_lens = [len(src) for src in sources]\n",
        "    sources = [src for src in sources]\n",
        "    ext_srcs = [ext for ext in ext_srcs]\n",
        "\n",
        "    tar_ins = [[start] + tgt for tgt in tar_ins]\n",
        "    targets = [tgt + [end] for tgt in targets]\n",
        "\n",
        "    source = pad_batch_tensorize(sources, pad, cuda)\n",
        "    tar_in = pad_batch_tensorize(tar_ins, pad, cuda)\n",
        "    target = pad_batch_tensorize(targets, pad, cuda)\n",
        "    ext_src = pad_batch_tensorize(ext_srcs, pad, cuda)\n",
        "\n",
        "    ext_vsize = ext_src.max().item() + 1\n",
        "    fw_args = (source, src_lens, tar_in, ext_src, ext_vsize)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "@curry\n",
        "def batchify_fn_extract_ptr(pad, data, cuda=True):\n",
        "    source_lists, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_nums = list(map(len, source_lists))\n",
        "    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n",
        "\n",
        "    # PAD is -1 (dummy extraction index) for using sequence loss\n",
        "    target = pad_batch_tensorize(targets, pad=-1, cuda=cuda)\n",
        "    remove_last = lambda tgt: tgt[:-1]\n",
        "    tar_in = pad_batch_tensorize(\n",
        "        list(map(remove_last, targets)),\n",
        "        pad=-0, cuda=cuda # use 0 here for feeding first conv sentence repr.\n",
        "    )\n",
        "\n",
        "    fw_args = (sources, src_nums, tar_in)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "@curry\n",
        "def batchify_fn_extract_ff(pad, data, cuda=True):\n",
        "    source_lists, targets = tuple(map(list, unzip(data)))\n",
        "\n",
        "    src_nums = list(map(len, source_lists))\n",
        "    sources = list(map(pad_batch_tensorize(pad=pad, cuda=cuda), source_lists))\n",
        "\n",
        "    tensor_type = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "    target = tensor_type(list(concat(targets)))\n",
        "\n",
        "    fw_args = (sources, src_nums)\n",
        "    loss_args = (target, )\n",
        "    return fw_args, loss_args\n",
        "\n",
        "\n",
        "def _batch2q(loader, prepro, q, single_run=True):\n",
        "    epoch = 0\n",
        "    while True:\n",
        "        for batch in loader:\n",
        "            q.put(prepro(batch))\n",
        "        if single_run:\n",
        "            break\n",
        "        epoch += 1\n",
        "        q.put(epoch)\n",
        "    q.put(None)\n",
        "\n",
        "class BucketedGenerater(object):\n",
        "    def __init__(self, loader, prepro,\n",
        "                 sort_key, batchify,\n",
        "                 single_run=True, queue_size=8, fork=True):\n",
        "        self._loader = loader\n",
        "        self._prepro = prepro\n",
        "        self._sort_key = sort_key\n",
        "        self._batchify = batchify\n",
        "        self._single_run = single_run\n",
        "        if fork:\n",
        "            ctx = mp.get_context('forkserver')\n",
        "            self._queue = ctx.Queue(queue_size)\n",
        "        else:\n",
        "            # for easier debugging\n",
        "            self._queue = None\n",
        "        self._process = None\n",
        "\n",
        "    def __call__(self, batch_size: int):\n",
        "        def get_batches(hyper_batch):\n",
        "            indexes = list(range(0, len(hyper_batch), batch_size))\n",
        "            if not self._single_run:\n",
        "                # random shuffle for training batches\n",
        "                random.shuffle(hyper_batch)\n",
        "                random.shuffle(indexes)\n",
        "            hyper_batch.sort(key=self._sort_key)\n",
        "            for i in indexes:\n",
        "                batch = self._batchify(hyper_batch[i:i+batch_size])\n",
        "                yield batch\n",
        "\n",
        "        if self._queue is not None:\n",
        "            ctx = mp.get_context('forkserver')\n",
        "            self._process = ctx.Process(\n",
        "                target=_batch2q,\n",
        "                args=(self._loader, self._prepro,\n",
        "                      self._queue, self._single_run)\n",
        "            )\n",
        "            self._process.start()\n",
        "            while True:\n",
        "                d = self._queue.get()\n",
        "                if d is None:\n",
        "                    break\n",
        "                if isinstance(d, int):\n",
        "                    print('\\nepoch {} done'.format(d))\n",
        "                    continue\n",
        "                yield from get_batches(d)\n",
        "            self._process.join()\n",
        "        else:\n",
        "            i = 0\n",
        "            while True:\n",
        "                for batch in self._loader:\n",
        "                    yield from get_batches(self._prepro(batch))\n",
        "                if self._single_run:\n",
        "                    break\n",
        "                i += 1\n",
        "                print('\\nepoch {} done'.format(i))\n",
        "\n",
        "    def terminate(self):\n",
        "        if self._process is not None:\n",
        "            self._process.terminate()\n",
        "            self._process.join()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tA4uDQK3Sbv"
      },
      "source": [
        "class PtrExtractorRL(nn.Module):\n",
        "    \"\"\" works only on single sample in RL setting\"\"\"\n",
        "    def __init__(self, ptr_net):\n",
        "        super().__init__()\n",
        "        assert isinstance(ptr_net, LSTMPointerNet)\n",
        "        self._init_h = nn.Parameter(ptr_net._init_h.clone())\n",
        "        self._init_c = nn.Parameter(ptr_net._init_c.clone())\n",
        "        self._init_i = nn.Parameter(ptr_net._init_i.clone())\n",
        "        self._lstm_cell = MultiLayerLSTMCells.convert(ptr_net._lstm)\n",
        "\n",
        "        # attention parameters\n",
        "        self._attn_wm = nn.Parameter(ptr_net._attn_wm.clone())\n",
        "        self._attn_wq = nn.Parameter(ptr_net._attn_wq.clone())\n",
        "        self._attn_v = nn.Parameter(ptr_net._attn_v.clone())\n",
        "\n",
        "        # hop parameters\n",
        "        self._hop_wm = nn.Parameter(ptr_net._hop_wm.clone())\n",
        "        self._hop_wq = nn.Parameter(ptr_net._hop_wq.clone())\n",
        "        self._hop_v = nn.Parameter(ptr_net._hop_v.clone())\n",
        "        self._n_hop = ptr_net._n_hop\n",
        "\n",
        "    def forward(self, attn_mem, n_step):\n",
        "        \"\"\"atten_mem: Tensor of size [num_sents, input_dim]\"\"\"\n",
        "        attn_feat = torch.mm(attn_mem, self._attn_wm)\n",
        "        hop_feat = torch.mm(attn_mem, self._hop_wm)\n",
        "        outputs = []\n",
        "        lstm_in = self._init_i.unsqueeze(0)\n",
        "        lstm_states = (self._init_h.unsqueeze(1), self._init_c.unsqueeze(1))\n",
        "        for _ in range(n_step):\n",
        "            h, c = self._lstm_cell(lstm_in, lstm_states)\n",
        "            query = h[:, -1, :]\n",
        "            for _ in range(self._n_hop):\n",
        "                query = PtrExtractorRL.attention(hop_feat, query,\n",
        "                                                self._hop_v, self._hop_wq)\n",
        "            score = PtrExtractorRL.attention_score(\n",
        "                attn_feat, query, self._attn_v, self._attn_wq)\n",
        "            if self.training:\n",
        "                prob = F.softmax(score, dim=-1)\n",
        "                out = torch.distributions.Categorical(prob)\n",
        "            else:\n",
        "                for o in outputs:\n",
        "                    score[0, o[0, 0].item()][0] = -1e18\n",
        "                out = score.max(dim=1, keepdim=True)[1]\n",
        "            outputs.append(out)\n",
        "            lstm_in = attn_mem[out[0, 0].item()].unsqueeze(0)\n",
        "            lstm_states = (h, c)\n",
        "        return outputs\n",
        "\n",
        "    @staticmethod\n",
        "    def attention_score(attention, query, v, w):\n",
        "        \"\"\" unnormalized attention score\"\"\"\n",
        "        sum_ = attention + torch.mm(query, w)\n",
        "        score = torch.mm(F.tanh(sum_), v.unsqueeze(1)).t()\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(attention, query, v, w):\n",
        "        \"\"\" attention context vector\"\"\"\n",
        "        score = F.softmax(\n",
        "            PtrExtractorRL.attention_score(attention, query, v, w), dim=-1)\n",
        "        output = torch.mm(score, attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class PtrExtractorRLStop(PtrExtractorRL):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        if args:\n",
        "            ptr_net = args[0]\n",
        "        else:\n",
        "            ptr_net = kwargs['ptr_net']\n",
        "        assert isinstance(ptr_net, LSTMPointerNet)\n",
        "        self._stop = nn.Parameter(\n",
        "            torch.Tensor(self._lstm_cell.input_size))\n",
        "        init.uniform_(self._stop, -INI, INI)\n",
        "\n",
        "    def forward(self, attn_mem, n_ext=None):\n",
        "        \"\"\"atten_mem: Tensor of size [num_sents, input_dim]\"\"\"\n",
        "        if n_ext is not None:\n",
        "            return super().forward(attn_mem, n_ext)\n",
        "        max_step = attn_mem.size(0)\n",
        "        attn_mem = torch.cat([attn_mem, self._stop.unsqueeze(0)], dim=0)\n",
        "        attn_feat = torch.mm(attn_mem, self._attn_wm)\n",
        "        hop_feat = torch.mm(attn_mem, self._hop_wm)\n",
        "        outputs = []\n",
        "        dists = []\n",
        "        lstm_in = self._init_i.unsqueeze(0)\n",
        "        lstm_states = (self._init_h.unsqueeze(1), self._init_c.unsqueeze(1))\n",
        "        while True:\n",
        "            h, c = self._lstm_cell(lstm_in, lstm_states)\n",
        "            query = h[:, -1, :]\n",
        "            for _ in range(self._n_hop):\n",
        "                query = PtrExtractorRL.attention(hop_feat, query,\n",
        "                                                self._hop_v, self._hop_wq)\n",
        "            score = PtrExtractorRL.attention_score(\n",
        "                attn_feat, query, self._attn_v, self._attn_wq)\n",
        "            for o in outputs:\n",
        "                score[0, o.item()] = -1e18\n",
        "            if self.training:\n",
        "                prob = F.softmax(score, dim=-1)\n",
        "                m = torch.distributions.Categorical(prob)\n",
        "                dists.append(m)\n",
        "                out = m.sample()\n",
        "            else:\n",
        "                out = score.max(dim=1, keepdim=True)[1]\n",
        "            outputs.append(out)\n",
        "            if out.item() == max_step:\n",
        "                break\n",
        "            lstm_in = attn_mem[out.item()].unsqueeze(0)\n",
        "            lstm_states = (h, c)\n",
        "        if dists:\n",
        "            # return distributions only when not empty (trining)\n",
        "            return outputs, dists\n",
        "        else:\n",
        "            return outputs\n",
        "\n",
        "\n",
        "class PtrScorer(nn.Module):\n",
        "    \"\"\" to be used as critic (predicts a scalar baseline reward)\"\"\"\n",
        "    def __init__(self, ptr_net):\n",
        "        super().__init__()\n",
        "        assert isinstance(ptr_net, LSTMPointerNet)\n",
        "        self._init_h = nn.Parameter(ptr_net._init_h.clone())\n",
        "        self._init_c = nn.Parameter(ptr_net._init_c.clone())\n",
        "        self._init_i = nn.Parameter(ptr_net._init_i.clone())\n",
        "        self._lstm_cell = MultiLayerLSTMCells.convert(ptr_net._lstm)\n",
        "\n",
        "        # attention parameters\n",
        "        self._attn_wm = nn.Parameter(ptr_net._attn_wm.clone())\n",
        "        self._attn_wq = nn.Parameter(ptr_net._attn_wq.clone())\n",
        "        self._attn_v = nn.Parameter(ptr_net._attn_v.clone())\n",
        "\n",
        "        # hop parameters\n",
        "        self._hop_wm = nn.Parameter(ptr_net._hop_wm.clone())\n",
        "        self._hop_wq = nn.Parameter(ptr_net._hop_wq.clone())\n",
        "        self._hop_v = nn.Parameter(ptr_net._hop_v.clone())\n",
        "        self._n_hop = ptr_net._n_hop\n",
        "\n",
        "        # regression layer\n",
        "        self._score_linear = nn.Linear(self._lstm_cell.input_size, 1)\n",
        "\n",
        "    def forward(self, attn_mem, n_step):\n",
        "        \"\"\"atten_mem: Tensor of size [num_sents, input_dim]\"\"\"\n",
        "        attn_feat = torch.mm(attn_mem, self._attn_wm)\n",
        "        hop_feat = torch.mm(attn_mem, self._hop_wm)\n",
        "        scores = []\n",
        "        lstm_in = self._init_i.unsqueeze(0)\n",
        "        lstm_states = (self._init_h.unsqueeze(1), self._init_c.unsqueeze(1))\n",
        "        for _ in range(n_step):\n",
        "            h, c = self._lstm_cell(lstm_in, lstm_states)\n",
        "            query = h[:, -1, :]\n",
        "            for _ in range(self._n_hop):\n",
        "                query = PtrScorer.attention(hop_feat, hop_feat, query,\n",
        "                                            self._hop_v, self._hop_wq)\n",
        "            output = PtrScorer.attention(\n",
        "                attn_mem, attn_feat, query, self._attn_v, self._attn_wq)\n",
        "            score = self._score_linear(output)\n",
        "            scores.append(score)\n",
        "            lstm_in = output\n",
        "        return scores\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(attention, attention_feat, query, v, w):\n",
        "        \"\"\" attention context vector\"\"\"\n",
        "        sum_ = attention_feat + torch.mm(query, w)\n",
        "        score = F.softmax(torch.mm(F.tanh(sum_), v.unsqueeze(1)).t(), dim=-1)\n",
        "        output = torch.mm(score, attention)\n",
        "        return output\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    \"\"\" shared encoder between actor/critic\"\"\"\n",
        "    def __init__(self, sent_encoder, art_encoder,\n",
        "                 extractor, art_batcher):\n",
        "        super().__init__()\n",
        "        self._sent_enc = sent_encoder\n",
        "        self._art_enc = art_encoder\n",
        "        self._ext = PtrExtractorRLStop(extractor)\n",
        "        self._scr = PtrScorer(extractor)\n",
        "        self._batcher = art_batcher\n",
        "\n",
        "    def forward(self, raw_article_sents, n_abs=None):\n",
        "        article_sent = self._batcher(raw_article_sents)\n",
        "        enc_sent = self._sent_enc(article_sent).unsqueeze(0)\n",
        "        enc_art = self._art_enc(enc_sent).squeeze(0)\n",
        "        if n_abs is not None and not self.training:\n",
        "            n_abs = min(len(raw_article_sents), n_abs)\n",
        "        if n_abs is None:\n",
        "            outputs = self._ext(enc_art)\n",
        "        else:\n",
        "            outputs = self._ext(enc_art, n_abs)\n",
        "        if self.training:\n",
        "            if n_abs is None:\n",
        "                n_abs = len(outputs[0])\n",
        "            scores = self._scr(enc_art, n_abs)\n",
        "            return outputs, scores\n",
        "        else:\n",
        "            return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51EC_SwnDQ3a"
      },
      "source": [
        "class PtrExtractSumm(nn.Module):\n",
        "    \"\"\" rnn-ext\"\"\"\n",
        "    def __init__(self, emb_dim, vocab_size, conv_hidden,\n",
        "                 lstm_hidden, lstm_layer, bidirectional,\n",
        "                 n_hop=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self._sent_enc = ConvSentEncoder(\n",
        "            vocab_size, emb_dim, conv_hidden, dropout)\n",
        "        self._art_enc = LSTMEncoder(\n",
        "            3*conv_hidden, lstm_hidden, lstm_layer,\n",
        "            dropout=dropout, bidirectional=bidirectional\n",
        "        )\n",
        "        enc_out_dim = lstm_hidden * (2 if bidirectional else 1)\n",
        "        self._extractor = LSTMPointerNet(\n",
        "            enc_out_dim, lstm_hidden, lstm_layer,\n",
        "            dropout, n_hop\n",
        "        )\n",
        "\n",
        "    def forward(self, article_sents, sent_nums, target):\n",
        "        enc_out = self._encode(article_sents, sent_nums)\n",
        "        bs, nt = target.size()\n",
        "        d = enc_out.size(2)\n",
        "        ptr_in = torch.gather(\n",
        "            enc_out, dim=1, index=target.unsqueeze(2).expand(bs, nt, d)\n",
        "        )\n",
        "        output = self._extractor(enc_out, sent_nums, ptr_in)\n",
        "        return output\n",
        "\n",
        "    def extract(self, article_sents, sent_nums=None, k=4):\n",
        "        enc_out = self._encode(article_sents, sent_nums)\n",
        "        output = self._extractor.extract(enc_out, sent_nums, k)\n",
        "        return output\n",
        "\n",
        "    def _encode(self, article_sents, sent_nums):\n",
        "        if sent_nums is None:  # test-time excode only\n",
        "            \n",
        "            enc_sent = self._sent_enc(article_sents[0]).unsqueeze(0)\n",
        "        else:\n",
        "            max_n = max(sent_nums)\n",
        "            enc_sents = [self._sent_enc(art_sent)\n",
        "                         for art_sent in article_sents]\n",
        "            def zero(n, device):\n",
        "                z = torch.zeros(n, self._art_enc.input_size).to(device)\n",
        "                return z\n",
        "            enc_sent = torch.stack(\n",
        "                [torch.cat([s, zero(max_n-n, s.device)], dim=0)\n",
        "                   if n != max_n\n",
        "                 else s\n",
        "                 for s, n in zip(enc_sents, sent_nums)],\n",
        "                dim=0\n",
        "            )\n",
        "        lstm_out = self._art_enc(enc_sent, sent_nums)\n",
        "        return lstm_out\n",
        "\n",
        "    def set_embedding(self, embedding):\n",
        "        self._sent_enc.set_embedding(embedding)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVe7bMySGhDi"
      },
      "source": [
        "def get_basic_grad_fn(net, clip_grad, max_grad=1e2):\n",
        "    def f():\n",
        "        grad_norm = clip_grad_norm_(\n",
        "            [p for p in net.parameters() if p.requires_grad], clip_grad)\n",
        "        grad_norm = grad_norm.item()\n",
        "        if max_grad is not None and grad_norm >= max_grad:\n",
        "            print('WARNING: Exploding Gradients {:.2f}'.format(grad_norm))\n",
        "            grad_norm = max_grad\n",
        "        grad_log = {}\n",
        "        grad_log['grad_norm'] = grad_norm\n",
        "        return grad_log\n",
        "    return f\n",
        "\n",
        "@curry\n",
        "def compute_loss(net, criterion, fw_args, loss_args):\n",
        "    loss = criterion(*((net(*fw_args),) + loss_args))\n",
        "    return loss\n",
        "\n",
        "@curry\n",
        "def val_step(loss_step, fw_args, loss_args):\n",
        "    loss = loss_step(fw_args, loss_args)\n",
        "    return loss.size(0), loss.sum().item()\n",
        "\n",
        "@curry\n",
        "def basic_validate(net, criterion, val_batches):\n",
        "    print('running validation ... ', end='')\n",
        "    net.eval()\n",
        "    start = time()\n",
        "    with torch.no_grad():\n",
        "        validate_fn = val_step(compute_loss(net, criterion))\n",
        "        n_data, tot_loss = reduce(\n",
        "            lambda a, b: (a[0]+b[0], a[1]+b[1]),\n",
        "            starmap(validate_fn, val_batches),\n",
        "            (0, 0)\n",
        "        )\n",
        "    val_loss = tot_loss / n_data\n",
        "    print(\n",
        "        'validation finished in {}                                    '.format(\n",
        "            timedelta(seconds=int(time()-start)))\n",
        "    )\n",
        "    print('validation loss: {:.4f} ... '.format(val_loss))\n",
        "    return {'loss': val_loss}\n",
        "\n",
        "\n",
        "class BasicPipeline(object):\n",
        "    def __init__(self, name, net,\n",
        "                 train_batcher, val_batcher, batch_size,\n",
        "                 val_fn, criterion, optim, grad_fn=None):\n",
        "        self.name = name\n",
        "        self._net = net\n",
        "        self._train_batcher = train_batcher\n",
        "        self._val_batcher = val_batcher\n",
        "        self._criterion = criterion\n",
        "        self._opt = optim\n",
        "        # grad_fn is calleble without input args that modifyies gradient\n",
        "        # it should return a dictionary of logging values\n",
        "        self._grad_fn = grad_fn\n",
        "        self._val_fn = val_fn\n",
        "\n",
        "        self._n_epoch = 0  # epoch not very useful?\n",
        "        self._batch_size = batch_size\n",
        "        self._batches = self.batches()\n",
        "\n",
        "    def batches(self):\n",
        "        while True:\n",
        "            for fw_args, bw_args in self._train_batcher(self._batch_size):\n",
        "                yield fw_args, bw_args\n",
        "            self._n_epoch += 1\n",
        "\n",
        "    def get_loss_args(self, net_out, bw_args):\n",
        "        if isinstance(net_out, tuple):\n",
        "            loss_args = net_out + bw_args\n",
        "        else:\n",
        "            loss_args = (net_out, ) + bw_args\n",
        "        return loss_args\n",
        "\n",
        "    def train_step(self):\n",
        "        # forward pass of model\n",
        "        self._net.train()\n",
        "        fw_args, bw_args = next(self._batches)\n",
        "        net_out = self._net(*fw_args)\n",
        "\n",
        "        # get logs and output for logging, backward\n",
        "        log_dict = {}\n",
        "        loss_args = self.get_loss_args(net_out, bw_args)\n",
        "\n",
        "        # backward and update ( and optional gradient monitoring )\n",
        "        loss = self._criterion(*loss_args).mean()\n",
        "        loss.backward()\n",
        "        log_dict['loss'] = loss.item()\n",
        "        if self._grad_fn is not None:\n",
        "            log_dict.update(self._grad_fn())\n",
        "        self._opt.step()\n",
        "        self._net.zero_grad()\n",
        "\n",
        "        return log_dict\n",
        "\n",
        "    def validate(self):\n",
        "        return self._val_fn(self._val_batcher(self._batch_size))\n",
        "\n",
        "    def checkpoint(self, save_path, step, val_metric=None):\n",
        "        save_dict = {}\n",
        "        if val_metric is not None:\n",
        "            name = 'ckpt-{:6f}-{}'.format(val_metric, step)\n",
        "            save_dict['val_metric'] = val_metric\n",
        "        else:\n",
        "            name = 'ckpt-{}'.format(step)\n",
        "\n",
        "        save_dict['state_dict'] = self._net.state_dict()\n",
        "        save_dict['optimizer'] = self._opt.state_dict()\n",
        "        torch.save(save_dict, join(save_path, name))\n",
        "        print(\"Check point at \",  join(save_path, name))\n",
        "    def terminate(self):\n",
        "        self._train_batcher.terminate()\n",
        "        self._val_batcher.terminate()\n",
        "\n",
        "\n",
        "class BasicTrainer(object):\n",
        "    \"\"\" Basic trainer with minimal function and early stopping\"\"\"\n",
        "    def __init__(self, pipeline, save_dir, ckpt_freq, patience,\n",
        "                 scheduler=None, val_mode='loss'):\n",
        "        assert isinstance(pipeline, BasicPipeline)\n",
        "        assert val_mode in ['loss', 'score']\n",
        "        self._pipeline = pipeline\n",
        "        self._save_dir = save_dir\n",
        "        self._logger = tensorboardX.SummaryWriter(join(save_dir, 'log'))\n",
        "        try:\n",
        "          os.makedirs(join(save_dir, 'rl_ckpt/ckpt'))\n",
        "        except:\n",
        "          print(\"Path already present\")\n",
        "        self._ckpt_freq = ckpt_freq\n",
        "        self._patience = patience\n",
        "        self._sched = scheduler\n",
        "        self._val_mode = val_mode\n",
        "\n",
        "        self._step = 0\n",
        "        self._running_loss = None\n",
        "        # state vars for early stopping\n",
        "        self._current_p = 0\n",
        "        self._best_val = None\n",
        "\n",
        "    def log(self, log_dict):\n",
        "        loss = log_dict['loss'] if 'loss' in log_dict else log_dict['reward']\n",
        "        if self._running_loss is not None:\n",
        "            self._running_loss = 0.99*self._running_loss + 0.01*loss\n",
        "        else:\n",
        "            self._running_loss = loss\n",
        "        print('train step: {}, {}: {:.4f}\\r'.format(\n",
        "            self._step,\n",
        "            'loss' if 'loss' in log_dict else 'reward',\n",
        "            self._running_loss), end='')\n",
        "        for key, value in log_dict.items():\n",
        "            self._logger.add_scalar(\n",
        "                '{}_{}'.format(key, self._pipeline.name), value, self._step)\n",
        "\n",
        "    def validate(self):\n",
        "        print()\n",
        "        val_log = self._pipeline.validate()\n",
        "        for key, value in val_log.items():\n",
        "            self._logger.add_scalar(\n",
        "                'val_{}_{}'.format(key, self._pipeline.name),\n",
        "                value, self._step\n",
        "            )\n",
        "        if 'reward' in val_log:\n",
        "            val_metric = val_log['reward']\n",
        "        else:\n",
        "            val_metric = (val_log['loss'] if self._val_mode == 'loss'\n",
        "                          else val_log['score'])\n",
        "        return val_metric\n",
        "\n",
        "    def checkpoint(self):\n",
        "        val_metric = self.validate()\n",
        "        self._pipeline.checkpoint(\n",
        "            join(self._save_dir, 'rl_dir/ckpt_new'), self._step, val_metric)\n",
        "        print(\"Checkpoint at \",join(self._save_dir, 'rl_dir/ckpt'))\n",
        "        if isinstance(self._sched, ReduceLROnPlateau):\n",
        "            self._sched.step(val_metric)\n",
        "        else:\n",
        "            self._sched.step()\n",
        "        stop = self.check_stop(val_metric)\n",
        "        return stop\n",
        "\n",
        "    def check_stop(self, val_metric):\n",
        "        if self._best_val is None:\n",
        "            self._best_val = val_metric\n",
        "        elif ((val_metric < self._best_val and self._val_mode == 'loss')\n",
        "              or (val_metric > self._best_val and self._val_mode == 'score')):\n",
        "            self._current_p = 0\n",
        "            self._best_val = val_metric\n",
        "        else:\n",
        "            self._current_p += 1\n",
        "        return self._current_p >= self._patience\n",
        "\n",
        "    def train(self):\n",
        "        try:\n",
        "            start = time()\n",
        "            print('Start training')\n",
        "            while True:\n",
        "                log_dict = self._pipeline.train_step()\n",
        "                self._step += 1\n",
        "                self.log(log_dict)\n",
        "\n",
        "                if self._step % self._ckpt_freq == 0:\n",
        "                    stop = self.checkpoint()\n",
        "                    if stop:\n",
        "                        break\n",
        "            print('Training finised in ', timedelta(seconds=time()-start))\n",
        "        finally:\n",
        "            self._pipeline.terminate()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyjmzQ8gIXEy"
      },
      "source": [
        "def make_n_grams(seq, n):\n",
        "    \"\"\" return iterator \"\"\"\n",
        "    ngrams = (tuple(seq[i:i+n]) for i in range(len(seq)-n+1))\n",
        "    return ngrams\n",
        "\n",
        "def _n_gram_match(summ, ref, n):\n",
        "    summ_grams = Counter(make_n_grams(summ, n))\n",
        "    ref_grams = Counter(make_n_grams(ref, n))\n",
        "    grams = min(summ_grams, ref_grams, key=len)\n",
        "    count = sum(min(summ_grams[g], ref_grams[g]) for g in grams)\n",
        "    return count\n",
        "\n",
        "@curry\n",
        "def compute_rouge_n(output, reference, n=1, mode='f'):\n",
        "    \"\"\" compute ROUGE-N for a single pair of summary and reference\"\"\"\n",
        "    assert mode in list('fpr')  # F-1, precision, recall\n",
        "    match = _n_gram_match(reference, output, n)\n",
        "    if match == 0:\n",
        "        score = 0.0\n",
        "    else:\n",
        "        precision = match / len(output)\n",
        "        recall = match / len(reference)\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "        if mode == 'p':\n",
        "            score = precision\n",
        "        elif mode == 'r':\n",
        "            score = recall\n",
        "        else:\n",
        "            score = f_score\n",
        "    return score\n",
        "\n",
        "\n",
        "def _lcs_dp(a, b):\n",
        "    \"\"\" compute the len dp of lcs\"\"\"\n",
        "    dp = [[0 for _ in range(0, len(b)+1)]\n",
        "          for _ in range(0, len(a)+1)]\n",
        "    # dp[i][j]: lcs_len(a[:i], b[:j])\n",
        "    for i in range(1, len(a)+1):\n",
        "        for j in range(1, len(b)+1):\n",
        "            if a[i-1] == b[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "    return dp\n",
        "\n",
        "def _lcs_len(a, b):\n",
        "    \"\"\" compute the length of longest common subsequence between a and b\"\"\"\n",
        "    dp = _lcs_dp(a, b)\n",
        "    return dp[-1][-1]\n",
        "\n",
        "@curry\n",
        "def compute_rouge_l(output, reference, mode='f'):\n",
        "    \"\"\" compute ROUGE-L for a single pair of summary and reference\n",
        "    output, reference are list of words\n",
        "    \"\"\"\n",
        "    assert mode in list('fpr')  # F-1, precision, recall\n",
        "    lcs = _lcs_len(output, reference)\n",
        "    if lcs == 0:\n",
        "        score = 0.0\n",
        "    else:\n",
        "        precision = lcs / len(output)\n",
        "        recall = lcs / len(reference)\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "        if mode == 'p':\n",
        "            score = precision\n",
        "        if mode == 'r':\n",
        "            score = recall\n",
        "        else:\n",
        "            score = f_score\n",
        "    return score\n",
        "\n",
        "\n",
        "def _lcs(a, b):\n",
        "    \"\"\" compute the longest common subsequence between a and b\"\"\"\n",
        "    dp = _lcs_dp(a, b)\n",
        "    i = len(a)\n",
        "    j = len(b)\n",
        "    lcs = deque()\n",
        "    while (i > 0 and j > 0):\n",
        "        if a[i-1] == b[j-1]:\n",
        "            lcs.appendleft(a[i-1])\n",
        "            i -= 1\n",
        "            j -= 1\n",
        "        elif dp[i-1][j] >= dp[i][j-1]:\n",
        "            i -= 1\n",
        "        else:\n",
        "            j -= 1\n",
        "    assert len(lcs) == dp[-1][-1]\n",
        "    return lcs\n",
        "\n",
        "def compute_rouge_l_summ(summs, refs, mode='f'):\n",
        "    \"\"\" summary level ROUGE-L\"\"\"\n",
        "    assert mode in list('fpr')  # F-1, precision, recall\n",
        "    tot_hit = 0\n",
        "    ref_cnt = Counter(concat(refs))\n",
        "    summ_cnt = Counter(concat(summs))\n",
        "    for ref in refs:\n",
        "        for summ in summs:\n",
        "            lcs = _lcs(summ, ref)\n",
        "            for gram in lcs:\n",
        "                if ref_cnt[gram] > 0 and summ_cnt[gram] > 0:\n",
        "                    tot_hit += 1\n",
        "                ref_cnt[gram] -= 1\n",
        "                summ_cnt[gram] -= 1\n",
        "    if tot_hit == 0:\n",
        "        score = 0.0\n",
        "    else:\n",
        "        precision = tot_hit / sum((len(s) for s in summs))\n",
        "        recall = tot_hit / sum((len(r) for r in refs))\n",
        "        f_score = 2 * (precision * recall) / (precision + recall)\n",
        "        if mode == 'p':\n",
        "            score = precision\n",
        "        if mode == 'r':\n",
        "            score = recall\n",
        "        else:\n",
        "            score = f_score\n",
        "    return score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZV5yRHvPI39"
      },
      "source": [
        "\n",
        "def a2c_validate(agent, abstractor, loader):\n",
        "    agent.eval()\n",
        "    start = time()\n",
        "    print('start running validation...', end='')\n",
        "    avg_reward = 0\n",
        "    i = 0\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for x,(art_batch, abs_batch) in enumerate(loader):\n",
        "            print(x+1,\" batches validated\")\n",
        "            x+=1\n",
        "            ext_sents = []\n",
        "            ext_inds = []\n",
        "            for raw_arts in art_batch:\n",
        "                indices = agent(raw_arts)\n",
        "                ext_inds += [(len(ext_sents), len(indices)-1)]\n",
        "                ext_sents += [raw_arts[idx.item()]\n",
        "                              for idx in indices if idx.item() < len(raw_arts)]\n",
        "            all_summs = abstractor(ext_sents)\n",
        "            for (j, n), abs_sents in zip(ext_inds, abs_batch):\n",
        "                summs = all_summs[j:j+n]\n",
        "                # python ROUGE-1 (not official evaluation)\n",
        "                avg_reward += compute_rouge_n(list(concat(summs)),\n",
        "                                              list(concat(abs_sents)), n=1)\n",
        "                i += 1\n",
        "                print(list(concat(summs)))\n",
        "            print(avg_reward/(i))\n",
        "            break\n",
        "    avg_reward /= (i)\n",
        "    print('finished in {}! avg reward: {:.2f}'.format(\n",
        "        timedelta(seconds=int(time()-start)), avg_reward))\n",
        "    return {'reward': avg_reward}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgqtLAJkSpZs"
      },
      "source": [
        "\n",
        "def a2c_train_step(agent, abstractor, loader, opt, grad_fn,\n",
        "                   gamma=0.99, reward_fn=compute_rouge_l,\n",
        "                   stop_reward_fn=compute_rouge_n(n=1), stop_coeff=1.0):\n",
        "    opt.zero_grad()\n",
        "    indices = []\n",
        "    probs = []\n",
        "    baselines = []\n",
        "    ext_sents = []\n",
        "    art_batch, abs_batch = next(loader)\n",
        "    for raw_arts in art_batch:\n",
        "        (inds, ms), bs = agent(raw_arts)\n",
        "        baselines.append(bs)\n",
        "        indices.append(inds)\n",
        "        probs.append(ms)\n",
        "        ext_sents += [raw_arts[idx.item()]\n",
        "                      for idx in inds if idx.item() < len(raw_arts)]\n",
        "    with torch.no_grad():\n",
        "        summaries = abstractor(ext_sents)\n",
        "    i = 0\n",
        "    rewards = []\n",
        "    avg_reward = 0\n",
        "    for inds, abss in zip(indices, abs_batch):\n",
        "        rs = ([reward_fn(summaries[i+j], abss[j])\n",
        "              for j in range(min(len(inds)-1, len(abss)))]\n",
        "              + [0 for _ in range(max(0, len(inds)-1-len(abss)))]\n",
        "              + [stop_coeff*stop_reward_fn(\n",
        "                  list(concat(summaries[i:i+len(inds)-1])),\n",
        "                  list(concat(abss)))])\n",
        "        assert len(rs) == len(inds)\n",
        "        avg_reward += rs[-1]/stop_coeff\n",
        "        i += len(inds)-1\n",
        "        # compute discounted rewards\n",
        "        R = 0\n",
        "        disc_rs = []\n",
        "        for r in rs[::-1]:\n",
        "            R = r + gamma * R\n",
        "            disc_rs.insert(0, R)\n",
        "        rewards += disc_rs\n",
        "    indices = list(concat(indices))\n",
        "    probs = list(concat(probs))\n",
        "    baselines = list(concat(baselines))\n",
        "    # standardize rewards\n",
        "    reward = torch.Tensor(rewards).to(baselines[0].device)\n",
        "    reward = (reward - reward.mean()) / (\n",
        "        reward.std() + float(np.finfo(np.float32).eps))\n",
        "    baseline = torch.cat(baselines).squeeze()\n",
        "    avg_advantage = 0\n",
        "    losses = []\n",
        "    for action, p, r, b in zip(indices, probs, reward, baseline):\n",
        "        advantage = r - b\n",
        "        avg_advantage += advantage\n",
        "        losses.append(-p.log_prob(action)\n",
        "                      * (advantage/len(indices))) # divide by T*B\n",
        "    critic_loss = F.mse_loss(baseline, reward)\n",
        "    # print(losses)\n",
        "    # print(len(losses))\n",
        "    # print(critic_loss)\n",
        "    losses.append(torch.stack([critic_loss]))\n",
        "    # print(len(losses))\n",
        "    # losses=torch.stack(losses).squeeze()\n",
        "    # critic_loss=torch.stack([critic_loss])\n",
        "    # backprop and update\n",
        "    # print(baseline.shape)\n",
        "    # print(reward.shape)\n",
        "    # print(critic_loss.shape)\n",
        "    # print(critic_loss)\n",
        "    # print(losses.shape)\n",
        "    # print(torch.cat((critic_loss,losses),dim=0).shape)\n",
        "    # print(losses)\n",
        "    autograd.backward(\n",
        "        torch.stack(losses).squeeze(),\n",
        "        torch.tensor([torch.ones(1).to(critic_loss.device)]*(len(losses)))\n",
        "    )\n",
        "    grad_log = grad_fn()\n",
        "    opt.step()\n",
        "    log_dict = {}\n",
        "    log_dict.update(grad_log)\n",
        "    log_dict['reward'] = avg_reward/len(art_batch)\n",
        "    log_dict['advantage'] = avg_advantage.item()/len(indices)\n",
        "    log_dict['mse'] = critic_loss.item()\n",
        "    assert not math.isnan(log_dict['grad_norm'])\n",
        "    return log_dict\n",
        "\n",
        "\n",
        "def get_grad_fn(agent, clip_grad, max_grad=1e2):\n",
        "    \"\"\" monitor gradient for each sub-component\"\"\"\n",
        "    params = [p for p in agent.parameters()]\n",
        "    def f():\n",
        "        grad_log = {}\n",
        "        for n, m in agent.named_children():\n",
        "            tot_grad = 0\n",
        "            for p in m.parameters():\n",
        "                if p.grad is not None:\n",
        "                    tot_grad += p.grad.norm(2) ** 2\n",
        "            tot_grad = tot_grad ** (1/2)\n",
        "            grad_log['grad_norm'+n] = tot_grad.item()\n",
        "        grad_norm = clip_grad_norm_(\n",
        "            [p for p in params if p.requires_grad], clip_grad)\n",
        "        grad_norm = grad_norm.item()\n",
        "        if max_grad is not None and grad_norm >= max_grad:\n",
        "            print('WARNING: Exploding Gradients {:.2f}'.format(grad_norm))\n",
        "            grad_norm = max_grad\n",
        "        grad_log['grad_norm'] = grad_norm\n",
        "        return grad_log\n",
        "    return f\n",
        "\n",
        "\n",
        "class A2CPipeline(BasicPipeline):\n",
        "    def __init__(self, name,\n",
        "                 net, abstractor,\n",
        "                 train_batcher, val_batcher,\n",
        "                 optim, grad_fn,\n",
        "                 reward_fn, gamma,\n",
        "                 stop_reward_fn, stop_coeff):\n",
        "        self.name = name\n",
        "        self._net = net\n",
        "        self._train_batcher = train_batcher\n",
        "        self._val_batcher = val_batcher\n",
        "        self._opt = optim\n",
        "        self._grad_fn = grad_fn\n",
        "\n",
        "        self._abstractor = abstractor\n",
        "        self._gamma = gamma\n",
        "        self._reward_fn = reward_fn\n",
        "        self._stop_reward_fn = stop_reward_fn\n",
        "        self._stop_coeff = stop_coeff\n",
        "\n",
        "        self._n_epoch = 0  # epoch not very useful?\n",
        "\n",
        "    def batches(self):\n",
        "        raise NotImplementedError('A2C does not use batcher')\n",
        "\n",
        "    def train_step(self):\n",
        "        # forward pass of model\n",
        "        self._net.train()\n",
        "        log_dict = a2c_train_step(\n",
        "            self._net, self._abstractor,\n",
        "            self._train_batcher,\n",
        "            self._opt, self._grad_fn,\n",
        "            self._gamma, self._reward_fn,\n",
        "            self._stop_reward_fn, self._stop_coeff\n",
        "        )\n",
        "        return log_dict\n",
        "\n",
        "    def validate(self):\n",
        "        return a2c_validate(self._net, self._abstractor, self._val_batcher)\n",
        "\n",
        "    def checkpoint(self, *args, **kwargs):\n",
        "        # explicitly use inherited function in case I forgot :)\n",
        "        return super().checkpoint(*args, **kwargs)\n",
        "\n",
        "    def terminate(self):\n",
        "        pass  # No extra processs so do nothing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PwU_BRsZufZ"
      },
      "source": [
        "def dot_attention_score(key,query):\n",
        "  return query.matmul(key.transpose(1,2))\n",
        "\n",
        "def prob_normalize(score,mask):\n",
        "  score=score.masked_fill(mask==0,-1e18)\n",
        "  norm_score=F.softmax(score,dim=-1)\n",
        "  return norm_score\n",
        "\n",
        "def attention_aggregate(value, score):\n",
        "  output=score.matmul(value)\n",
        "  return output\n",
        "\n",
        "def step_attention(query, key, value, mem_mask=None):\n",
        "    \"\"\" query[(Bs), B, D], key[B, T, D], value[B, T, D]\"\"\"\n",
        "    score = dot_attention_score(key, query.unsqueeze(-2))\n",
        "    if mem_mask is None:\n",
        "        norm_score = F.softmax(score, dim=-1)\n",
        "    else:\n",
        "        norm_score = prob_normalize(score, mem_mask)\n",
        "    output = attention_aggregate(value, norm_score)\n",
        "    return output.squeeze(-2), norm_score.squeeze(-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1XpzIweZ0Ia"
      },
      "source": [
        "def len_mask(lens, device):\n",
        "    \"\"\" users are resposible for shaping\n",
        "    Return: tensor_type [B, T]\n",
        "    \"\"\"\n",
        "    max_len = max(lens)\n",
        "    batch_size = len(lens)\n",
        "    mask = torch.ByteTensor(batch_size, max_len).to(device)\n",
        "    mask.fill_(0)\n",
        "    for i, l in enumerate(lens):\n",
        "        mask[i, :l].fill_(1)\n",
        "    return mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlX5EJ1aZhB6"
      },
      "source": [
        "INIT = 1e-2\n",
        "\n",
        "\n",
        "class Seq2SeqSumm(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim,\n",
        "                 n_hidden, bidirectional, n_layer, dropout=0.0):\n",
        "        super().__init__()\n",
        "        # embedding weight parameter is shared between encoder, decoder,\n",
        "        # and used as final projection layer to vocab logit\n",
        "        # can initialize with pretrained word vectors\n",
        "        self._embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self._enc_lstm = nn.LSTM(\n",
        "            emb_dim, n_hidden, n_layer,\n",
        "            bidirectional=bidirectional, dropout=dropout\n",
        "        )\n",
        "        # initial encoder LSTM states are learned parameters\n",
        "        state_layer = n_layer * (2 if bidirectional else 1)\n",
        "        self._init_enc_h = nn.Parameter(\n",
        "            torch.Tensor(state_layer, n_hidden)\n",
        "        )\n",
        "        self._init_enc_c = nn.Parameter(\n",
        "            torch.Tensor(state_layer, n_hidden)\n",
        "        )\n",
        "        init.uniform_(self._init_enc_h, -INIT, INIT)\n",
        "        init.uniform_(self._init_enc_c, -INIT, INIT)\n",
        "\n",
        "        # vanillat lstm / LNlstm\n",
        "        self._dec_lstm = MultiLayerLSTMCells(\n",
        "            2*emb_dim, n_hidden, n_layer, dropout=dropout\n",
        "        )\n",
        "        # project encoder final states to decoder initial states\n",
        "        enc_out_dim = n_hidden * (2 if bidirectional else 1)\n",
        "        self._dec_h = nn.Linear(enc_out_dim, n_hidden, bias=False)\n",
        "        self._dec_c = nn.Linear(enc_out_dim, n_hidden, bias=False)\n",
        "        # multiplicative attention\n",
        "        self._attn_wm = nn.Parameter(torch.Tensor(enc_out_dim, n_hidden))\n",
        "        self._attn_wq = nn.Parameter(torch.Tensor(n_hidden, n_hidden))\n",
        "        init.xavier_normal_(self._attn_wm)\n",
        "        init.xavier_normal_(self._attn_wq)\n",
        "        # project decoder output to emb_dim, then\n",
        "        # apply weight matrix from embedding layer\n",
        "        self._projection = nn.Sequential(\n",
        "            nn.Linear(2*n_hidden, n_hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(n_hidden, emb_dim, bias=False)\n",
        "        )\n",
        "        # functional object for easier usage\n",
        "        self._decoder = AttentionalLSTMDecoder(\n",
        "            self._embedding, self._dec_lstm,\n",
        "            self._attn_wq, self._projection\n",
        "        )\n",
        "\n",
        "    def forward(self, article, art_lens, abstract):\n",
        "        attention, init_dec_states = self.encode(article, art_lens)\n",
        "        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n",
        "        logit = self._decoder((attention, mask), init_dec_states, abstract)\n",
        "        return logit\n",
        "\n",
        "    def encode(self, article, art_lens=None):\n",
        "        size = (\n",
        "            self._init_enc_h.size(0),\n",
        "            len(art_lens) if art_lens else 1,\n",
        "            self._init_enc_h.size(1)\n",
        "        )\n",
        "        init_enc_states = (\n",
        "            self._init_enc_h.unsqueeze(1).expand(*size),\n",
        "            self._init_enc_c.unsqueeze(1).expand(*size)\n",
        "        )\n",
        "        enc_art, final_states = lstm_encoder(\n",
        "            article, self._enc_lstm, art_lens,\n",
        "            init_enc_states, self._embedding\n",
        "        )\n",
        "        if self._enc_lstm.bidirectional:\n",
        "            h, c = final_states\n",
        "            final_states = (\n",
        "                torch.cat(h.chunk(2, dim=0), dim=2),\n",
        "                torch.cat(c.chunk(2, dim=0), dim=2)\n",
        "            )\n",
        "        init_h = torch.stack([self._dec_h(s)\n",
        "                              for s in final_states[0]], dim=0)\n",
        "        init_c = torch.stack([self._dec_c(s)\n",
        "                              for s in final_states[1]], dim=0)\n",
        "        init_dec_states = (init_h, init_c)\n",
        "        attention = torch.matmul(enc_art, self._attn_wm).transpose(0, 1)\n",
        "        init_attn_out = self._projection(torch.cat(\n",
        "            [init_h[-1], sequence_mean(attention, art_lens, dim=1)], dim=1\n",
        "        ))\n",
        "        return attention, (init_dec_states, init_attn_out)\n",
        "\n",
        "    def batch_decode(self, article, art_lens, go, eos, max_len):\n",
        "        \"\"\" greedy decode support batching\"\"\"\n",
        "        batch_size = len(art_lens)\n",
        "        attention, init_dec_states = self.encode(article, art_lens)\n",
        "        mask = len_mask(art_lens, attention.device).unsqueeze(-2)\n",
        "        attention = (attention, mask)\n",
        "        tok = torch.LongTensor([go]*batch_size).to(article.device)\n",
        "        outputs = []\n",
        "        attns = []\n",
        "        states = init_dec_states\n",
        "        for i in range(max_len):\n",
        "            tok, states, attn_score = self._decoder.decode_step(\n",
        "                tok, states, attention)\n",
        "            outputs.append(tok[:, 0])\n",
        "            attns.append(attn_score)\n",
        "        return outputs, attns\n",
        "\n",
        "    def decode(self, article, go, eos, max_len):\n",
        "        attention, init_dec_states = self.encode(article)\n",
        "        attention = (attention, None)\n",
        "        tok = torch.LongTensor([go]).to(article.device)\n",
        "        outputs = []\n",
        "        attns = []\n",
        "        states = init_dec_states\n",
        "        for i in range(max_len):\n",
        "            tok, states, attn_score = self._decoder.decode_step(\n",
        "                tok, states, attention)\n",
        "            if tok[0, 0].item() == eos:\n",
        "                break\n",
        "            outputs.append(tok[0, 0].item())\n",
        "            attns.append(attn_score.squeeze(0))\n",
        "        return outputs, attns\n",
        "\n",
        "    def set_embedding(self, embedding):\n",
        "        \"\"\"embedding is the weight matrix\"\"\"\n",
        "        print(self._embedding.weight.size())\n",
        "        print(embedding.size())\n",
        "        assert self._embedding.weight.size() == embedding.size()\n",
        "        self._embedding.weight.data.copy_(embedding)\n",
        "\n",
        "\n",
        "class AttentionalLSTMDecoder(object):\n",
        "    def __init__(self, embedding, lstm, attn_w, projection):\n",
        "        super().__init__()\n",
        "        self._embedding = embedding\n",
        "        self._lstm = lstm\n",
        "        self._attn_w = attn_w\n",
        "        self._projection = projection\n",
        "\n",
        "    def __call__(self, attention, init_states, target):\n",
        "        max_len = target.size(1)\n",
        "        states = init_states\n",
        "        logits = []\n",
        "        for i in range(max_len):\n",
        "            tok = target[:, i:i+1]\n",
        "            logit, states, _ = self._step(tok, states, attention)\n",
        "            logits.append(logit)\n",
        "        logit = torch.stack(logits, dim=1)\n",
        "        return logit\n",
        "\n",
        "    def _step(self, tok, states, attention):\n",
        "        prev_states, prev_out = states\n",
        "        lstm_in = torch.cat(\n",
        "            [self._embedding(tok).squeeze(1), prev_out],\n",
        "            dim=1\n",
        "        )\n",
        "        states = self._lstm(lstm_in, prev_states)\n",
        "        lstm_out = states[0][-1]\n",
        "        query = torch.mm(lstm_out, self._attn_w)\n",
        "        attention, attn_mask = attention\n",
        "        context, score = step_attention(\n",
        "            query, attention, attention, attn_mask)\n",
        "        dec_out = self._projection(torch.cat([lstm_out, context], dim=1))\n",
        "        states = (states, dec_out)\n",
        "        logit = torch.mm(dec_out, self._embedding.weight.t())\n",
        "        return logit, states, score\n",
        "\n",
        "    def decode_step(self, tok, states, attention):\n",
        "        logit, states, score = self._step(tok, states, attention)\n",
        "        out = torch.max(logit, dim=1, keepdim=True)[1]\n",
        "        return out, states, score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uhYtNdbamOM"
      },
      "source": [
        "def conver2id(unk, word2id, words_list):\n",
        "    word2id = defaultdict(lambda: unk, word2id)\n",
        "    return [[word2id[w] for w in words] for words in words_list]\n",
        "    \n",
        "@curry\n",
        "def pad_batch_tensorize(inputs, pad, cuda=True):\n",
        "    \"\"\"pad_batch_tensorize\n",
        "\n",
        "    :param inputs: List of size B containing torch tensors of shape [T, ...]\n",
        "    :type inputs: List[np.ndarray]\n",
        "    :rtype: TorchTensor of size (B, T, ...)\n",
        "    \"\"\"\n",
        "    tensor_type = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
        "    batch_size = len(inputs)\n",
        "    max_len = max(len(ids) for ids in inputs)\n",
        "    tensor_shape = (batch_size, max_len)\n",
        "    tensor = tensor_type(*tensor_shape)\n",
        "    tensor.fill_(pad)\n",
        "    for i, ids in enumerate(inputs):\n",
        "        tensor[i, :len(ids)] = tensor_type(ids)\n",
        "    return tensor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnAjAOr7bBTG"
      },
      "source": [
        "class DecodeDataset(CnnDmDataset):\n",
        "    \"\"\" get the article sentences only (for decoding use)\"\"\"\n",
        "    def __init__(self, split):\n",
        "        assert split in ['validation', 'test']\n",
        "        super().__init__(split, DATASET_DIR)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        js_data = super().__getitem__(i)\n",
        "        art_sents = js_data['article']\n",
        "        return art_sents\n",
        "\n",
        "\n",
        "def make_html_safe(s):\n",
        "    \"\"\"Rouge use html, has to make output html safe\"\"\"\n",
        "    return s.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n",
        "\n",
        "\n",
        "def load_best_ckpt(model_dir, reverse=False):\n",
        "    \"\"\" reverse=False->loss, reverse=True->reward/score\"\"\"\n",
        "    ckpts = os.listdir(join(model_dir, 'ckpt'))\n",
        "    ckpt_matcher = re.compile('^ckpt-.*-[0-9]*')\n",
        "    ckpts = sorted([c for c in ckpts if ckpt_matcher.match(c)],\n",
        "                   key=lambda c: float(c.split('-')[1]), reverse=reverse)\n",
        "    print('loading checkpoint {}...'.format(ckpts[0]))\n",
        "    ckpt = torch.load(\n",
        "        join(model_dir, 'ckpt/{}'.format(ckpts[0]))\n",
        "    )['state_dict']\n",
        "    return ckpt\n",
        "\n",
        "\n",
        "class Abstractor(object):\n",
        "    def __init__(self, abs_dir, max_len=30, cuda=True):\n",
        "        abs_meta = json.load(open(join(abs_dir, 'meta.json')))\n",
        "        assert abs_meta['net'] == 'base_abstractor'\n",
        "        abs_args = abs_meta['net_args']\n",
        "        abs_ckpt = load_best_ckpt(abs_dir)\n",
        "        word2id = pkl.load(open(join(abs_dir, 'vocab.pkl'), 'rb'))\n",
        "        print(abs_args)\n",
        "        # sys.exit()\n",
        "        abstractor = CopySumm(**abs_args)\n",
        "        abstractor.load_state_dict(abs_ckpt)\n",
        "        self._device = torch.device('cuda' if cuda else 'cpu')\n",
        "        self._net = abstractor.to(self._device)\n",
        "        self._word2id = word2id\n",
        "        self._id2word = {i: w for w, i in word2id.items()}\n",
        "        self._max_len = max_len\n",
        "\n",
        "    def _prepro(self, raw_article_sents):\n",
        "        ext_word2id = dict(self._word2id)\n",
        "        ext_id2word = dict(self._id2word)\n",
        "        for raw_words in raw_article_sents:\n",
        "            for w in raw_words:\n",
        "                if not w in ext_word2id:\n",
        "                    ext_word2id[w] = len(ext_word2id)\n",
        "                    ext_id2word[len(ext_id2word)] = w\n",
        "        articles = conver2id(UNK, self._word2id, raw_article_sents)\n",
        "        art_lens = [len(art) for art in articles]\n",
        "        article = pad_batch_tensorize(articles, PAD, cuda=False\n",
        "                                     ).to(self._device)\n",
        "        extend_arts = conver2id(UNK, ext_word2id, raw_article_sents)\n",
        "        extend_art = pad_batch_tensorize(extend_arts, PAD, cuda=False\n",
        "                                        ).to(self._device)\n",
        "        extend_vsize = len(ext_word2id)\n",
        "        dec_args = (article, art_lens, extend_art, extend_vsize,\n",
        "                    START, END, UNK, self._max_len)\n",
        "        return dec_args, ext_id2word\n",
        "\n",
        "    def __call__(self, raw_article_sents):\n",
        "        self._net.eval()\n",
        "        dec_args, id2word = self._prepro(raw_article_sents)\n",
        "        decs, attns = self._net.batch_decode(*dec_args)\n",
        "        def argmax(arr, keys):\n",
        "            return arr[max(range(len(arr)), key=lambda i: keys[i].item())]\n",
        "        dec_sents = []\n",
        "        for i, raw_words in enumerate(raw_article_sents):\n",
        "            dec = []\n",
        "            for id_, attn in zip(decs, attns):\n",
        "                if id_[i] == END:\n",
        "                    break\n",
        "                elif id_[i] == UNK:\n",
        "                    dec.append(argmax(raw_words, attn[i]))\n",
        "                else:\n",
        "                    dec.append(id2word[id_[i].item()])\n",
        "            dec_sents.append(dec)\n",
        "        return dec_sents\n",
        "\n",
        "\n",
        "class BeamAbstractor(Abstractor):\n",
        "    def __call__(self, raw_article_sents, beam_size=5, diverse=1.0):\n",
        "        self._net.eval()\n",
        "        dec_args, id2word = self._prepro(raw_article_sents)\n",
        "        dec_args = (*dec_args, beam_size, diverse)\n",
        "        all_beams = self._net.batched_beamsearch(*dec_args)\n",
        "        all_beams = list(starmap(_process_beam(id2word),\n",
        "                                 zip(all_beams, raw_article_sents)))\n",
        "        return all_beams\n",
        "\n",
        "@curry\n",
        "def _process_beam(id2word, beam, art_sent):\n",
        "    def process_hyp(hyp):\n",
        "        seq = []\n",
        "        for i, attn in zip(hyp.sequence[1:], hyp.attns[:-1]):\n",
        "            if i == UNK:\n",
        "                copy_word = art_sent[max(range(len(art_sent)),\n",
        "                                         key=lambda j: attn[j].item())]\n",
        "                seq.append(copy_word)\n",
        "            else:\n",
        "                seq.append(id2word[i])\n",
        "        hyp.sequence = seq\n",
        "        del hyp.hists\n",
        "        del hyp.attns\n",
        "        return hyp\n",
        "    return list(map(process_hyp, beam))\n",
        "\n",
        "\n",
        "class Extractor(object):\n",
        "    def __init__(self, ext_dir, max_ext=5, cuda=True):\n",
        "        ext_meta = json.load(open(join(ext_dir, 'meta.json')))\n",
        "        if ext_meta['net'] == 'ml_ff_extractor':\n",
        "            ext_cls = ExtractSumm\n",
        "        elif ext_meta['net'] == 'ml_rnn_extractor':\n",
        "            ext_cls = PtrExtractSumm\n",
        "        else:\n",
        "            raise ValueError()\n",
        "        ext_ckpt = load_best_ckpt(ext_dir)\n",
        "        ext_args = ext_meta['net_args']\n",
        "        extractor = ext_cls(**ext_args)\n",
        "        extractor.load_state_dict(ext_ckpt)\n",
        "        word2id = pkl.load(open(join(ext_dir, 'vocab.pkl'), 'rb'))\n",
        "        self._device = torch.device('cuda' if cuda else 'cpu')\n",
        "        self._net = extractor.to(self._device)\n",
        "        self._word2id = word2id\n",
        "        self._id2word = {i: w for w, i in word2id.items()}\n",
        "        self._max_ext = max_ext\n",
        "\n",
        "    def __call__(self, raw_article_sents):\n",
        "        self._net.eval()\n",
        "        n_art = len(raw_article_sents)\n",
        "        articles = conver2id(UNK, self._word2id, raw_article_sents)\n",
        "        article = pad_batch_tensorize(articles, PAD, cuda=False\n",
        "                                     ).to(self._device)\n",
        "        indices = self._net.extract([article], k=min(n_art, self._max_ext))\n",
        "        return indices\n",
        "\n",
        "\n",
        "class ArticleBatcher(object):\n",
        "    def __init__(self, word2id, cuda=True):\n",
        "        self._device = torch.device('cuda' if cuda else 'cpu')\n",
        "        self._word2id = word2id\n",
        "        self._device = torch.device('cuda' if cuda else 'cpu')\n",
        "\n",
        "    def __call__(self, raw_article_sents):\n",
        "        articles = conver2id(UNK, self._word2id, raw_article_sents)\n",
        "        article = pad_batch_tensorize(articles, PAD, cuda=False\n",
        "                                     ).to(self._device)\n",
        "        return article\n",
        "\n",
        "class RLExtractor(object):\n",
        "    def __init__(self, ext_dir, cuda=True):\n",
        "        ext_meta = json.load(open(join(ext_dir, 'meta.json')))\n",
        "        assert ext_meta['net'] == 'rnn-ext_abs_rl'\n",
        "        ext_args = ext_meta['net_args']['extractor']['net_args']\n",
        "        word2id = pkl.load(open(join(ext_dir, 'agent_vocab.pkl'), 'rb'))\n",
        "        extractor = PtrExtractSumm(**ext_args)\n",
        "        agent = ActorCritic(extractor._sent_enc,\n",
        "                            extractor._art_enc,\n",
        "                            extractor._extractor,\n",
        "                            ArticleBatcher(word2id, cuda))\n",
        "        ext_ckpt = load_best_ckpt(ext_dir, reverse=True)\n",
        "        agent.load_state_dict(ext_ckpt)\n",
        "        self._device = torch.device('cuda' if cuda else 'cpu')\n",
        "        self._net = agent.to(self._device)\n",
        "        self._word2id = word2id\n",
        "        self._id2word = {i: w for w, i in word2id.items()}\n",
        "\n",
        "    def __call__(self, raw_article_sents):\n",
        "        self._net.eval()\n",
        "        indices = self._net(raw_article_sents)\n",
        "        return indices\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBrYifdDdYbN"
      },
      "source": [
        "class RLDataset(CnnDmDataset):\n",
        "    \"\"\" get the article sentences only (for decoding use)\"\"\"\n",
        "    def __init__(self, split):\n",
        "        super().__init__(split, DATA_DIR)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        js_data = super().__getitem__(i)\n",
        "        art_sents = js_data['report_sents']\n",
        "        abs_sents = js_data['summary_sents']\n",
        "        # print('Sentences Extracted')\n",
        "        return art_sents, abs_sents\n",
        "\n",
        "def load_ext_net(ext_dir):\n",
        "    ext_meta = json.load(open(join(ext_dir, 'meta.json')))\n",
        "    assert ext_meta['net'] == 'ml_rnn_extractor'\n",
        "    ext_ckpt = load_best_ckpt(ext_dir)\n",
        "    ext_args = ext_meta['net_args']\n",
        "    vocab = pkl.load(open(join(ext_dir, 'vocab.pkl'), 'rb'))\n",
        "    ext = PtrExtractSumm(**ext_args)\n",
        "    ext.load_state_dict(ext_ckpt)\n",
        "    return ext, vocab\n",
        "\n",
        "\n",
        "def configure_net(abs_dir, ext_dir, cuda):\n",
        "    \"\"\" load pretrained sub-modules and build the actor-critic network\"\"\"\n",
        "    # load pretrained abstractor model\n",
        "    if abs_dir is not None:\n",
        "        abstractor = Abstractor(abs_dir, MAX_ABS_LEN, cuda)\n",
        "    else:\n",
        "        abstractor = identity\n",
        "\n",
        "    # load ML trained extractor net and buiild RL agent\n",
        "    extractor, agent_vocab = load_ext_net(ext_dir)\n",
        "    agent = ActorCritic(extractor._sent_enc,\n",
        "                        extractor._art_enc,\n",
        "                        extractor._extractor,\n",
        "                        ArticleBatcher(agent_vocab, cuda))\n",
        "    if cuda:\n",
        "        agent = agent.cuda()\n",
        "\n",
        "    net_args = {}\n",
        "    net_args['abstractor'] = (None if abs_dir is None\n",
        "                              else json.load(open(join(abs_dir, 'meta.json'))))\n",
        "    net_args['extractor'] = json.load(open(join(ext_dir, 'meta.json')))\n",
        "\n",
        "    return agent, agent_vocab, abstractor, net_args\n",
        "\n",
        "\n",
        "def configure_training(opt, lr, clip_grad, lr_decay, batch_size,\n",
        "                       gamma, reward, stop_coeff, stop_reward):\n",
        "    assert opt in ['adam']\n",
        "    opt_kwargs = {}\n",
        "    opt_kwargs['lr'] = lr\n",
        "\n",
        "    train_params = {}\n",
        "    train_params['optimizer']      = (opt, opt_kwargs)\n",
        "    train_params['clip_grad_norm'] = clip_grad\n",
        "    train_params['batch_size']     = batch_size\n",
        "    train_params['lr_decay']       = lr_decay\n",
        "    train_params['gamma']          = gamma\n",
        "    train_params['reward']         = reward\n",
        "    train_params['stop_coeff']     = stop_coeff\n",
        "    train_params['stop_reward']    = stop_reward\n",
        "\n",
        "    return train_params\n",
        "\n",
        "def build_batchers(batch_size):\n",
        "    def coll(batch):\n",
        "        art_batch, abs_batch = unzip(batch)\n",
        "        # art_batch = list(filter(bool, concat(art_batch)))\n",
        "        # abs_batch = list(filter(bool, concat(abs_batch)))\n",
        "        art_sents = list(filter(bool, map(tokenize(None), art_batch)))\n",
        "        abs_sents = list(filter(bool, map(tokenize(None), abs_batch)))\n",
        "        # print(art_sents)\n",
        "        # sys.exit()\n",
        "        # print(\"colled\")\n",
        "        return art_sents, abs_sents\n",
        "    loader = DataLoader(\n",
        "        RLDataset('training'), batch_size=batch_size,\n",
        "        shuffle=True, num_workers=4,\n",
        "        collate_fn=coll\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        RLDataset('validation'), batch_size=batch_size,\n",
        "        shuffle=False, num_workers=4,\n",
        "        collate_fn=coll\n",
        "    )\n",
        "    return cycle(loader), val_loader\n",
        "\n",
        "\n",
        "def train(args):\n",
        "    if not exists(args.path):\n",
        "        os.makedirs(args.path)\n",
        "\n",
        "    # make net\n",
        "    agent, agent_vocab, abstractor, net_args = configure_net(\n",
        "        args.abs_dir, args.ext_dir, args.cuda)\n",
        "\n",
        "    # configure training setting\n",
        "    assert args.stop > 0\n",
        "    train_params = configure_training(\n",
        "        'adam', args.lr, args.clip, args.decay, args.batch,\n",
        "        args.gamma, args.reward, args.stop, 'rouge-1'\n",
        "    )\n",
        "    train_batcher, val_batcher = build_batchers(args.batch)\n",
        "    # TODO different reward\n",
        "    reward_fn = compute_rouge_l\n",
        "    stop_reward_fn = compute_rouge_n(n=1)\n",
        "\n",
        "    # save abstractor binary\n",
        "    if args.abs_dir is not None:\n",
        "        abs_ckpt = {}\n",
        "        abs_ckpt['state_dict'] = load_best_ckpt(args.abs_dir)\n",
        "        abs_vocab = pkl.load(open(join(args.abs_dir, 'vocab.pkl'), 'rb'))\n",
        "        abs_dir = join(args.path, 'abstractor')\n",
        "        try:\n",
        "          os.makedirs(join(abs_dir, 'ckpt'))\n",
        "        except:\n",
        "          print(\"Already Present\")\n",
        "        with open(join(abs_dir, 'meta.json'), 'w') as f:\n",
        "            json.dump(net_args['abstractor'], f, indent=4)\n",
        "        torch.save(abs_ckpt, join(abs_dir, 'ckpt/ckpt-0-0'))\n",
        "        with open(join(abs_dir, 'vocab.pkl'), 'wb') as f:\n",
        "            pkl.dump(abs_vocab, f)\n",
        "    # save configuration\n",
        "    meta = {}\n",
        "    meta['net']           = 'rnn-ext_abs_rl'\n",
        "    meta['net_args']      = net_args\n",
        "    meta['train_params']  = train_params\n",
        "    with open(join(args.path, 'rl_dir/meta.json'), 'w') as f:\n",
        "        json.dump(meta, f, indent=4)\n",
        "    with open(join(args.path, 'rl_dir/agent_vocab.pkl'), 'wb') as f:\n",
        "        pkl.dump(agent_vocab, f)\n",
        "\n",
        "    # prepare trainer\n",
        "    grad_fn = get_grad_fn(agent, args.clip)\n",
        "    optimizer = optim.Adam(agent.parameters(), **train_params['optimizer'][1])\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'max', verbose=True,\n",
        "                                  factor=args.decay, min_lr=0,\n",
        "                                  patience=args.lr_p)\n",
        "\n",
        "    pipeline = A2CPipeline(meta['net'], agent, abstractor,\n",
        "                           train_batcher, val_batcher,\n",
        "                           optimizer, grad_fn,\n",
        "                           reward_fn, args.gamma,\n",
        "                           stop_reward_fn, args.stop)\n",
        "    trainer = BasicTrainer(pipeline, args.path,\n",
        "                           args.ckpt_freq, args.patience, scheduler,\n",
        "                           val_mode='score')\n",
        "\n",
        "    print('start training with the following hyper-parameters:')\n",
        "    print(meta)\n",
        "    trainer.train()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4rUnf3YzaM0"
      },
      "source": [
        "DATA_DIR='/content/drive/My Drive/finsummary/Data/'\n",
        "MAX_ABS_LEN = 32\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "START = 2\n",
        "END = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2ickSR38v8X"
      },
      "source": [
        "# f.re%tb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA1t-SjxyupE"
      },
      "source": [
        "\n",
        "class Args():\n",
        "  path=DATA_DIR\n",
        "  abs_dir=join(DATA_DIR,'abs_dir')\n",
        "  ext_dir=join(DATA_DIR,'extract_dir')\n",
        "  ckpt=join(DATA_DIR,'rl_dir')\n",
        "  reward='rouge-l'\n",
        "  lr=1e-4\n",
        "  lr_p=0\n",
        "  decay=0.5\n",
        "  gamma=0.95\n",
        "  stop=1.0\n",
        "  clip=2.0\n",
        "  # batch=32\n",
        "  patience=3\n",
        "  cuda=torch.cuda.is_available()\n",
        "\n",
        "  # net_type='rnn'\n",
        "\n",
        "  # vsize=20000\n",
        "                          \n",
        "  # emb_dim=300\n",
        "                        \n",
        "  # w2v=w2v_bin_path\n",
        "\n",
        "  # n_hidden=256\n",
        "  # lstm_layer=1            \n",
        "  # n_layer=2\n",
        "  # conv_hidden=100    \n",
        "  # no_bi=False\n",
        "  # max_word=100\n",
        "  # max_sent=60\n",
        "  # lstm_hidden=256\n",
        "      # length limit\n",
        "  # max_art=100\n",
        "  # max_abs=50\n",
        "  # lr=1e-3\n",
        "  # decay=0.5\n",
        "  # lr_p=0\n",
        "  # clip=2.0\n",
        "  batch=2\n",
        "  ckpt_freq=16\n",
        "  patience=5\n",
        "  debug=True\n",
        "  no_cuda=True\n",
        "  # bi = not no_bi\n",
        "  cuda = torch.cuda.is_available() and not no_cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZQiuSFxTy5C",
        "outputId": "ecc10417-881c-48b3-dce4-17c9e38cb905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Aug 23 12:33:37 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    27W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMImqg5h4V-G",
        "outputId": "f9c3d628-692a-4b11-906d-422191142e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "args=Args()\n",
        "train(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading checkpoint ckpt-0.049020-1920...\n",
            "{'vocab_size': 20004, 'emb_dim': 300, 'n_hidden': 256, 'bidirectional': True, 'n_layer': 2}\n",
            "256\n",
            "loading checkpoint ckpt-0.950905-928...\n",
            "loading checkpoint ckpt-0.049020-1920...\n",
            "Already Present\n",
            "Path already present\n",
            "start training with the following hyper-parameters:\n",
            "{'net': 'rnn-ext_abs_rl', 'net_args': {'abstractor': {'net': 'base_abstractor', 'net_args': {'vocab_size': 20004, 'emb_dim': 300, 'n_hidden': 256, 'bidirectional': True, 'n_layer': 2}, 'traing_params': {'optimizer': ['adam', {'lr': 0.001}], 'clip_grad_norm': 2.0, 'batch_size': 16, 'lr_decay': 0.5}}, 'extractor': {'net': 'ml_rnn_extractor', 'net_args': {'vocab_size': 20004, 'emb_dim': 300, 'conv_hidden': 100, 'lstm_hidden': 256, 'lstm_layer': 1, 'bidirectional': True}, 'traing_params': {'optimizer': ['adam', {'lr': 0.001}], 'clip_grad_norm': 2.0, 'batch_size': 16, 'lr_decay': 0.5}}}, 'train_params': {'optimizer': ('adam', {'lr': 0.0001}), 'clip_grad_norm': 2.0, 'batch_size': 2, 'lr_decay': 0.5, 'gamma': 0.95, 'reward': 'rouge-l', 'stop_coeff': 1.0, 'stop_reward': 'rouge-1'}}\n",
            "Start training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1558: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train step: 16, reward: 0.2072\n",
            "start running validation...1  batches validated\n",
            "['we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', 'we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', 'skills.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.']\n",
            "0.5720006391818473\n",
            "finished in 0:00:19! avg reward: 0.57\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.572001-16\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "train step: 32, reward: 0.2353\n",
            "start running validation...1  batches validated\n",
            "['we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', 'we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.', 'this', 'is', 'where', 'we', 'are', 'differentiated', 'by', 'our', 'embedded', 'product', 'experience', 'framework,', 'which', 'is', 'designed', 'to', 'deliver', 'a', 'consistently', 'great', 'experience', 'for', 'shoppers', 'and', 'retailers', 'across', 'our', 'portfolio.', 'furthermore,', 'we', 'remain', 'the', 'only', 'european', 'reit', 'with', 'strategic', 'exposure', 'to', 'the', 'premium', 'outlets', 'market.', 'our', 'investments', 'in', 'value', 'retail', 'and', 'via', 'outlets', 'materially', 'boosted', 'our', 'portfolio', 'returns', 'in', '2015.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'skills.', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.', 'board', 'changes', 'and', 'senior', 'management', 'overview', 'i', 'am', 'pleased', 'to', 'announce', 'that,', 'with', 'effect', 'from', '24', 'january', '2018,', 'chris', 'pullen', 'is', 'appointed', 'as', 'chief', 'executive', 'of', 'staffline', 'group', 'plc.', 'chris', 'succeeds', 'andy', 'hogarth', 'who', 'will', 'step', 'down', 'from', 'his', 'current', 'role', 'while', 'remaining', 'on', 'the', 'board', 'as', 'a', 'non-executive', 'director.', 'andy', 'hogarth', 'has', 'been', 'chief', 'executive', 'of', 'staffline', 'group', 'plc', 'since', '2003,', 'during', 'which', 'time', 'the', 'group', 'has', 'grown', 'to', 'almost', '£1bn', 'in', 'revenue.']\n",
            "0.5842284618474416\n",
            "finished in 0:00:19! avg reward: 0.58\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.584228-32\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "train step: 48, reward: 0.2610\n",
            "start running validation...1  batches validated\n",
            "['we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.', 'this', 'is', 'where', 'we', 'are', 'differentiated', 'by', 'our', 'embedded', 'product', 'experience', 'framework,', 'which', 'is', 'designed', 'to', 'deliver', 'a', 'consistently', 'great', 'experience', 'for', 'shoppers', 'and', 'retailers', 'across', 'our', 'portfolio.', 'furthermore,', 'we', 'remain', 'the', 'only', 'european', 'reit', 'with', 'strategic', 'exposure', 'to', 'the', 'premium', 'outlets', 'market.', 'our', 'investments', 'in', 'value', 'retail', 'and', 'via', 'outlets', 'materially', 'boosted', 'our', 'portfolio', 'returns', 'in', '2015.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'skills.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.', 'board', 'changes', 'and', 'senior', 'management', 'overview', 'i', 'am', 'pleased', 'to', 'announce', 'that,', 'with', 'effect', 'from', '24', 'january', '2018,', 'chris', 'pullen', 'is', 'appointed', 'as', 'chief', 'executive', 'of', 'staffline', 'group', 'plc.', 'chris', 'succeeds', 'andy', 'hogarth', 'who', 'will', 'step', 'down', 'from', 'his', 'current', 'role', 'while', 'remaining', 'on', 'the', 'board', 'as', 'a', 'non-executive', 'director.', 'andy', 'hogarth', 'has', 'been', 'chief', 'executive', 'of', 'staffline', 'group', 'plc', 'since', '2003,', 'during', 'which', 'time', 'the', 'group', 'has', 'grown', 'to', 'almost', '£1bn', 'in', 'revenue.']\n",
            "0.5842284618474416\n",
            "finished in 0:00:19! avg reward: 0.58\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.584228-48\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "Epoch     3: reducing learning rate of group 0 to 5.0000e-05.\n",
            "train step: 64, reward: 0.2891\n",
            "start running validation...1  batches validated\n",
            "['we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.', 'this', 'is', 'where', 'we', 'are', 'differentiated', 'by', 'our', 'embedded', 'product', 'experience', 'framework,', 'which', 'is', 'designed', 'to', 'deliver', 'a', 'consistently', 'great', 'experience', 'for', 'shoppers', 'and', 'retailers', 'across', 'our', 'portfolio.', 'furthermore,', 'we', 'remain', 'the', 'only', 'european', 'reit', 'with', 'strategic', 'exposure', 'to', 'the', 'premium', 'outlets', 'market.', 'our', 'investments', 'in', 'value', 'retail', 'and', 'via', 'outlets', 'materially', 'boosted', 'our', 'portfolio', 'returns', 'in', '2015.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'skills.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.', 'board', 'changes', 'and', 'senior', 'management', 'overview', 'i', 'am', 'pleased', 'to', 'announce', 'that,', 'with', 'effect', 'from', '24', 'january', '2018,', 'chris', 'pullen', 'is', 'appointed', 'as', 'chief', 'executive', 'of', 'staffline', 'group', 'plc.', 'chris', 'succeeds', 'andy', 'hogarth', 'who', 'will', 'step', 'down', 'from', 'his', 'current', 'role', 'while', 'remaining', 'on', 'the', 'board', 'as', 'a', 'non-executive', 'director.', 'andy', 'hogarth', 'has', 'been', 'chief', 'executive', 'of', 'staffline', 'group', 'plc', 'since', '2003,', 'during', 'which', 'time', 'the', 'group', 'has', 'grown', 'to', 'almost', '£1bn', 'in', 'revenue.']\n",
            "0.5842284618474416\n",
            "finished in 0:00:19! avg reward: 0.58\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.584228-64\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "Epoch     4: reducing learning rate of group 0 to 2.5000e-05.\n",
            "train step: 80, reward: 0.3116\n",
            "start running validation...1  batches validated\n",
            "['we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.', 'this', 'is', 'where', 'we', 'are', 'differentiated', 'by', 'our', 'embedded', 'product', 'experience', 'framework,', 'which', 'is', 'designed', 'to', 'deliver', 'a', 'consistently', 'great', 'experience', 'for', 'shoppers', 'and', 'retailers', 'across', 'our', 'portfolio.', 'furthermore,', 'we', 'remain', 'the', 'only', 'european', 'reit', 'with', 'strategic', 'exposure', 'to', 'the', 'premium', 'outlets', 'market.', 'our', 'investments', 'in', 'value', 'retail', 'and', 'via', 'outlets', 'materially', 'boosted', 'our', 'portfolio', 'returns', 'in', '2015.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'skills.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.', 'board', 'changes', 'and', 'senior', 'management', 'overview', 'i', 'am', 'pleased', 'to', 'announce', 'that,', 'with', 'effect', 'from', '24', 'january', '2018,', 'chris', 'pullen', 'is', 'appointed', 'as', 'chief', 'executive', 'of', 'staffline', 'group', 'plc.', 'chris', 'succeeds', 'andy', 'hogarth', 'who', 'will', 'step', 'down', 'from', 'his', 'current', 'role', 'while', 'remaining', 'on', 'the', 'board', 'as', 'a', 'non-executive', 'director.', 'andy', 'hogarth', 'has', 'been', 'chief', 'executive', 'of', 'staffline', 'group', 'plc', 'since', '2003,', 'during', 'which', 'time', 'the', 'group', 'has', 'grown', 'to', 'almost', '£1bn', 'in', 'revenue.']\n",
            "0.5842284618474416\n",
            "finished in 0:00:19! avg reward: 0.58\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.584228-80\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "Epoch     5: reducing learning rate of group 0 to 1.2500e-05.\n",
            "train step: 96, reward: 0.3360\n",
            "start running validation...1  batches validated\n",
            "['we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.', 'this', 'is', 'where', 'we', 'are', 'differentiated', 'by', 'our', 'embedded', 'product', 'experience', 'framework,', 'which', 'is', 'designed', 'to', 'deliver', 'a', 'consistently', 'great', 'experience', 'for', 'shoppers', 'and', 'retailers', 'across', 'our', 'portfolio.', 'furthermore,', 'we', 'remain', 'the', 'only', 'european', 'reit', 'with', 'strategic', 'exposure', 'to', 'the', 'premium', 'outlets', 'market.', 'our', 'investments', 'in', 'value', 'retail', 'and', 'via', 'outlets', 'materially', 'boosted', 'our', 'portfolio', 'returns', 'in', '2015.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'skills.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.', 'board', 'changes', 'and', 'senior', 'management', 'overview', 'i', 'am', 'pleased', 'to', 'announce', 'that,', 'with', 'effect', 'from', '24', 'january', '2018,', 'chris', 'pullen', 'is', 'appointed', 'as', 'chief', 'executive', 'of', 'staffline', 'group', 'plc.', 'chris', 'succeeds', 'andy', 'hogarth', 'who', 'will', 'step', 'down', 'from', 'his', 'current', 'role', 'while', 'remaining', 'on', 'the', 'board', 'as', 'a', 'non-executive', 'director.', 'andy', 'hogarth', 'has', 'been', 'chief', 'executive', 'of', 'staffline', 'group', 'plc', 'since', '2003,', 'during', 'which', 'time', 'the', 'group', 'has', 'grown', 'to', 'almost', '£1bn', 'in', 'revenue.']\n",
            "0.5842284618474416\n",
            "finished in 0:00:20! avg reward: 0.58\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.584228-96\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "Epoch     6: reducing learning rate of group 0 to 6.2500e-06.\n",
            "train step: 112, reward: 0.3482\n",
            "start running validation...1  batches validated\n",
            "['we', 'own,', 'operate', 'and', 'develop', 'retail', 'destinations', 'that', 'interact', 'seamlessly', 'with', 'digital', 'and', 'bring', 'together', 'the', 'very', 'best', 'retail,', 'leisure', 'and', 'entertainment', 'brands.', 'we', 'seek', 'to', 'deliver', 'value', 'for', 'all', 'our', 'stakeholders,', 'and', 'to', 'create', 'a', 'positive', 'and', 'sustainable', 'impact', 'for', 'generations', 'to', 'come.', '21', 'shopping', 'centres', '21', 'retail', 'parks', '15', 'premium', 'outlets', '2.2', 'million', 'm', '2', 'lettable', 'area', '4,500', 'tenants', 'les', 'terrasses', 'du', 'port,', 'marseille', 'battery', 'retail', 'park,', 'bristol', 'brent', 'south', 'shopping', 'park,', 'london', 'central', 'key', 'resources', 'the', 'success', 'of', 'our', 'business', 'depends', 'on', 'a', 'number', 'of', 'principal', 'inputs.', 'our', 'portfolio', 'includes', 'investments', 'in', 'prime', 'shopping', 'centres', 'in', 'the', 'uk', 'and', 'france,', 'convenient', 'retail', 'parks', 'in', 'the', 'uk', 'and', 'premium', 'outlets', 'across', 'europe.', 'a', 'clear', 'operational', 'model', 'the', 'key', 'actions', 'that', 'we', 'undertake', 'towards', 'achieving', 'our', 'strategic', 'objectives', 'to', 'create', 'value.', 'asset', 'management', 'we', 'skilfully', 'manage', 'our', 'portfolio', 'in', 'a', 'sustainable', 'way', 'to', 'generate', 'income', 'growth', 'and', 'to', 'attract', 'tenants', 'and', 'shoppers', 'investment', 'management', 'we', 'employ', 'market', 'expertise', 'to', 'recycle', 'our', 'portfolio.', 'taking', 'advantage', 'of', 'acquisition', 'opportunities', 'in', '2015', '+', '4', '4', 'hammerson', 'plc', 'annual', 'report', '2015', 'uniquely', 'differentiated', 'by', 'our', 'product', 'experience', 'framework', 'our', 'product', 'experience', 'framework', 'is', 'embedded', 'across', 'everything', 'we', 'do,', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', 'other', 'information', '1', 'hammerson.com', 'our', 'business', 'at', 'a', 'glance', '57', 'places', 'where', 'more', 'happens', 'we', 'are', 'an', 'owner,', 'manager', 'and', 'developer', 'of', 'retail', 'destinations', 'in', 'europe.', 'we', 'constantly', 'challenge', 'ourselves', 'to', 'apply', 'best', 'practice', 'in', 'retail', 'design', 'and', 'digital', 'solutions,', 'customer', 'engagement', 'and', 'sustainability.', 'to', 'deliver', 'value', 'for', 'our', 'stakeholders', 'by', 'successfully', 'employing', 'our', 'business', 'model', 'we', 'aim', 'to', 'deliver', 'a', 'positive', 'result', 'for', 'all', 'our', 'stakeholder', 'groups.', 'financial', 'returns…', 'for', 'shareholders', 'destinations…', 'for', 'retailers', 'and', 'shoppers', 'economic', 'and', 'social', 'benefits…', 'for', 'our', 'people', 'and', 'communities', 'iconic', 'destinations', 'we', 'create', 'outstanding', 'architecture', 'to', 'enhance', 'locations.', 'we', 'place', 'our', 'centres', 'at', 'the', 'heart', 'of', 'local', 'communities,', 'connected', 'by', 'seamless', 'technology', 'developments', 'completed:', '–', '25', 'new', 'retailers', 'in', 'french', 'portfolio,', 'of', 'which', 'four', 'are', 'firsts', 'to', 'france', '–', 'first', 'i', 'am', 'delighted', 'that', 'the', 'focus', 'on', 'our', 'strategic', 'priorities', 'continues', 'to', 'deliver', 'positive', 'results', 'for', 'our', 'stakeholders.”', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'plc', 'annual', 'report', '2015', '8', 'hammerson', 'our', 'focus', 'on', 'generating', 'sustainable', 'income', 'growth', 'continues', 'to', 'succeed,', 'and', 'we', 'have', 'grown', 'eps', 'by', '13%.', 'strategic', 'report', 'highlights', '01', 'our', 'business', 'at', 'a', 'glance', '02', 'our', 'business', 'model', '04', 'our', 'business', 'model', 'in', 'action', '06', 'in', 'conversation', 'with', 'david', 'atkins,', 'chief', 'executive', '08', 'our', 'market', '14', 'key', 'this', 'enables', 'us', 'to', 'once', 'more', 'increase', 'the', 'dividend', 'for', 'our', 'shareholders,', 'which', 'at', '22.3p', 'per', 'share', 'is', 'up', '9', '.3%', 'on', 'last', 'year', 'and', 'with', 'compound', 'annual', 'growth', 'of', '7', '.7%', 'nav', 'per', 'share', 'was', 'up', '11%', 'principally', 'due', 'to', 'a', 'total', 'property', 'return', 'of', '12.4%,', 'significantly', 'beating', 'ipd', '.', 'this', 'strong', 'financial', 'performance', 'is', 'in', 'part', 'thanks', 'to', 'the', 'high', 'quality', 'of', 'our', 'portfolio.', 'we', 'continue', 'to', 'recycle', 'capital', 'into', 'those', 'assets', 'and', 'developments', 'which', 'are', 'best', 'positioned', 'to', 'create', 'value', 'for', 'shareholders.', 'a', 'key', 'highlight', 'of', 'this', 'year', 'was', 'our', 'acquisition', 'in', 'ireland', 'which', 'provides', 'a', 'market-leading', 'platform', 'in', 'europe’s', 'fastest-growing', 'economy.', 'acquiring', 'a', 'portfolio', 'of', 'loans', 'requires', 'some', 'extra', 'steps', 'before', 'we', 'own', 'the', 'properties', 'but,', 'once', 'the', 'transaction', 'is', 'complete,', 'we', 'will', 'operate', 'one', 'of', 'europe’s', 'leading', 'shopping', 'centres,', 'dundrum', 'town', 'centre,', 'we', 'also', 'increased', 'our', 'investment', 'in', 'the', 'uk’s', 'second', 'city,', 'birmingham,', 'through', 'the', 'acquisition', 'of', 'grand', 'central', 'shopping', 'centre', 'in', 'joint', 'venture', 'with', 'cppib.', 'hammerson', 'plc', 'annual', 'report', '2015', 'where', 'more', 'happens', 'annual', 'report', '2015', 'we', 'are', 'hammerson', 'contents', 'at', 'hammerson,', 'we', 'create', 'destinations', 'that', 'excite', 'shoppers,', 'attract', 'and', 'support', 'retailers,', 'reward', 'where', 'more', 'happens.', 'birmingham', 'offers', 'an', 'increasingly', 'wealthy', 'catchment', 'and', 'improving', 'public', 'infrastructure', 'investment.', 'to', 'fund', 'these', 'transactions,', 'we', 'are', 'on', 'track', 'with', 'a', 'programme', 'of', 'disposals', 'and', 'we', 'remain', 'focused', 'on', 'maintaining', 'a', 'prudent', 'balance', 'sheet.', 'like-for-like', 'nri', 'growth', 'of', '2.3%', 'is', 'higher', 'than', 'last', 'year', 'as', 'we', 'continue', 'to', 'see', 'a', 'growing', 'demand', 'for', 'prime', 'retail', 'and', 'leisure', 'space.', 'against', 'a', 'backdrop', 'of', 'falling', 'vacancy,', 'combined', 'with', 'our', 'product', 'experience', 'framework', 'initiatives,', 'we', 'are', 'well-positioned', 'to', 'drive', 'future', 'rental', 'income', 'growth.', 'this', 'year', 'we', 'successfully', 'relocated', 'our', 'uk', 'offices', 'to', 'more', 'cost-efficient', 'sites,', 'moving', 'our', 'london', 'headquarters', 'to', 'kings', 'place,', 'king’s', 'cross.', 'costs', 'were', 'tightly', 'controlled', 'across', 'the', 'business', 'and', 'we', 'reduced', 'total', 'administrative', 'costs,', 'alongside', 'investing', 'further', 'in', 'important', 'areas', 'such', 'as', 'digital', 'and', 'marketing.', 'there', 'is', 'still', 'more', 'to', 'be', 'done', 'however,', 'especially', 'as', 'a', 'result', 'of', 'additional', 'property', 'costs', 'from', 'the', 'strategic', 'development', 'properties', 'we', 'hold.', 'in', '2015,', 'we', 'completed', 'an', 'impressive', 'four', 'developments,', 'adding', '64,900m', '2', 'of', 'incremental', 'retail', 'space.', 'this', 'included', 'le', 'jeu', 'de', 'paume,', 'a', 'new', 'regional', 'shopping', 'centre', 'in', 'beauvais;', 'the', 'winter', 'garden', 'restaurant', 'and', 'leisure', 'extension', 'at', 'silverburn,', 'glasgow;', 'a', 'next-', 'generation', 'fashion', 'park', 'in', 'rugby', 'with', 'elliott’', 'we', 'also', 'made', 'good', 'progress', 'at', 'our', 'major', 'london', 'development', 'schemes.', 'how', 'is', 'the', 'investment', 'proposition', 'for', 'shareholders', 'in', '2016', 'differentiated', 'versus', 'peers?', 'we', 'are', 'better', 'positioned', 'than', 'ever', 'to', 'take', 'advantage', 'of', 'the', 'improving', 'consumer', 'backdrop', 'and', 'to', 'continue', 'to', 'generate', 'earnings', 'growth', 'ahead', 'of', 'our', 'peers.', 'as', 'well', 'as', 'owning', 'prime', 'real', 'estate,', 'driving', 'rental', 'growth', 'requires', 'a', 'thorough', 'and', 'hands-on', 'approach', 'to', 'asset', 'management.', 'this', 'is', 'where', 'we', 'are', 'differentiated', 'by', 'our', 'embedded', 'product', 'experience', 'framework,', 'which', 'is', 'designed', 'to', 'deliver', 'a', 'consistently', 'great', 'experience', 'for', 'shoppers', 'and', 'retailers', 'across', 'our', 'portfolio.', 'furthermore,', 'we', 'remain', 'the', 'only', 'european', 'reit', 'with', 'strategic', 'exposure', 'to', 'the', 'premium', 'outlets', 'market.', 'our', 'investments', 'in', 'value', 'retail', 'and', 'via', 'outlets', 'materially', 'boosted', 'our', 'portfolio', 'returns', 'in', '2015.']\n",
            "['**', 'net', 'debt', 'including', 'unamortised', 'transaction', 'costs', 'the', 'below', 'charts', 'show', 'the', 'split', 'of', 'underlying', 'operating', 'profit', 'between', 'the', 'two', 'divisions', 'for', 'the', 'last', 'three', 'years.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector.', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'optimisation', 'of', 'work', 'programme', 'business', '•', 'deliver', 'growth', 'in', 'our', 'prisons’', 'offender', 'learning', 'and', 'skills', 'services', 'contracts', '•', 'develop', 'and', 'grow', 'private', 'skills', 'market', 'key', 'priorities', 'for', '2018', 'are', 'as', 'follows:', '•', 'continued', 'organic', 'growth', '–', 'focus', 'on', 'our', 'core', 'business', '•', 'continued', 'strong', 'cash', 'conversion', '–', 'focus', 'on', 'margin', 'and', 'payment', 'terms', '•', 'bolt', 'on', '3', 'overview', 'strategic', 'report', 'corporate', 'governance', 'financial', 'statements', '2017', '2016', '2011', '2008', '112.6', '114.0', '45.8', '25.3', '92.4', '36.7', '3.5', '10.5', '7.5', '3.4', '7.5', '7.5', '3.4', 'annual', 'reported', 'proﬁt', 'before', 'tax', '£’m', 'compound', 'on', 'track', 'to', 'be', 'in', 'a', 'net', 'cash', 'position', 'in', '2018', '•', 'senior', 'management', 'changes', '•', 'operational', 'efficiencies', 'and', 'top', 'performance', 'within', 'peopleplus', 'division:', '•', 'continued', 'cost', 'efficiencies', 'leading', 'to', 'improved', 'margins', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'eire', 'and', 'poland.', '£54m', 'of', 'new', 'business', 'won', 'in', '2017,', 'including', '£24m', 'for', 'scotland', 'work', 'programme', '(“fair', 'start”)', 'and', 'over', '£10m', 'adult', 'education', 'funding', '•', 'our', 'work', 'to', 'help', 'the', 'long', 'term', 'unemployed', 'into', 'work', 'all', 'nine', 'of', 'our', 'work', 'programme', 'contracts', 'are', 'in', 'the', 'top', 'ten', '(out', 'of', '39)', 'nationally', 'for', 'performance', '•', 'adult', 'education', 'division', 'awarded', 'a', '2', 'rating', '(“good”)', 'by', 'ofsted', 'during', '2017', 'ten', 'staffline', 'recruitment', '(“recruitment”)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'skills.', 'jobs.', 'company', 'overview', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'recruitment', 'division', '(previously', '“staffing”', 'division)', 'supplied', 'up', 'to', '52,000', 'workers', 'per', 'day', 'in', '2017', 'to', 'more', 'than', '1,500', 'clients.', 'visit', 'www.stafflinegroupplc.co.uk', 'staffline', 'is', 'a', 'leading', 'workforce', 'recruitment', 'and', 'management', 'organisation', 'providing', 'services,', 'mainly', 'in', 'the', 'uk', 'and', 'eire,', 'to', 'both', 'government', 'and', 'commercial', 'customers.', 'the', 'peopleplus', 'division', 'is', 'a', 'leading', 'provider', 'to', 'both', 'central', 'and', 'local', 'government,', 'as', 'well', 'as', 'commercial', 'customers,', 'offering', 'a', 'wide', 'range', 'of', 'services', 'to', 'help', 'and', 'support', 'in', 'the', 'employability', '(welfare', 'recruitment', 'strong', 'growth', 'potential:', 'specialising', 'in', 'providing', 'complete', 'labour', 'solutions', 'in', 'agriculture,', 'food', 'processing,', 'manufacturing,', 'e-retail,', 'driving', 'and', 'the', 'logistics', 'sectors,', 'the', 'recruitment', 'business', 'operates', 'from', 'over', '400', 'locations', 'in', 'the', 'uk,', 'overview', 'strategic', 'report', 'report', 'corporate', 'governance', 'financial', 'statements', 'peopleplus', 'established', 'platform,', 'strong', 'credentials:', 'a', 'trusted', 'partner', 'in', 'delivering', 'employability,', 'skills', 'and', 'well-being', 'services.', 'contracts', 'span', 'central,', 'local', 'and', 'devolved', 'government', 'and', 'the', 'private', 'sector,', 'including:', 'employability:', '•', 'work', 'programme,', 'prime', 'contractor', 'in', 'nine', 'regions', 'and', 'sub-contractor', 'in', 'three', 'regions', 'in', 'england', 'and', 'wales', '•', 'fair', 'our', 'peopleplus', 'division', 'has', 'met', 'the', 'challenge', 'of', 'delivering', 'the', 'final', 'years', 'of', 'the', 'existing', 'work', 'programme,', 'continuing', 'to', 'be', 'the', 'best', 'performing', 'supplier', 'to', 'the', 'dwp', 'and', 'positioning', 'itself', 'to', 'become', 'other', 'opportunities', 'during', '2017', 'included', 'winning', 'the', 'fair', 'start', 'programme', 'in', 'scotland.', 'performance', '2017', 'was', 'the', 'final', 'year', 'of', 'our', 'five-year', 'plan', 'to', '“burst', 'the', 'billion”,', 'aiming', 'to', 'grow', 'group', 'revenues', 'to', 'over', '£1bn', 'by', '2017.', 'group', 'revenues', 'of', '£957.8m', '(2016:', '£882.4m),', 'up', '9%,', 'leaves', 'us', 'just', 'short', 'of', 'this', 'target.', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'ena', 'bl', 'i', 'ng', 'the', 'future', 'of', 'work', '™', 'staffline', 'staffline', 'group', 'plc', 'annual', 'report', '2017', 'established', 'in', '1986', 'the', 'group', 'has', 'two', 'business', 'divisions:', 'staffline', 'however,', 'the', 'exiting', 'run', 'rate', 'of', '2017', 'exceeds', 'this', 'target', 'and', 'remains', 'a', 'significant', 'achievement', 'compared', 'to', 'the', 'starting', 'point', 'of', 'revenues', 'of', '£367m', 'in', '2012,', 'when', 'the', 'five-year', 'plan', 'began.', 'the', 'five-year', 'underlying', 'operating', 'profit', 'target', 'of', '£30m', 'was', 'comfortably', 'exceeded,', 'being', 'achieved', 'two', 'years', 'early,', 'in', '2015.', 'underlying', 'profit', 'before', 'tax*', 'reduced', 'by', '1%', 'to', '£36.3m', '(2016:', '£36.7m).', 'whilst', 'a', 'reduction,', 'this', 'is', 'a', 'good', 'achievement', 'given', 'the', 'work', 'programme', 'run-off', 'in', 'peopleplus', 'that', 'began', 'in', 'march', '2017.', 'reported', 'profit', 'before', 'tax', 'increased', 'by', '28%', 'to', '£24.1m', '(2016:', '£18.9m),', 'primarily', 'due', 'to', 'exceptional', 'reorganisation', 'costs', 'incurred', 'in', '2016', 'not', 'being', 'repeated', 'in', '2017.', 'cash', 'generation', 'was', 'again', 'strong,', 'with', 'free', 'cash', 'flows', '(being', 'ebitda', 'plus', 'working', 'capital', 'movement,', 'less', 'tax', 'paid', 'and', 'capex)', 'amounting', 'to', '£37.9m.', 'net', 'debt**', 'fell', 'by', '£20.2m,', 'from', '£36.7m', 'at', 'the', 'end', 'of', 'december', '2016', 'to', '£16.5m', 'at', 'the', 'end', 'of', 'december', '2017.', 'this', 'provides', 'the', 'group', 'with', 'a', 'solid', 'base', 'from', 'which', 'to', 'continue', 'to', 'generate', 'shareholder', 'value', 'into', 'the', 'future.', 'john', 'crabtree', 'obe', 'chairman', '*', 'underlying', 'profit', 'before', 'tax', 'excludes', 'amortisation', 'of', 'intangible', 'assets', 'arising', 'on', 'business', 'combinations,', 'acquisition', 'and', 'exceptional', 'reorganisation', 'costs,', 'and', 'the', 'non-cash', 'charge/', 'credit', 'for', 'share', 'based', 'payment', 'our', 'financial', 'position', 'at', 'the', 'end', 'of', '2017', 'and', 'confidence', 'for', 'the', 'future', 'enables', 'us', 'to', 'propose', 'an', 'increased', 'final', 'dividend', 'of', '15.7p', '(2016:', '15.3p),', 'payable', 'on', 'tuesday', '3', 'july', '2018.', 'the', 'group’s', 'dividend', 'policy,', 'whilst', 'in', 'a', 'net', 'debt', 'position,', 'is', 'to', 'maintain', 'a', 'dividend', 'cover', 'ratio', 'of', 'between', '4.0', 'and', '4.5', 'times', 'our', 'underlying', 'diluted', 'eps.', 'our', 'proposed', 'final', 'dividend', 'will', 'ensure', 'the', 'full', 'year', 'dividend', 'cover', 'is', 'within', 'this', 'range', 'at', '4.22', 'times.', 'further', 'details', 'on', 'the', 'group’s', 'dividend', 'policy', 'can', 'be', 'found', 'within', 'the', 'chief', 'financial', 'officer’s', 'report', 'on', 'page', '23.', 'board', 'changes', 'and', 'senior', 'management', 'overview', 'i', 'am', 'pleased', 'to', 'announce', 'that,', 'with', 'effect', 'from', '24', 'january', '2018,', 'chris', 'pullen', 'is', 'appointed', 'as', 'chief', 'executive', 'of', 'staffline', 'group', 'plc.', 'chris', 'succeeds', 'andy', 'hogarth', 'who', 'will', 'step', 'down', 'from', 'his', 'current', 'role', 'while', 'remaining', 'on', 'the', 'board', 'as', 'a', 'non-executive', 'director.', 'andy', 'hogarth', 'has', 'been', 'chief', 'executive', 'of', 'staffline', 'group', 'plc', 'since', '2003,', 'during', 'which', 'time', 'the', 'group', 'has', 'grown', 'to', 'almost', '£1bn', 'in', 'revenue.']\n",
            "0.5842284618474416\n",
            "finished in 0:00:19! avg reward: 0.58\n",
            "Check point at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt_new/ckpt-0.584228-112\n",
            "Checkpoint at  /content/drive/My Drive/finsummary/Data/rl_dir/ckpt\n",
            "Epoch     7: reducing learning rate of group 0 to 3.1250e-06.\n",
            "Training finised in  0:12:35.370773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN6JH52yv5Yo"
      },
      "source": [
        "def coll(batch):\n",
        "        art_batch, abs_batch = unzip(batch)\n",
        "        art_batch = list(filter(bool, concat(art_batch)))\n",
        "        abs_batch = list(filter(bool, concat(abs_batch)))\n",
        "        print(art_batch)\n",
        "        sys.exit()\n",
        "        return art_sents, abs_sents\n",
        "loader = DataLoader(RLDataset('training'), batch_size=1,\n",
        "                    shuffle=True, num_workers=4,\n",
        "                    collate_fn=coll\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB9-UTyWwUry"
      },
      "source": [
        "# for i,a in enumerate(loader):\n",
        "#   print(i,'-->',a)\n",
        "#   if i==2:\n",
        "#     break\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OKioEH6y7Sg"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser(\n",
        "#         description='program to demo a Seq2Seq model'\n",
        "#     )\n",
        "#     parser.add_argument('--path', required=True, help='root of the model')\n",
        "\n",
        "\n",
        "#     # model options\n",
        "#     parser.add_argument('--abs_dir', action='store',\n",
        "#                         help='pretrained summarizer model root path')\n",
        "#     parser.add_argument('--ext_dir', action='store',\n",
        "#                         help='root of the extractor model')\n",
        "#     parser.add_argument('--ckpt', type=int, action='store', default=None,\n",
        "#                         help='ckeckpoint used decode')\n",
        "\n",
        "#     # training options\n",
        "#     parser.add_argument('--reward', action='store', default='rouge-l',\n",
        "#                         help='reward function for RL')\n",
        "#     parser.add_argument('--lr', type=float, action='store', default=1e-4,\n",
        "#                         help='learning rate')\n",
        "#     parser.add_argument('--decay', type=float, action='store', default=0.5,\n",
        "#                         help='learning rate decay ratio')\n",
        "#     parser.add_argument('--lr_p', type=int, action='store', default=0,\n",
        "#                         help='patience for learning rate decay')\n",
        "#     parser.add_argument('--gamma', type=float, action='store', default=0.95,\n",
        "#                         help='discount factor of RL')\n",
        "#     parser.add_argument('--stop', type=float, action='store', default=1.0,\n",
        "#                         help='stop coefficient for rouge-1')\n",
        "#     parser.add_argument('--clip', type=float, action='store', default=2.0,\n",
        "#                         help='gradient clipping')\n",
        "#     parser.add_argument('--batch', type=int, action='store', default=32,\n",
        "#                         help='the training batch size')\n",
        "#     parser.add_argument(\n",
        "#         '--ckpt_freq', type=int, action='store', default=1000,\n",
        "#         help='number of update steps for checkpoint and validation'\n",
        "#     )\n",
        "#     parser.add_argument('--patience', type=int, action='store', default=3,\n",
        "#                         help='patience for early stopping')\n",
        "#     parser.add_argument('--no-cuda', action='store_true',\n",
        "#                         help='disable GPU training')\n",
        "#     args = parser.parse_args()\n",
        "#     args.cuda = torch.cuda.is_available() and not args.no_cuda\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXRRI_2jcUzZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}