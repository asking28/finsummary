{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "combining_data_word2vec_parallel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asking28/finsummary/blob/master/combining_data_word2vec_parallel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaBGrIgkon3L"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "import pandas as pd\n",
        "import gensim\n",
        "import glob\n",
        "import nltk.data\n",
        "from tqdm import tqdm\n",
        "import ast\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "paths=['/content/drive/My Drive/finsummary/Data/validation/annual_reports','/content/drive/My Drive/finsummary/Data/testing/annual_reports','/content/drive/My Drive/finsummary/Data/training/annual_reports']\n",
        "word_tokenizer=nltk.word_tokenize\n",
        "\n",
        "# glob.glob('/content/drive/My Drive/finsummary/Data/validation/annual_reports/*.txt')\n",
        "\n",
        "nltk.download('punkt')\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "# fp = open('/content/drive/My Drive/finsummary/Data/validation/annual_reports/31201.txt')\n",
        "# data = fp.read()\n",
        "# for sent in tokenizer.tokenize(data):\n",
        "  # sent.replace('\\n',' ')\n",
        "# print('\\n-----\\n'.join(tokenizer.tokenize(data)))\n",
        "# sentences=[]\n",
        "# for dir_p in paths:\n",
        "#   for i,file_p in enumerate(glob.glob(dir_p+'/*.txt')):\n",
        "#     fp=open(file_p)\n",
        "#     data=fp.read()\n",
        "#     for sent in tokenizer.tokenize(data):\n",
        "#       sent=sent.replace('\\n',' ')\n",
        "#       sent=word_tokenizer(sent)\n",
        "#       sentences.append(sent)\n",
        "#     if i%100==0:\n",
        "#       df=pd.DataFrame({'Sentences':sentences})\n",
        "#       df.to_csv(dir_p+'/combine_sents_emb_train'+str(i)+'.csv')\n",
        "#       sentences=[]\n",
        "#   df=pd.DataFrame({'Sentences':sentences})\n",
        "#   df.to_csv(dir_p+'/combine_sents_emb_train'+str(i)+'.csv')\n",
        "#   print(dir_p)\n",
        "# df=pd.DataFrame({'Sentences':sentences})\n",
        "# df.to_csv('/content/drive/My Drive/finsummary/combine_sents_emb_train.csv')\n",
        "\n",
        "sentences=[]\n",
        "for dir_p in paths:\n",
        "  for i,file_p in enumerate(glob.glob(dir_p+'/*.csv')):\n",
        "    df=pd.read_csv(file_p,converters={'Sentences':ast.literal_eval})\n",
        "    for i in tqdm(range(len(df))):\n",
        "      sentences.append(['<s>']+df['Sentences'][i]+[r'<\\s>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mggPiJKob-j7"
      },
      "source": [
        "# df=pd.read_csv('/content/drive/My Drive/finsummary/word2vec/combine_sents_emb_train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emf9ezwFcOwR"
      },
      "source": [
        "# df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqSUyrqacSJj"
      },
      "source": [
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b2X0Qkrerfx"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs99yczYV2_m"
      },
      "source": [
        "def remove_nn(a):\n",
        "  result = re.sub('[^0-9a-zA-Z]',' ', a)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjoZkorbWRv5"
      },
      "source": [
        "for i in tqdm(range(len(sentences))):\n",
        "  sentences[i]=remove_nn(sentences[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZu_WiJv7JS9"
      },
      "source": [
        "model=gensim.models.Word2Vec(size=300,min_count=3,workers=16,sg=1)\n",
        "model.build_vocab(sentences)\n",
        "# model.train(sentences,total_examples=model.corpus_count,epochs=model.iter)\n",
        "# save_dir='/content/drive/My Drive/finsummary/Data'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi9D4wHlYvGX"
      },
      "source": [
        "model.train(sentences,total_examples=model.corpus_count,epochs=model.iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuHSmzHX9HSs"
      },
      "source": [
        "model.save(os.path.join(save_dir, 'word2vec.{}d.{}k.bin'.format(300, len(model.wv.vocab)//1000)))\n",
        "model.wv.save_word2vec_format(os.path.join(save_dir,'word2vec.{}d.{}k.w2v'.format(300, len(model.wv.vocab)//1000)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}